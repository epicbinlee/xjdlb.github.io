<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>HBase基础开发</title>
      <link href="/2018/05/05/HBase%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91/"/>
      <url>/2018/05/05/HBase%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91/</url>
      <content type="html"><![CDATA[<p>本文介绍了如何利用eclipse进行HBase相关的开发。</p><a id="more"></a><h2 id="模拟场景"><a href="#模拟场景" class="headerlink" title="模拟场景"></a>模拟场景</h2><ul><li>运营商通话记录<br>查询通话详单<br>本机号码 主叫/被叫 通话时长 时间 对方号码 号码属性 归属地</li></ul><h2 id="第1次启动分布式hadoop和hbase"><a href="#第1次启动分布式hadoop和hbase" class="headerlink" title="第1次启动分布式hadoop和hbase"></a>第1次启动分布式hadoop和hbase</h2><ul><li>删除所有的临时文件夹(重要)</li><li>启动zk集群，三台，最好在xshell下面一起启动，并且查看状态status</li><li>edits交给JN,手动启动三个JN</li><li>格式化一台NN，并启动这一台(只能格式化一次，不然重新来)</li><li>另外一台NN执行同步standby</li><li>格式化zk</li><li>启动dfs(可以直接全部启动)</li><li>最后启动RS<pre><code>rm -rdf /opt/hadoop/* /opt/journal/* /opt/zookeeper/v* /opt/zookeeper/z*zkServer.sh start(xshell同时启动)vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode)hadoop-daemon.sh start journalnode(根据hdfs配置文件，n2 n3 n4)hdfs namenode -format(选一个NN格式化，一次机会，失败重头再来)hadoop-daemon.sh start namenode(格式化的NN上)hdfs namenode -bootstrapStandby(n2,没有格式化的NN上)hdfs zkfc -formatZK(n1)start-all.sh(n1)yarn-daemon.sh start resourcemanager(指定的机器，n3 n4)start-hbase.sh</code></pre></li></ul><h2 id="第2次及以后启动分布式hadoop和hbase"><a href="#第2次及以后启动分布式hadoop和hbase" class="headerlink" title="第2次及以后启动分布式hadoop和hbase"></a>第2次及以后启动分布式hadoop和hbase</h2><ul><li>启动过程<pre><code>zkServer.sh start(xshell同时启动)vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode)start-all.sh(n1)yarn-daemon.sh start resourcemanager(指定的机器，n3 n4)start-hbase.sh</code></pre></li></ul><h2 id="eclipse连接hadoop"><a href="#eclipse连接hadoop" class="headerlink" title="eclipse连接hadoop"></a>eclipse连接hadoop</h2><ul><li>连接过程具体<a href="https://leebin.top/2018/04/30/%E6%9C%AC%E5%9C%B0eclipse%E9%93%BE%E6%8E%A5%E8%BF%9C%E7%A8%8Bhadoop%E7%BC%96%E5%86%99hdfs%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81/" target="_blank" rel="noopener">参考</a></li></ul><h2 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h2><ul><li>新建项目</li><li>到入hbase安装包下面的lib文件夹到项目目录下</li><li><p>写代码创建表</p><pre><code>package com.hikvision.hbase;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.log4j.BasicConfigurator;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HBase_test {  HBaseAdmin hAdmin;  String TABLE_NAME = &quot;phone&quot;;  // 列族 一般是1到2个  String COLUMN_FAMILY = &quot;cf1&quot;;  @SuppressWarnings(&quot;deprecation&quot;)  @Before  public void begin() throws Exception {      BasicConfigurator.configure();      Configuration conf = new Configuration();      conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;n1,n2,n3&quot;);      conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);      conf.set(&quot;hbase.master&quot;, &quot;n5:16020&quot;);      hAdmin = new HBaseAdmin(conf);  }  @After  public void end() {      if (hAdmin != null) {          try {              hAdmin.close();          } catch (IOException e) {              e.printStackTrace();          }      }  }  @Test  public void createTable() throws Exception {      // 0.容错,先判断是不是存在      if (hAdmin.tableExists(TABLE_NAME)) {          hAdmin.disableTable(TABLE_NAME);          hAdmin.deleteTable(TABLE_NAME);      }      // 1.表描述      HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(TABLE_NAME));      // 2.列族描述      HColumnDescriptor family = new HColumnDescriptor(COLUMN_FAMILY);      // 2.1读缓存      family.setBlockCacheEnabled(true);      family.setInMemory(true);      // 2.2设定最大版本数默认为1      family.setMaxVersions(1);      // 为表指定列族      desc.addFamily(family);      hAdmin.createTable(desc);  }}</code></pre></li><li><p>查看hbase shell</p><pre><code>root@n5:~# hbase shellSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/root/app/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/root/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017hbase(main):001:0&gt; listTABLE                                                                                                                                                                                                            phone                                                                                                                                                                                                            t3                                                                                                                                                                                                               tab-1                                                                                                                                                                                                            3 row(s) in 0.2560 seconds=&gt; [&quot;phone&quot;, &quot;t3&quot;, &quot;tab-1&quot;]hbase(main):002:0&gt; describe &quot;phone&quot;Table phone is ENABLED                                                                                                                                                                                           phone                                                                                                                                                                                                            COLUMN FAMILIES DESCRIPTION                                                                                                                                                                                      {NAME =&gt; &#39;cf1&#39;, BLOOMFILTER =&gt; &#39;ROW&#39;, VERSIONS =&gt; &#39;1&#39;, IN_MEMORY =&gt; &#39;true&#39;, KEEP_DELETED_CELLS =&gt; &#39;FALSE&#39;, DATA_BLOCK_ENCODING =&gt; &#39;NONE&#39;, TTL =&gt; &#39;FOREVER&#39;, COMPRESSION =&gt; &#39;NONE&#39;, MIN_VERSIONS =&gt; &#39;0&#39;, BLOCKCACHE =&gt; &#39;true&#39;, BLOCKSIZE =&gt; &#39;65536&#39;, REPLICATION_SCOPE =&gt; &#39;0&#39;}                                                                                                                                                     1 row(s) in 0.1350 seconds</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> HBase </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HBase原理搭建高可用与基本使用</title>
      <link href="/2018/05/03/HBase%E5%8E%9F%E7%90%86%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B8%8E%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/05/03/HBase%E5%8E%9F%E7%90%86%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B8%8E%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>本文介绍如何安装并使用HBase。</p><a id="more"></a><h2 id="伪分布式的HBase环境搭建Pseudo-Distributed-Local-Install-参考"><a href="#伪分布式的HBase环境搭建Pseudo-Distributed-Local-Install-参考" class="headerlink" title="伪分布式的HBase环境搭建Pseudo-Distributed Local Install 参考"></a>伪分布式的HBase环境搭建Pseudo-Distributed Local Install <a href="https://hbase.apache.org/book.html#standalone_dist" target="_blank" rel="noopener">参考</a></h2><ul><li><p>配置</p><pre><code>&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="完全分布式安装"><a href="#完全分布式安装" class="headerlink" title="完全分布式安装"></a>完全分布式安装</h2></li><li><p>架构</p></li></ul><table><thead><tr><th>VM</th><th>NN</th><th>DN</th><th>JN</th><th>ZK</th><th>ZKFC</th><th>RS</th><th>HM</th><th>HRS</th></tr></thead><tbody><tr><td>n1</td><td>y</td><td>n</td><td>n</td><td>y</td><td>y</td><td>n</td><td>y(b)</td><td>n</td></tr><tr><td>n2</td><td>y</td><td>y</td><td>y</td><td>y</td><td>y</td><td>n</td><td>n</td><td>y</td></tr><tr><td>n3</td><td>n</td><td>y</td><td>y</td><td>y</td><td>n</td><td>y</td><td>n</td><td>y</td></tr><tr><td>n4</td><td>n</td><td>y</td><td>y</td><td>n</td><td>n</td><td>y</td><td>n</td><td>y</td></tr><tr><td>n5</td><td>n</td><td>y</td><td>n</td><td>n</td><td>n</td><td>n</td><td>y(m)</td><td>n</td></tr><tr><td>n6</td><td>n</td><td>y</td><td>n</td><td>n</td><td>n</td><td>n</td><td>n</td><td>n</td></tr></tbody></table><ul><li><p>hbase-env.sh</p><pre><code>export JAVA_HOME=/root/app/jdkexport HBASE_MANAGES_ZK=false (不启动自带的zookeeper)</code></pre></li><li><p>hbase-site.xml</p><pre><code>&lt;configuration&gt;&lt;property&gt;  &lt;name&gt;hbase.rootdir&lt;/name&gt;  &lt;value&gt;hdfs://sxt:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;  &lt;value&gt;n1,n2,n3&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>conf/regionservers</p><pre><code>#根据架构把n5设定为HMaster，下面的为HReginServern2n3n4</code></pre></li><li><p>conf/backup-masters file(master的HA)</p><pre><code>vim backup-mastersn1#根据架构将n1设定为HMaster从节点</code></pre></li><li><p>HDFS Client Configuration(hbase访问hdfs)</p><pre><code>copy hdfs-site.xml ${HBASE_HOME}/confroot@n1:~/app/hbase/conf# pwd/root/app/hbase/confcp /root/app/hadoop/etc/hadoop/hdfs-site.xml ./</code></pre></li><li><p>启动</p><pre><code>#-----------------------------------在n5 HMaster主节点启动root@n5:~/app/hbase/conf# jps1058 NodeManager962 DataNode2706 HMaster3083 Jps#-----------------------------------在n2,n3,n4查看HRegionServerroot@n4:~# jps1041 DataNode949 JournalNode2618 Jps2396 HRegionServer1198 NodeManager1263 ResourceManager</code></pre></li><li><p>webUI查看</p><pre><code>主节点 http://n5:16010/master-status从节点 http://n1:16010/master-statusroot@n5:~/app/hbase/conf# netstat -ntlp | grep &quot;java&quot;tcp        0      0 0.0.0.0:50010           0.0.0.0:*               LISTEN      962/java        tcp        0      0 0.0.0.0:50075           0.0.0.0:*               LISTEN      962/java        tcp        0      0 0.0.0.0:50020           0.0.0.0:*               LISTEN      962/java        tcp        0      0 127.0.0.1:42933         0.0.0.0:*               LISTEN      962/java        tcp6       0      0 :::13562                :::*                    LISTEN      1058/java       tcp6       0      0 192.168.44.105:16000    :::*                    LISTEN      2706/java       tcp6       0      0 :::8040                 :::*                    LISTEN      1058/java       tcp6       0      0 :::16010                :::*                    LISTEN      2706/java       tcp6       0      0 :::8042                 :::*                    LISTEN      1058/java       tcp6       0      0 :::34187                :::*                    LISTEN      1058/java </code></pre></li></ul><h2 id="常见的命令"><a href="#常见的命令" class="headerlink" title="常见的命令"></a>常见的命令</h2><ul><li><p>增删改查</p><pre><code>hbase shellhelp#-----------------------------------status table_help version whoamiddl（disable drop enable exist is_disableed is_enabled list show_filters）namespace（alter_namespace ...）dml（append put get delete scan）tools（balancer flush:menorystore2storefile）replicationsnapshots#-----------------------------------createcreate &#39;person&#39;,&#39;cf1&#39;,&#39;cf2&#39;#-----------------------------------lsitdesc &#39;person&#39;TTL =&gt; &#39;forever&#39;#-----------------------------------putput &#39;person&#39;,&#39;r1&#39;,&#39;c1&#39;,&#39;value&#39;put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;,&#39;xiaoming&#39;put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:sex&#39;,&#39;boy&#39;#-----------------------------------scanscan &#39;person&#39;#-----------------------------------getget &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;#-----------------------------------插入数据put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;,&#39;xiaoming2&#39;get &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;#-----------------------------------#查看命名空间list_namespacedefaulthbase#-----------------------------------#先禁用再删除create &#39;tbl&#39;,&#39;cf&#39;listdisabledisabled &#39;tbl&#39;listdrop &#39;tbl&#39;</code></pre></li><li><p>查看hbase的文件<br>单机模式文件在Linux上<br>伪分布式模式在Linux上<br>分布式模式在hdfs上</p><pre><code>D:\HBASE目录结构└─namespace  ├─主节点HRegionMaster  └─域节点HRegionServer      └─表table          ├─域HRegion          │  ├─存储Store          │  │  ├─内存存储memoryStore          │  │  ├─内存存储memoryStore1          │  │  ├─磁盘储存storeFile          │  │  └─磁盘储存storeFile1          │  └─存储Store1          │      ├─内存存储memoryStore          │      └─内存存储memoryStore1          └─域HRegion1              ├─存储Store              │  ├─内存存储memoryStore              │  └─内存存储memoryStore1              └─存储Store1                  ├─内存存储memoryStore                  └─内存存储memoryStore1</code></pre></li><li><p>webUI查看 n1:60010</p></li></ul><h2 id="HBase-的ha验证"><a href="#HBase-的ha验证" class="headerlink" title="HBase 的ha验证"></a>HBase 的ha验证</h2><ul><li><p>创建表</p><pre><code>#--------------------------------------------create &#39;tab-1&#39; ,&#39;cf-1&#39;put &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:name&#39;,&#39;xiaoli&#39;#--------------------------------------------hbase(main):007:0&gt; scan &#39;tab-1&#39;ROW                        COLUMN+CELL                                                                 rk-0001                   column=cf-1:name, timestamp=1525506703143, value=xiaoli                     1 row(s) in 0.2510 seconds#--------------------------------------------hbase(main):008:0&gt; get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:name&#39;COLUMN                     CELL                                                                        cf-1:name                 timestamp=1525506703143, value=xiaoli                                       1 row(s) in 0.0480 seconds#--------------------------------------------put &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:age&#39;,&#39;18&#39;#--------------------------------------------hbase(main):010:0&gt; scan &#39;tab-1&#39;ROW                        COLUMN+CELL                                                                 rk-0001                   column=cf-1:age, timestamp=1525507159178, value=18                          rk-0001                   column=cf-1:name, timestamp=1525506703143, value=xiaoli                     1 row(s) in 0.0320 seconds#--------------------------------------------get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1&#39;#--------------------------------------------hbase(main):011:0&gt; get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1&#39;COLUMN                     CELL                                                                        cf-1:age                  timestamp=1525507159178, value=18                                           cf-1:name                 timestamp=1525506703143, value=xiaoli                                       2 row(s) in 0.0200 seconds</code></pre></li><li><p>验证</p><pre><code>#--------------------------------------------root@n5:~# jps1058 NodeManager962 DataNode5444 Jps3625 HMaster#--------------------------------------------kill -9 3625查看n5:16010查看n1:16010查看hbase shellscan &#39;tab-1&#39;#--------------------------------------------重新启动n5上的master#--------------------------------------------</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> HBase </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MR实例温度</title>
      <link href="/2018/05/01/MR%E5%AE%9E%E4%BE%8B%E6%B8%A9%E5%BA%A6/"/>
      <url>/2018/05/01/MR%E5%AE%9E%E4%BE%8B%E6%B8%A9%E5%BA%A6/</url>
      <content type="html"><![CDATA[<p>本文介绍如何分析具体的MR实例。</p><a id="more"></a><h2 id="数据与需求"><a href="#数据与需求" class="headerlink" title="数据与需求"></a>数据与需求</h2><ul><li><p>数据</p><pre><code>  1949-10-01 14:21:02 34c  1949-10-02 14:01:02 36c  1950-01-01 11:21:02 32c  1950-10-01 12:21:02 37c  1951-12-01 12:21:02 23c  1950-10-02 12:21:02 41c  1950-10-03 12:21:02 27c  1951-07-01 12:21:02 45c  1951-07-02 12:21:02 46c  1951-07-03 12:21:03 47c</code></pre></li><li><p>需求</p><pre><code>  输出：得出每个年月下，温度最高的前两天  年月：升序  温度：降序</code></pre></li><li><p>技术点分析</p><pre><code>  sort需要进行二次排序，需要从写sort方法  reduce个数设定为3个，可能有%3操作，进行负载均衡  全部的MR模型中包含了map,reduce,part,sort,group,combine</code></pre></li></ul><h2 id="写代码"><a href="#写代码" class="headerlink" title="写代码"></a>写代码</h2><ul><li>代码涉及到了MapReduce的整个生命周期</li><li><p>每个周期的代码如下</p><blockquote><p>TQJob  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.FileSystem;  import org.apache.hadoop.fs.Path;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.mapreduce.Job;  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  public class TQJob {      public static void main(String[] args) throws Exception {          // 默认加载根目录src目录下的配置文件          Configuration conf = new Configuration();          // 运行方式1：手动指定提交服务器的地址          // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);          // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);          // 运行方式2：使用配置文件指定，屏蔽上面的手动指定,报错的需要指定跨平台，指定本地jar的位置          conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);          conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\tq.jar&quot;);          // 运行方式3：手动上传到服务器上，屏蔽上的面的conf指定          // System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);          // 开始job相关的设定          Job job = Job.getInstance(conf);          // 设定MR主类          job.setJarByClass(TQJob.class);          // 设定mapper          job.setMapperClass(TQMapper.class);          job.setOutputKeyClass(Weather.class);          job.setMapOutputValueClass(IntWritable.class);          // 设定reducer          job.setReducerClass(TQReducer.class);          job.setPartitionerClass(TQPartition.class);          job.setSortComparatorClass(TQSort.class);          job.setGroupingComparatorClass(TQGroup.class);          job.setNumReduceTasks(3);          // 要分析的文件          FileInputFormat.addInputPath(job, new Path(&quot;/weather/input/tq.txt&quot;));          // 输出路径          Path outPath = new Path(&quot;/weather/output&quot;);          FileSystem fs = FileSystem.get(conf);          if (fs.exists(outPath)) {              fs.delete(outPath, true);          }          FileOutputFormat.setOutputPath(job, outPath);          // 提交job作业          boolean flag = job.waitForCompletion(true);          if (flag) {              System.out.println(&quot;job commit successfully...&quot;);          }      }  }</code></pre><blockquote><p>TQMapper  </p></blockquote><pre><code>  package com.hikvision.tq;  import java.io.IOException;  import java.text.ParseException;  import java.text.SimpleDateFormat;  import java.util.Calendar;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.LongWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Mapper;  import org.apache.hadoop.util.StringUtils;  public class TQMapper extends Mapper&lt;LongWritable, Text, Weather, IntWritable&gt; {      @Override      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String[] strs = StringUtils.split(value.toString(), &#39;\t&#39;);          SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);          Calendar cal = Calendar.getInstance();          try {              cal.setTime(sdf.parse(strs[0]));              Weather weather = new Weather();              weather.setYear(cal.get(Calendar.YEAR));              weather.setMonth(cal.get(Calendar.MONTH) + 1);              weather.setDay(cal.get(Calendar.DAY_OF_MONTH));              int temperature = Integer.parseInt(strs[1].substring(0, strs[1].lastIndexOf(&#39;c&#39;)));              weather.setTemperature(temperature);              context.write(weather, new IntWritable(temperature));          } catch (ParseException e) {              e.printStackTrace();          }      }  }</code></pre><blockquote><p>TQPartition  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;  public class TQPartition extends HashPartitioner&lt;Weather, IntWritable&gt; {      @Override      public int getPartition(Weather key, IntWritable value, int numReduceTasks) {          // 重写partition的过程，需要满足业务的规则          // 越简单越好，会影响计算速度          return (key.getYear() - 1949) % numReduceTasks;          // return super.getPartition(key, value, numReduceTasks);      }  }</code></pre><blockquote><p>TQSort  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.io.WritableComparable;  import org.apache.hadoop.io.WritableComparator;  public class TQSort extends WritableComparator {      // 重写构造方法      public TQSort() {          super(Weather.class, true);      }      @SuppressWarnings(&quot;rawtypes&quot;)      @Override      public int compare(WritableComparable a, WritableComparable b) {          Weather w1 = (Weather) a;          Weather w2 = (Weather) b;          int c1 = Integer.compare(w1.getYear(), w2.getYear());          if (c1 == 0) {              int c2 = Integer.compare(w1.getMonth(), w2.getMonth());              if (c2 == 0) {                  return -Integer.compare(w1.getTemperature(), w2.getTemperature());              }              return c2;          }          return c1;      }  }</code></pre><blockquote><p>TQGroup  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.io.WritableComparable;  import org.apache.hadoop.io.WritableComparator;  public class TQGroup extends WritableComparator {      public TQGroup() {          super(Weather.class, true);      }      @SuppressWarnings(&quot;rawtypes&quot;)      @Override      public int compare(WritableComparable a, WritableComparable b) {          Weather w1 = (Weather) a;          Weather w2 = (Weather) b;          int c1 = Integer.compare(w1.getYear(), w2.getYear());          if (c1 == 0) {              return Integer.compare(w1.getMonth(), w2.getMonth());          }          return c1;      }  }</code></pre><blockquote><p>TQReducer  </p></blockquote><pre><code>  package com.hikvision.tq;  import java.io.IOException;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.NullWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Reducer;  public class TQReducer extends Reducer&lt;Weather, IntWritable, Text, NullWritable&gt; {      @Override      protected void reduce(Weather weather, Iterable&lt;IntWritable&gt; iterable, Context context) throws IOException, InterruptedException {          int flag = 0;          for (IntWritable i : iterable) {              flag++;              if (flag &gt; 2) {                  break;              }              String msg = weather.getYear() + &quot;-&quot; + weather.getMonth() + &quot;-&quot; + weather.getDay() + &quot;-&quot; + i.get();              context.write(new Text(msg), NullWritable.get());          }      }  }</code></pre></li></ul><h2 id="提交job操作"><a href="#提交job操作" class="headerlink" title="提交job操作"></a>提交job操作</h2><ul><li>新建文件夹，上传文件到hdfs</li><li>已经有了配置文件所以，使用方式2进行提交，需要手动打包，直接运行<pre><code>  // 运行方式2：使用配置文件指定，屏蔽上面的手动指定,报错的需要指定跨平台，指定本地jar的位置  conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\tq.jar&quot;);</code></pre></li></ul><h2 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h2><ul><li><p>控制台日志</p><pre><code>  2018-05-02 15:22:31,473 WARN  mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.  2018-05-02 15:22:34,937 INFO  input.FileInputFormat (FileInputFormat.java:listStatus(283)) - Total input paths to process : 1  2018-05-02 15:22:35,103 INFO  mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:1  2018-05-02 15:22:35,119 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - mapred.jar is deprecated. Instead, use mapreduce.job.jar  2018-05-02 15:22:35,237 INFO  mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1525168992297_0007  2018-05-02 15:22:35,674 INFO  impl.YarnClientImpl (YarnClientImpl.java:submitApplication(273)) - Submitted application application_1525168992297_0007  2018-05-02 15:22:35,825 INFO  mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://n3:8088/proxy/application_1525168992297_0007/  2018-05-02 15:22:35,826 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1525168992297_0007  2018-05-02 15:22:48,438 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1525168992297_0007 running in uber mode : false  2018-05-02 15:22:48,439 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%  2018-05-02 15:22:58,537 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%  2018-05-02 15:23:14,820 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 33%  2018-05-02 15:23:15,837 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 100%  2018-05-02 15:23:16,853 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_1525168992297_0007 completed successfully  2018-05-02 15:23:17,092 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 50      File System Counters          FILE: Number of bytes read=238          FILE: Number of bytes written=501967          FILE: Number of read operations=0          FILE: Number of large read operations=0          FILE: Number of write operations=0          HDFS: Number of bytes read=347          HDFS: Number of bytes written=101          HDFS: Number of read operations=12          HDFS: Number of large read operations=0          HDFS: Number of write operations=6      Job Counters           Killed reduce tasks=1          Launched map tasks=1          Launched reduce tasks=3          Data-local map tasks=1          Total time spent by all maps in occupied slots (ms)=7783          Total time spent by all reduces in occupied slots (ms)=43298          Total time spent by all map tasks (ms)=7783          Total time spent by all reduce tasks (ms)=43298          Total vcore-milliseconds taken by all map tasks=7783          Total vcore-milliseconds taken by all reduce tasks=43298          Total megabyte-milliseconds taken by all map tasks=7969792          Total megabyte-milliseconds taken by all reduce tasks=44337152      Map-Reduce Framework          Map input records=10          Map output records=10          Map output bytes=200          Map output materialized bytes=238          Input split bytes=96          Combine input records=0          Combine output records=0          Reduce input groups=5          Reduce shuffle bytes=238          Reduce input records=10          Reduce output records=8          Spilled Records=20          Shuffled Maps =3          Failed Shuffles=0          Merged Map outputs=3          GC time elapsed (ms)=498          CPU time spent (ms)=8400          Physical memory (bytes) snapshot=570830848          Virtual memory (bytes) snapshot=7779991552          Total committed heap usage (bytes)=175120384      Shuffle Errors          BAD_ID=0          CONNECTION=0          IO_ERROR=0          WRONG_LENGTH=0          WRONG_MAP=0          WRONG_REDUCE=0      File Input Format Counters           Bytes Read=251      File Output Format Counters           Bytes Written=101  job commit successfully...</code></pre></li><li><p>webUI和输出的目录</p><blockquote><p>输出目录下分为三个文件,和设定的三个reducer有关  </p><pre><code>  1949-10-2-36  1949-10-1-34  #  1950-1-1-32  1950-10-2-41  1950-10-1-37  #  1951-7-3-47  1951-7-2-46  1951-12-1-23</code></pre><p>查看webUI <a href="http://n3:8088/cluster" target="_blank" rel="noopener">http://n3:8088/cluster</a> 显示已经执行成功</p></blockquote></li></ul>]]></content>
      
      <categories>
          
          <category> 开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> mapreduce </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>工欲善其事必先利其器(持续更新中)</title>
      <link href="/2018/05/01/%E5%B7%A5%E6%AC%B2%E5%96%84%E5%85%B6%E4%BA%8B%E5%BF%85%E5%85%88%E5%88%A9%E5%85%B6%E5%99%A8/"/>
      <url>/2018/05/01/%E5%B7%A5%E6%AC%B2%E5%96%84%E5%85%B6%E4%BA%8B%E5%BF%85%E5%85%88%E5%88%A9%E5%85%B6%E5%99%A8/</url>
      <content type="html"><![CDATA[<p>本文介绍所有的优化配置过程，设计到操作系统，开发工具，常用的小工具等。</p><a id="more"></a><h2 id="win10-系统优化"><a href="#win10-系统优化" class="headerlink" title="win10 系统优化"></a>win10 系统优化</h2><ul><li><p>安装win10:</p><blockquote><p>搜索关键词：msdn我告诉你，下载原生的win10系统<br>下载魔方，用魔方组件将ISO文件写入到U盘<br>重启，开机即可安装win10<br>用户名可以设置为root，很多开发场景中需要windows的用户名为root  </p></blockquote></li><li><p>安装驱动：</p><blockquote><p>360官网 &gt; 驱动大师<br>安装驱动大师 &gt; 安装驱动</p></blockquote></li><li><p>安装office 2016</p><blockquote><p>搜索关键词：msdn我告诉你，下载office2016<br>安装  </p></blockquote></li><li><p>激活win10 office2016</p><blockquote><p>小马激活OEM10.exe激活win10<br>KMSpico_setup.exe激活office 2016  </p></blockquote></li><li><p>关闭/打开windows BISO启动入口</p><blockquote><p>控制面板\硬件和声音\电源选项\系统设置<br>更改不可用设置 &gt; 启用快速启动  </p></blockquote></li><li><p>关闭掉windows update</p><blockquote><p>在中文输入法下 &gt; 按win键 &gt; 输入服务<br>windows update &gt; 停止 &gt; 禁止启动  </p></blockquote></li><li><p>关闭掉windows defender</p><blockquote><p>设置 &gt; 更新和安全 &gt; Windows Defender &gt; 关闭实时保护  </p></blockquote></li><li><p>关闭windows 防火墙</p><blockquote><p>计算机右键属性 &gt; windows防火墙 &gt; 启用和关闭windows防火墙  </p></blockquote></li><li><p>备份系统</p><blockquote><p><a href="http://www.uqidong.com/" target="_blank" rel="noopener">http://www.uqidong.com/</a> 下载U启动工具，制作U盘启动<br>查找电脑主板 &gt; 查找进入bios的方法 &gt; 重启按键 &gt; 从U盘启动<br>选择备份系统 &gt; 备份C盘  </p></blockquote></li></ul><h2 id="windows-常用的工具优化"><a href="#windows-常用的工具优化" class="headerlink" title="windows 常用的工具优化"></a>windows 常用的工具优化</h2><ul><li><p>everything+wox 实现mac的sportlight搜索框功能</p></li><li><p>autohotkey实现workflow功能</p><blockquote><p>ahk常用的快捷键  </p></blockquote></li></ul><table><thead><tr><th>ahk</th><th>win</th></tr></thead><tbody><tr><td>^</td><td>Ctrl键</td></tr><tr><td>+</td><td>Shift键</td></tr><tr><td>!</td><td>Alt键</td></tr><tr><td>井</td><td>Win键</td></tr><tr><td>Up</td><td>上箭头键</td></tr><tr><td>Down</td><td>下箭头键</td></tr><tr><td>Left</td><td>左箭头键</td></tr><tr><td>Right</td><td>右箭头键</td></tr><tr><td>PgUp</td><td>PageUp键</td></tr><tr><td>PgDn</td><td>PageDn键</td></tr><tr><td>F1-F12</td><td>功能键</td></tr><tr><td>a-z</td><td>a-z键</td></tr><tr><td>LButton</td><td>鼠标左键</td></tr><tr><td>RButton</td><td>鼠标右键</td></tr><tr><td>MButton</td><td>鼠标中键</td></tr><tr><td>WheelUp</td><td>鼠标滑轮向上</td></tr><tr><td>WheelDown</td><td>鼠标滑轮向下</td></tr><tr><td>Del</td><td>Del删除</td></tr><tr><td>Enter</td><td>Enter回车</td></tr><tr><td>Tab</td><td>Table制表符</td></tr><tr><td>Space</td><td>Space空格</td></tr></tbody></table><h2 id="eclipse优化"><a href="#eclipse优化" class="headerlink" title="eclipse优化"></a>eclipse优化</h2><ul><li>eclipse快捷键 <a href="https://www.cnblogs.com/zhangqie/p/6432477.html" target="_blank" rel="noopener">参考</a></li></ul><table><thead><tr><th>key</th><th>meaning</th></tr></thead><tbody><tr><td>ctrl+o</td><td>打开类</td></tr><tr><td>alt+shift+s v</td><td>重写方法</td></tr><tr><td>alt+shift+s</td><td>source菜单</td></tr><tr><td>alt+shift+l</td><td>补全变量名</td></tr><tr><td>ctrl+2+l</td><td>补全变量名</td></tr></tbody></table><ul><li><p>依赖包的导入方式</p><blockquote><p>手动创建lib，然后导入<br>添加用户依赖  </p></blockquote></li><li><p>设置代码显示最大宽度</p><blockquote><p>Window &gt; Preferences &gt; Java &gt; Code Style &gt; Formatter &gt; new Profile<br>Line Wrapping &gt; Maximum line width  </p></blockquote></li><li><p>eclipse插件</p><ol><li><p>maven插件M2Eclipse</p><blockquote><p>找到maven链接 <a href="http://www.eclipse.org/m2e/" target="_blank" rel="noopener">http://www.eclipse.org/m2e/</a><br>软件 &gt; 安装其他软件  </p></blockquote></li><li><p>hadoop插件</p><blockquote><p>连接远程hadoop集群，需要本地的windows用户为root用户  </p></blockquote></li></ol></li></ul><h2 id="ubuntu设置"><a href="#ubuntu设置" class="headerlink" title="ubuntu设置"></a>ubuntu设置</h2><ul><li>启动用户名和密码的登录界面<pre><code>  sudo passwd -u root  sudo passwd root  su root  cd /usr/share/lightdm/lightdm.conf.d/  vim 50-unity-greeter.conf  # 添加  user-session=ubuntu  greeter-show-manual-login=true  all-guest=false  # 重启  reboot  # 使用user和passwd进入root报错  vim /root/.profile  # 找到mesg n || true  # 改为tty -s &amp;&amp; mesg n || true</code></pre></li><li><p>更换国内的源(fq除外)</p><pre><code>  sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup  vi /etc/apt/sources.list  #Ubuntu 官方源   deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse  #网易163  deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse  sudo apt-get update  sudo apt-get dist-upgrade</code></pre></li><li><p>desktop桌面版本vm下的ubuntu更改为固定的IP</p><blockquote><p>查看vm的IP地址 &gt; 网段和DNS<br>编辑ubutnu的网卡配置 &gt; 设置在合理的IP范围<br>重启网卡  </p></blockquote></li><li><p>server服务器版本vm下的ubuntu更改为固定的IP</p><pre><code>  vmware &gt; 编辑 &gt; 虚拟网络编辑器 &gt; 点击vmnet8 &gt; NET设置 &gt; 查看由vmnet8设定的子网先关参数 &gt; 也可以自己更改子网的IP和网段  配置固定IP三个步骤  首先ifconfig查看自己ubuntu工作网卡名称，替换下面的eth0  #----------------------------------------  /etc/network/interfaces  原来为：  auto lo  iface lo inet loopback  auto eth0  iface eth0 inet dhcp  #----------------------------------------  改为：  auto lo  iface lo inet loopback  auto eth0  iface eth0 inet static      #定义为静态IP  address 192.168.2.29        #所要设置的IP地址  netmask 255.255.255.0       #子网掩码  gateway 192.168.2.1         #网关（路由地址）  #----------------------------------------  手动设动网关和DNS    /etc/resolv.conf  nameserver 192.168.2.1      #网关（同上）  #nameserver 202.106.0.20    #DNS服务器地址（参考其他电脑，VM上的ubuntu可以不用设定）  #----------------------------------------  永久性更改网关和DNS    /etc/resolvconf/resolv.conf.d/base  nameserver 192.168.2.1      #网关  #nameserver 202.106.0.20    #DNS  #----------------------------------------  /etc/init.d/networking restart  reboot</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop开发代码三种提交方式</title>
      <link href="/2018/05/01/hadoop%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/"/>
      <url>/2018/05/01/hadoop%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/</url>
      <content type="html"><![CDATA[<p>本文介绍如何在本地编写wordcount程序，在远程的hadoop集群上运行测试代码，重点介绍了hadoop本地开发后的三种提交到服务器的方式，分别涉及到了项目开发，项目测试，项目交付时期的不同提交方式。</p><a id="more"></a><h2 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h2><ul><li>新建java项目</li><li><p>添加依赖</p><blockquote><p>maven：下载设置maven，新建maven项目，在maven repository找到坐标<br>手动添加依赖：用户依赖，直接把hadoop/share中lib包逐个引用过来<br>手动添加依赖：lib依赖，将所有的jar放在一个目录下  </p></blockquote></li><li><p>关联源代码：ctrl+单击打开api，管理源码，打开解压后的源码文件夹</p><blockquote><p>JDK基本的源码在JDK安装路径中src.zip<br>hadoop源码在官网下载解压后关联文件夹到eclipse</p></blockquote></li><li><p>代码分为三个部分：主程序设置job任务，mapper，reducer</p></li></ul><ol><li>main<pre><code>package com.hikvision.wc;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WCJob { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {     // 默认加载根目录src目录下的配置文件     Configuration conf = new Configuration();     conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);     conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);     Job job = Job.getInstance(conf);     // 设定MR主类     job.setJarByClass(WCJob.class);     // 设定mapper     job.setMapperClass(WCMapper.class);     job.setOutputKeyClass(Text.class);     job.setMapOutputValueClass(IntWritable.class);     // 设定reducer     job.setReducerClass(WCReducer.class);     // 要分析的文件     FileInputFormat.addInputPath(job, new Path(&quot;/wc/input/data.txt&quot;));     // 输出路径     Path outPath = new Path(&quot;/wc/output&quot;);     FileSystem fs = FileSystem.get(conf);     if (fs.exists(outPath)) {         fs.delete(outPath, true);     }     FileOutputFormat.setOutputPath(job, outPath);     // 提交job作业     boolean flag = job.waitForCompletion(true);     if (flag) {         System.out.println(&quot;job commit successfully...&quot;);     } }}</code></pre></li><li>mapper<pre><code>package com.hikvision.wc;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.util.StringUtils;public class WCMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {     String str = value.toString();     String[] strs = StringUtils.split(str, &#39; &#39;);     for (String s : strs) {         context.write(new Text(s), new IntWritable(1));     } }}</code></pre></li><li>reducer<pre><code>package com.hikvision.wc;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { @Override protected void reduce(Text text, Iterable&lt;IntWritable&gt; iterable, Context context) throws IOException, InterruptedException {     int sum = 0;     for (IntWritable i : iterable) {         sum += i.get();     }     context.write(text, new IntWritable(sum)); }}</code></pre></li></ol><h2 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h2><ul><li>测试数据放在src下</li><li>三种运行方式<blockquote><p>本地测试环境（企业开发过程中测试）<br>提交到服务器（测试代码的执行能力）<br>自动打包<br>手动打包  </p></blockquote></li></ul><h2 id="提交1：不打包本地提交到远程服务器直接执行（功能开发过程）"><a href="#提交1：不打包本地提交到远程服务器直接执行（功能开发过程）" class="headerlink" title="提交1：不打包本地提交到远程服务器直接执行（功能开发过程）"></a>提交1：不打包本地提交到远程服务器直接执行（功能开发过程）</h2><ul><li>没有配置文件+必要的conf设置</li><li><p>本地eclipse提交代码</p><blockquote><p>解压hadoop，配置环境变量<br>安装windows中的hadoop依赖工具win-utils的%HADOOP_HOME%\bin<br>拷贝hadoop源码到自己的工程中，检查包是不是报错<br>配置本地hosts文件<br>启动zkServer.sh start<br>启动start-all.sh(dfs yarn)<br>启动resource manager(RS)<br>测试50070，找到active的dfs节点<br>测试8088，找到active的resourcemanager节点<br>配置本地的hadoop连接<br>创建输入文件夹，上传数据文件<br>配置conf提交地址<br>在main中填写输入输出路径<br>直接以java application运行代码  </p></blockquote></li><li><p>实验结果</p><pre><code>2018-05-01 18:11:01,551 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - session.id is deprecated. Instead, use dfs.metrics.session-id2018-05-01 18:11:01,559 INFO  jvm.JvmMetrics (JvmMetrics.java:init(76)) - Initializing JVM Metrics with processName=JobTracker, sessionId=2018-05-01 18:11:02,328 WARN  mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.2018-05-01 18:11:02,401 WARN  mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(171)) - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).2018-05-01 18:11:02,466 INFO  input.FileInputFormat (FileInputFormat.java:listStatus(283)) - Total input paths to process : 12018-05-01 18:11:02,601 INFO  mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:12018-05-01 18:11:02,776 INFO  mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_local2042689909_00012018-05-01 18:11:03,239 INFO  mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://localhost:8080/2018-05-01 18:11:03,242 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_local2042689909_00012018-05-01 18:11:03,251 INFO  mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(471)) - OutputCommitter set in config null2018-05-01 18:11:03,260 INFO  output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 12018-05-01 18:11:03,265 INFO  mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(489)) - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter2018-05-01 18:11:03,438 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for map tasks2018-05-01 18:11:03,445 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(224)) - Starting task: attempt_local2042689909_0001_m_000000_02018-05-01 18:11:03,493 INFO  output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 12018-05-01 18:11:03,502 INFO  util.ProcfsBasedProcessTree (ProcfsBasedProcessTree.java:isAvailable(192)) - ProcfsBasedProcessTree currently is supported only on Linux.2018-05-01 18:11:03,567 INFO  mapred.Task (Task.java:initialize(614)) -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5b5d72a52018-05-01 18:11:03,574 INFO  mapred.MapTask (MapTask.java:runNewMapper(756)) - Processing split: hdfs://n1:8020/wc/input/data.txt:0+442018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:setEquator(1205)) - (EQUATOR) 0 kvi 26214396(104857584)2018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(998)) - mapreduce.task.io.sort.mb: 1002018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(999)) - soft limit at 838860802018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(1000)) - bufstart = 0; bufvoid = 1048576002018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(1001)) - kvstart = 26214396; length = 65536002018-05-01 18:11:03,671 INFO  mapred.MapTask (MapTask.java:createSortingCollector(403)) - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer2018-05-01 18:11:04,185 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 2018-05-01 18:11:04,189 INFO  mapred.MapTask (MapTask.java:flush(1460)) - Starting flush of map output2018-05-01 18:11:04,190 INFO  mapred.MapTask (MapTask.java:flush(1482)) - Spilling map output2018-05-01 18:11:04,190 INFO  mapred.MapTask (MapTask.java:flush(1483)) - bufstart = 0; bufend = 70; bufvoid = 1048576002018-05-01 18:11:04,191 INFO  mapred.MapTask (MapTask.java:flush(1485)) - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/65536002018-05-01 18:11:04,212 INFO  mapred.MapTask (MapTask.java:sortAndSpill(1667)) - Finished spill 02018-05-01 18:11:04,217 INFO  mapred.Task (Task.java:done(1046)) - Task:attempt_local2042689909_0001_m_000000_0 is done. And is in the process of committing2018-05-01 18:11:04,234 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - map2018-05-01 18:11:04,234 INFO  mapred.Task (Task.java:sendDone(1184)) - Task &#39;attempt_local2042689909_0001_m_000000_0&#39; done.2018-05-01 18:11:04,242 INFO  mapred.Task (Task.java:done(1080)) - Final Counters for attempt_local2042689909_0001_m_000000_0: Counters: 22  File System Counters      FILE: Number of bytes read=150      FILE: Number of bytes written=335288      FILE: Number of read operations=0      FILE: Number of large read operations=0      FILE: Number of write operations=0      HDFS: Number of bytes read=44      HDFS: Number of bytes written=0      HDFS: Number of read operations=6      HDFS: Number of large read operations=0      HDFS: Number of write operations=1  Map-Reduce Framework      Map input records=8      Map output records=8      Map output bytes=70      Map output materialized bytes=92      Input split bytes=97      Combine input records=0      Spilled Records=8      Failed Shuffles=0      Merged Map outputs=0      GC time elapsed (ms)=0      Total committed heap usage (bytes)=233832448  File Input Format Counters       Bytes Read=442018-05-01 18:11:04,243 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(249)) - Finishing task: attempt_local2042689909_0001_m_000000_02018-05-01 18:11:04,243 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.2018-05-01 18:11:04,246 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for reduce tasks2018-05-01 18:11:04,249 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(302)) - Starting task: attempt_local2042689909_0001_r_000000_02018-05-01 18:11:04,249 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_local2042689909_0001 running in uber mode : false2018-05-01 18:11:04,250 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%2018-05-01 18:11:04,259 INFO  output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 12018-05-01 18:11:04,260 INFO  util.ProcfsBasedProcessTree (ProcfsBasedProcessTree.java:isAvailable(192)) - ProcfsBasedProcessTree currently is supported only on Linux.2018-05-01 18:11:04,317 INFO  mapred.Task (Task.java:initialize(614)) -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@368baef32018-05-01 18:11:04,328 INFO  mapred.ReduceTask (ReduceTask.java:run(362)) - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@42ca90ce2018-05-01 18:11:04,347 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:&lt;init&gt;(205)) - MergerManager: memoryLimit=1987680640, maxSingleShuffleLimit=496920160, mergeThreshold=1311869312, ioSortFactor=10, memToMemMergeOutputsThreshold=102018-05-01 18:11:04,349 INFO  reduce.EventFetcher (EventFetcher.java:run(61)) - attempt_local2042689909_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events2018-05-01 18:11:04,428 INFO  reduce.LocalFetcher (LocalFetcher.java:copyMapOutput(144)) - localfetcher#1 about to shuffle output of map attempt_local2042689909_0001_m_000000_0 decomp: 88 len: 92 to MEMORY2018-05-01 18:11:04,446 INFO  reduce.InMemoryMapOutput (InMemoryMapOutput.java:shuffle(100)) - Read 88 bytes from map-output for attempt_local2042689909_0001_m_000000_02018-05-01 18:11:04,455 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:closeInMemoryFile(319)) - closeInMemoryFile -&gt; map-output of size: 88, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;882018-05-01 18:11:04,461 INFO  reduce.EventFetcher (EventFetcher.java:run(76)) - EventFetcher is interrupted.. Returning2018-05-01 18:11:04,462 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied.2018-05-01 18:11:04,462 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(691)) - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs2018-05-01 18:11:04,482 INFO  mapred.Merger (Merger.java:merge(606)) - Merging 1 sorted segments2018-05-01 18:11:04,482 INFO  mapred.Merger (Merger.java:merge(705)) - Down to the last merge-pass, with 1 segments left of total size: 82 bytes2018-05-01 18:11:04,483 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(758)) - Merged 1 segments, 88 bytes to disk to satisfy reduce memory limit2018-05-01 18:11:04,484 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(788)) - Merging 1 files, 92 bytes from disk2018-05-01 18:11:04,484 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(803)) - Merging 0 segments, 0 bytes from memory into reduce2018-05-01 18:11:04,485 INFO  mapred.Merger (Merger.java:merge(606)) - Merging 1 sorted segments2018-05-01 18:11:04,486 INFO  mapred.Merger (Merger.java:merge(705)) - Down to the last merge-pass, with 1 segments left of total size: 82 bytes2018-05-01 18:11:04,486 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied.2018-05-01 18:11:04,546 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords2018-05-01 18:11:04,924 INFO  mapred.Task (Task.java:done(1046)) - Task:attempt_local2042689909_0001_r_000000_0 is done. And is in the process of committing2018-05-01 18:11:04,935 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied.2018-05-01 18:11:04,935 INFO  mapred.Task (Task.java:commit(1225)) - Task attempt_local2042689909_0001_r_000000_0 is allowed to commit now2018-05-01 18:11:05,044 INFO  output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task &#39;attempt_local2042689909_0001_r_000000_0&#39; to hdfs://n1:8020/wc/output/_temporary/0/task_local2042689909_0001_r_0000002018-05-01 18:11:05,051 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - reduce &gt; reduce2018-05-01 18:11:05,052 INFO  mapred.Task (Task.java:sendDone(1184)) - Task &#39;attempt_local2042689909_0001_r_000000_0&#39; done.2018-05-01 18:11:05,060 INFO  mapred.Task (Task.java:done(1080)) - Final Counters for attempt_local2042689909_0001_r_000000_0: Counters: 29  File System Counters      FILE: Number of bytes read=366      FILE: Number of bytes written=335380      FILE: Number of read operations=0      FILE: Number of large read operations=0      FILE: Number of write operations=0      HDFS: Number of bytes read=44      HDFS: Number of bytes written=32      HDFS: Number of read operations=9      HDFS: Number of large read operations=0      HDFS: Number of write operations=3  Map-Reduce Framework      Combine input records=0      Combine output records=0      Reduce input groups=5      Reduce shuffle bytes=92      Reduce input records=8      Reduce output records=5      Spilled Records=8      Shuffled Maps =1      Failed Shuffles=0      Merged Map outputs=1      GC time elapsed (ms)=28      Total committed heap usage (bytes)=267386880  Shuffle Errors      BAD_ID=0      CONNECTION=0      IO_ERROR=0      WRONG_LENGTH=0      WRONG_MAP=0      WRONG_REDUCE=0  File Output Format Counters       Bytes Written=322018-05-01 18:11:05,060 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(325)) - Finishing task: attempt_local2042689909_0001_r_000000_02018-05-01 18:11:05,061 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - reduce task executor complete.2018-05-01 18:11:05,253 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 100%2018-05-01 18:11:06,254 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_local2042689909_0001 completed successfully2018-05-01 18:11:06,267 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 35  File System Counters      FILE: Number of bytes read=516      FILE: Number of bytes written=670668      FILE: Number of read operations=0      FILE: Number of large read operations=0      FILE: Number of write operations=0      HDFS: Number of bytes read=88      HDFS: Number of bytes written=32      HDFS: Number of read operations=15      HDFS: Number of large read operations=0      HDFS: Number of write operations=4  Map-Reduce Framework      Map input records=8      Map output records=8      Map output bytes=70      Map output materialized bytes=92      Input split bytes=97      Combine input records=0      Combine output records=0      Reduce input groups=5      Reduce shuffle bytes=92      Reduce input records=8      Reduce output records=5      Spilled Records=16      Shuffled Maps =1      Failed Shuffles=0      Merged Map outputs=1      GC time elapsed (ms)=28      Total committed heap usage (bytes)=501219328  Shuffle Errors      BAD_ID=0      CONNECTION=0      IO_ERROR=0      WRONG_LENGTH=0      WRONG_MAP=0      WRONG_REDUCE=0  File Input Format Counters       Bytes Read=44  File Output Format Counters       Bytes Written=32job commit successfully...</code></pre></li><li><p>查看结果</p><pre><code>aaa    2apple    3ooo    1ttt    1yyy    1</code></pre></li></ul><h2 id="提交2：打包到本地，直接运行（项目性能测试）"><a href="#提交2：打包到本地，直接运行（项目性能测试）" class="headerlink" title="提交2：打包到本地，直接运行（项目性能测试）"></a>提交2：打包到本地，直接运行（项目性能测试）</h2><ul><li>配置文件+必要的conf设置</li><li>使用配置文件<blockquote><p>拷贝四个配置文件到源代码下面  </p><pre><code>  core-site.xml  hdfs-site.xml  mapred-site.xml  yarn-site.xml</code></pre><p>在主代码中指定打包的位置，并注释掉代码配置  </p><pre><code>  // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);  // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);  conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;);</code></pre><p>手动打jar：项目右键 &gt; export &gt; &gt; java jar file &gt; 指定路径<br>选择main calss<br>运行下java程序<br>刷新 <a href="http://n3:8088/cluster" target="_blank" rel="noopener">http://n3:8088/cluster</a> 是不是有新的任务<br>运行任务时，常见的报错如下<br><a href="https://stackoverflow.com/questions/24075669/mapreduce-job-fail-when-submitted-from-windows-machine" target="_blank" rel="noopener">解决链接1:conf设置夸平台提交</a><br><a href="http://zy19982004.iteye.com/blog/2031172" target="_blank" rel="noopener">解决链接2:修改源码</a>  </p><pre><code>  问题：  org.apache.hadoop.util.Shell$ExitCodeException: /bin/bash: line 0: fg: no job control  解决1：  这是由于跨平台造成的，经过Google搜索口在StackOverflow上发现答案  conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  解决2：  需要手动修改hadoop源码，目前还没有弄明白</code></pre><p>处理后，现在的main如下  </p><pre><code>  // 默认加载根目录src目录下的配置文件  Configuration conf = new Configuration();  // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);  // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);  conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;);</code></pre><p>以java application的方式运行程序<br>在控制台和active节点 <a href="http://n3:8088/cluster" target="_blank" rel="noopener">http://n3:8088/cluster</a> 查看运行的日志，WebUI会显示提交的任务<br>在eclipse-hadoop文件系统查看是不是有数据输出  </p></blockquote></li></ul><h2 id="提交3：打包到本地，手动上传（项目上线时候）"><a href="#提交3：打包到本地，手动上传（项目上线时候）" class="headerlink" title="提交3：打包到本地，手动上传（项目上线时候）"></a>提交3：打包到本地，手动上传（项目上线时候）</h2><ul><li>配置文件+删除不必要的conf设置</li><li>屏蔽不必要的cond指定<pre><code>  // 默认加载根目录src目录下的配置文件  Configuration conf = new Configuration();  // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);  // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);  // conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  // conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;);</code></pre></li><li>手动打包，指定main class</li><li>通过命令运行程序<pre><code>  hadoop jar jar路径 程序的入口(主程序的入口)  hadoop jar wc-1.jar com.hikvision.wc.WCJob</code></pre></li><li>查看输出和webUI信息</li></ul>]]></content>
      
      <categories>
          
          <category> 程序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> 程序 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PE hadoop高可用集群搭建总结</title>
      <link href="/2018/04/30/PE%20hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%80%BB%E7%BB%93/"/>
      <url>/2018/04/30/PE%20hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>本文简单记录生产环境中的hadoop高可用集群搭建总结。</p><a id="more"></a><h2 id="hdfs-ha理论总结"><a href="#hdfs-ha理论总结" class="headerlink" title="hdfs ha理论总结"></a>hdfs ha理论总结</h2><ul><li>NN内存受到限制：federation</li><li>NN单点故障：edits和fsimage原理，在没有格式化的NN上standby操作</li><li>edits记录日志文件，保障一致性，交给第三方JNs来管理，奇数个，不用NFS不可靠</li><li>zookeeper，奇数个，它的ZKFC心跳监控NN；管理和切换zkfc</li></ul><h2 id="yarn-ha搭建过程总结"><a href="#yarn-ha搭建过程总结" class="headerlink" title="yarn ha搭建过程总结"></a>yarn ha搭建过程总结</h2><ul><li>删除所有的临时文件夹(重要)</li><li>启动zk集群，三台，最好在xshell下面一起启动，并且查看状态status</li><li>edits交给JN,手动启动三个JN</li><li>格式化一台NN，并启动这一台(只能格式化一次，不然重新来)</li><li>另外一台NN执行同步standby</li><li>格式化zk</li><li>启动dfs(可以直接全部启动)</li><li>最后启动RS<pre><code>rm -rdf /opt/hadoop/* /opt/journal/* /opt/zookeeper/v* /opt/zookeeper/z*zkServer.sh start(xshell同时启动)vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode)hadoop-daemon.sh start journalnode(根据hdfs配置文件，n2 n3 n4)hdfs namenode -format(选一个NN格式化，一次机会，失败重头再来)hadoop-daemon.sh start namenode(格式化的NN上)hdfs namenode -bootstrapStandby(n2,没有格式化的NN上)hdfs zkfc -formatZK(n1)start-all.sh(n1)yarn-daemon.sh start resourcemanager(指定的机器，n3 n4)</code></pre></li></ul><h2 id="mr-yarn理论"><a href="#mr-yarn理论" class="headerlink" title="mr yarn理论"></a>mr yarn理论</h2><ul><li>四个阶段：split，map,shuffle,reduce, 最小的MR程序包含了前面两个</li><li>split块</li><li>map task按行读&lt;K,V&gt;排序</li><li>分区partition，memory buffer，sort &amp; combine（手动实现，不能代替reduce，可以提升效率）</li><li>reduce&lt;K,V&gt;迭代器取数据</li></ul><h2 id="整合mr到hadoop，实现RS-ha"><a href="#整合mr到hadoop，实现RS-ha" class="headerlink" title="整合mr到hadoop，实现RS ha"></a>整合mr到hadoop，实现RS ha</h2><ul><li>先停止hdfs</li><li>配置mr，配置yarn高可用</li><li>resource manager需要单独启动</li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> 配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>本地eclipse链接远程hadoop编写hdfs测试代码</title>
      <link href="/2018/04/30/%E6%9C%AC%E5%9C%B0eclipse%E9%93%BE%E6%8E%A5%E8%BF%9C%E7%A8%8Bhadoop%E7%BC%96%E5%86%99hdfs%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81/"/>
      <url>/2018/04/30/%E6%9C%AC%E5%9C%B0eclipse%E9%93%BE%E6%8E%A5%E8%BF%9C%E7%A8%8Bhadoop%E7%BC%96%E5%86%99hdfs%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81/</url>
      <content type="html"><![CDATA[<p>本文介绍如何使用eclipse插件连接远程的hadoop，并且编写hdfs测试代码。</p><a id="more"></a><h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><ul><li>local: win10 + eclipse neon + hadoop-eclipse-2.7.3.jar</li><li>remote: hadoop 2.7.6 + hdfs namenode ha 架构 + yarn resource manager ha 架构</li></ul><h2 id="本地eclipse连接远程的hadoop集群"><a href="#本地eclipse连接远程的hadoop集群" class="headerlink" title="本地eclipse连接远程的hadoop集群"></a>本地eclipse连接远程的hadoop集群</h2><ul><li>后面所有开发的前提是windows用户名必须为root，win+x+G &gt; 本地用户和组 &gt; 用户 &gt; 修改系统的用户名为root</li><li>上面的步骤，我没有成功，后来重新安装系统设置为root用户</li><li>安装hadoop-eclipse插件，copy jar到插件目录，启动，选择windows &gt; show view &gt; other &gt; hadoop/mr</li><li>需要在windows本地安装配置和服务器相同版本的hadoop，并且下载关键字为&lt;hadoo2.7.3的hadoop.dll和winutils.exe&gt;的组件放在本地hadoop/bin下</li><li>在MR locations窗口下配置连接远程hadoop服务器，其中dfs master的端口为 50070 webUI能够访问的且显示为active的端口</li><li>比如：我的n2:50070 webUI上面显示8020端口 active, 所以hadoop配置为8020</li><li>暂时没有用到MR，所以默认就好</li><li>完成上述步骤即可连接远程服务器成功</li><li>可以创建文件进行测试：eclipse窗口创建，webUI显示，或者终端上使用hdfs shell查看文件列表</li></ul><h2 id="创建项目手动导入依赖包"><a href="#创建项目手动导入依赖包" class="headerlink" title="创建项目手动导入依赖包"></a>创建项目手动导入依赖包</h2><ul><li>创建项目，直接常见一个Java项目；不着急创建hadoop/mapreduce项目</li><li>手动添加依赖；不着急使用maven构建项目</li><li>build path &gt; libraries窗口 &gt; add library &gt; user library &gt; user libraries &gt; new</li><li>name随意hadoop2.7.6 &gt; and external jars</li><li>导入hadoop安装路径下面的C:\app2\hadoop-2.7.6\share\hadoop下面全部组件的lib下面的所有jars</li><li>最后可以看到窗口有了外部依赖hadoop2.7.6</li><li>最后还需要以上面的方式导入junit测试包</li></ul><h2 id="编写连接hdfs的测试代码"><a href="#编写连接hdfs的测试代码" class="headerlink" title="编写连接hdfs的测试代码"></a>编写连接hdfs的测试代码</h2><ul><li><p>代码文件为/hdfs-test/src/com/hik/hdfs/HdfsDemo.java，下面的代码包含了连接, 创建文件夹，上传下载文件，合并小文件，下载小文件等操作，简单实现网盘的功能</p><pre><code>package com.hik.hdfs;import java.io.File;import java.io.FileNotFoundException;import java.io.IOException;import org.apache.commons.io.FileUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.SequenceFile;import org.apache.hadoop.io.SequenceFile.Writer;import org.apache.hadoop.io.Text;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HdfsDemo {  FileSystem fs;  Configuration configuration;  @Before  public void begin() throws Exception {      // load configuration file from src      configuration = new Configuration();      // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n2:8020&quot;);      fs = FileSystem.get(configuration);  }  @After  public void end() throws Exception {      fs.close();  }  @Test  public void mkdir() throws Exception {      Path path = new Path(&quot;/tmp_innerConfig&quot;);      boolean newdir = fs.mkdirs(path);      System.out.println(newdir);  }  @Test  public void upload() throws IOException {      Path path = new Path(&quot;/tmp/a2.txt&quot;);      FSDataOutputStream outputStream = fs.create(path);      FileUtils.copyFile(new File(&quot;D:\\test.txt&quot;), outputStream);  }  @Test  public void list() throws FileNotFoundException, IOException {      Path path = new Path(&quot;/tmp/&quot;);      FileStatus[] listStatus = fs.listStatus(path);      for (FileStatus x : listStatus) {          String str = x.getPath() + &quot;--&quot; + x.getLen() + &quot;--&quot; + x.getAccessTime();          System.out.println(str);      }  }  @Test  public void uploadSmalltoBig() throws Exception {      Path path = new Path(&quot;/seq.txt&quot;);      @SuppressWarnings(&quot;deprecation&quot;)      Writer writer = SequenceFile.createWriter(fs, configuration, path, Text.class, Text.class);      File file = new File(&quot;D:\\file&quot;);      for (File f : file.listFiles()) {          Text name = new Text(f.getName());          Text content = new Text(FileUtils.readFileToString(f, &quot;UTF-8&quot;));          writer.append(name, content);      }  }  @Test  public void downloadBig() throws Exception {      Path path = new Path(&quot;/seq.txt&quot;);      @SuppressWarnings({ &quot;resource&quot;, &quot;deprecation&quot; })      SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, configuration);      Text key = new Text();      Text value = new Text();      while (reader.next(key, value)) {          System.out.println(key);          System.out.println(value);      }  }}</code></pre></li></ul><h2 id="运行测试程序"><a href="#运行测试程序" class="headerlink" title="运行测试程序"></a>运行测试程序</h2><ul><li>配置本地windows的hosts文件，直接用everything搜索找到c盘下面的hosts,C:\Windows\System32\drivers\etc\hosts, 加入ip和主机名的映射关系<pre><code>192.168.44.100 n1192.168.44.101 n2192.168.44.102 n3192.168.44.103 n4</code></pre></li><li>运行代码前，需要导入连接先关的配置文件，将hadoop/etc/hadoop下的hdfs-site.xml和core-site.xml拷贝到src文件夹下，直接在eclipse里面粘贴</li><li>上面的步骤也可以在程序的configuration中设置对应的rpc接口地址</li><li>直接在函数名上右键run as &gt; junit test 即可</li><li>如果成功绿色状态，查看hdfs文件系统是不是生成相应的文件</li></ul>]]></content>
      
      <categories>
          
          <category> 代码 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> hdfs </tag>
            
            <tag> eclipse </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PE mr configuration and yarn resource manager ha</title>
      <link href="/2018/04/30/PE%20mr%20configuration%20and%20yarn%20resource%20manager%20ha/"/>
      <url>/2018/04/30/PE%20mr%20configuration%20and%20yarn%20resource%20manager%20ha/</url>
      <content type="html"><![CDATA[<p>本文介绍如何将MR计算模型集成到hdfs ha中，并且实现yarn的RS的高可用。</p><a id="more"></a><h2 id="集群配置需求"><a href="#集群配置需求" class="headerlink" title="集群配置需求"></a>集群配置需求</h2><ul><li>集群配置地点<pre><code>  NN  DN  JN  ZK  ZKFC    RSn1  1           1   1n2  1   1   1   1   1n3      1   1   1           1n4      1   1               1</code></pre></li></ul><h2 id="MR配置，yarn-RS-ha配置"><a href="#MR配置，yarn-RS-ha配置" class="headerlink" title="MR配置，yarn RS ha配置"></a>MR配置，yarn RS ha配置</h2><ul><li><p>mapred-site.xml</p><pre><code>vim mapred-site.xml &lt;configuration&gt;      &lt;property&gt;              &lt;name&gt;mapreduce.framework.name&lt;/name&gt;              &lt;value&gt;yarn&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>yarn-site.xml</p><pre><code>&lt;configuration&gt;      &lt;!-- 配置MR --&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;      &lt;/property&gt;      &lt;!-- resourcemanager高可用ha --&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;        &lt;value&gt;sxt2yarn&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;        &lt;value&gt;rm1,rm2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;        &lt;value&gt;n3&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;        &lt;value&gt;n4&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;        &lt;value&gt;n3:8088&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;        &lt;value&gt;n4:8088&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置zk集群 --&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;        &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>同步到其他机器上</p><pre><code>scp ./*.xml n2:`pwd`scp ./*.xml n3:`pwd`scp ./*.xml n4:`pwd`</code></pre></li></ul><h2 id="MR启动，查看yarn-RS-ha"><a href="#MR启动，查看yarn-RS-ha" class="headerlink" title="MR启动，查看yarn RS ha"></a>MR启动，查看yarn RS ha</h2><ul><li><p>停掉所有</p><pre><code>zkServer.sh stopstop-dfs.shkill -9 pid</code></pre></li><li><p>启动zk</p><pre><code>root@n1:~# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower</code></pre></li><li><p>启动所有</p><pre><code>root@n1:~# start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.shStarting namenodes on [n1 n2]n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.outn2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.outn4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outStarting journal nodes [n2 n3 n4]n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outn3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.outn4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.outStarting ZK Failover Controllers on NN hosts [n1 n2]n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outn2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.outstarting yarn daemonsstarting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n1.outn3: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n3.outn2: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n2.outn4: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n4.out</code></pre></li><li><p>查看进程</p><pre><code>root@n1:~# jps6162 Jps5363 QuorumPeerMain5923 DFSZKFailoverController5578 NameNode#root@n2:~# jps5600 NodeManager5715 Jps5044 NameNode5157 DataNode5448 DFSZKFailoverController4920 QuorumPeerMain5294 JournalNode#root@n3:~# jps3856 Jps3746 NodeManager3477 DataNode3609 JournalNode3354 QuorumPeerMain#root@n4:~# jps3441 DataNode3575 JournalNode3817 Jps3710 NodeManager</code></pre></li><li><p>查看DN(也是nodemanager的配置，RS yarn需要手动启动)</p><pre><code>root@n1:~#  cat /root/app/hadoop/etc/hadoop/slavesn2n3n4</code></pre></li><li><p>启动resourcemanager/yarn/nodemanager（n3, n4）</p><pre><code>yarn-daemon.sh start resourcemanager</code></pre></li><li><p>启动结果slaves配置就是DN和nodemanager的配置</p><pre><code>root@n3:~# yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n3.outroot@n3:~# jps3905 ResourceManager3746 NodeManager3477 DataNode3609 JournalNode3354 QuorumPeerMain3951 Jpsroot@n3:~# #root@n4:~# yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n4.outroot@n4:~# jps3441 DataNode3859 ResourceManager3910 Jps3575 JournalNode3710 NodeManagerroot@n4:~# #root@n2:~# jps5600 NodeManager5764 Jps5044 NameNode5157 DataNode5448 DFSZKFailoverController4920 QuorumPeerMain5294 JournalNoderoot@n2:~# #root@n1:~# jps5363 QuorumPeerMain5923 DFSZKFailoverController5578 NameNode6333 Jps</code></pre></li><li><p>查看yarn http端口8088</p><pre><code>root@n3:~# netstat -nltpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:33139         0.0.0.0:*               LISTEN      3477/java       tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      1210/dnsmasq    tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1181/sshd       tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1048/cupsd      tcp        0      0 0.0.0.0:50010           0.0.0.0:*               LISTEN      3477/java       tcp        0      0 0.0.0.0:50075           0.0.0.0:*               LISTEN      3477/java       tcp        0      0 0.0.0.0:8480            0.0.0.0:*               LISTEN      3609/java       tcp        0      0 0.0.0.0:50020           0.0.0.0:*               LISTEN      3477/java       tcp        0      0 0.0.0.0:8485            0.0.0.0:*               LISTEN      3609/java       tcp6       0      0 192.168.44.102:3888     :::*                    LISTEN      3354/java       tcp6       0      0 :::22                   :::*                    LISTEN      1181/sshd       tcp6       0      0 ::1:631                 :::*                    LISTEN      1048/cupsd      tcp6       0      0 192.168.44.102:8088     :::*                    LISTEN      3905/java       tcp6       0      0 :::41529                :::*                    LISTEN      3354/java       tcp6       0      0 :::13562                :::*                    LISTEN      3746/java       tcp6       0      0 192.168.44.102:8030     :::*                    LISTEN      3905/java       tcp6       0      0 192.168.44.102:8031     :::*                    LISTEN      3905/java       tcp6       0      0 192.168.44.102:8032     :::*                    LISTEN      3905/java       tcp6       0      0 192.168.44.102:8033     :::*                    LISTEN      3905/java       tcp6       0      0 :::2181                 :::*                    LISTEN      3354/java       tcp6       0      0 :::8040                 :::*                    LISTEN      3746/java       tcp6       0      0 :::8042                 :::*                    LISTEN      3746/java       tcp6       0      0 :::43405                :::*                    LISTEN      3746/java </code></pre></li><li><p>查看wenUI</p><pre><code>http://n1:50070/dfshealth.html#tab-overviewhttp://n2:50070/dfshealth.html#tab-overview观察下n1和n2谁是active和standby观察下datanode是不是全n1,n2,n3观察下文件系统是不是active的NN可用，standby的NN不可用http://n3:8088/clusterhttp://n4:8088/cluster(会自动跳转到上面的链接，n4 RS 处于standby状态)n3和n4上启动RS，自动关联到nodemanager，管理DN，查看节点ActiveNodes是不是和DN数目一样3个</code></pre></li></ul><h2 id="测试yarn-RS-ha"><a href="#测试yarn-RS-ha" class="headerlink" title="测试yarn RS ha"></a>测试yarn RS ha</h2><ul><li>测试ha的状态<pre><code>yarn-daemon.sh stop resourcemanager（n3）jps挂掉n3 RS，看看n4:8088的状态active再次启动n3，这时候n3处于standby状态</code></pre></li></ul><h2 id="生产环境的高可用Hadoop集群的搭建总结"><a href="#生产环境的高可用Hadoop集群的搭建总结" class="headerlink" title="生产环境的高可用Hadoop集群的搭建总结"></a>生产环境的高可用Hadoop集群的搭建总结</h2><ul><li>高可用解决了1.x的单节点不可靠的问题</li><li>高可用ha一方面指的是hdfs的namenode的高可用，解决NN的单节点故障问题</li><li>高可用ha另一方面指的是yarn的resource manager的高可用，解决yarn单节点不可靠的问题</li><li>2.x为什么引入了yarn呢？yarn可用保障计算的高可靠</li><li>实验的一般流程：</li></ul><ol><li>Hadoop单机模式：直接跑jar文件</li><li>伪分布式模式：配置成分布式模式，只有一台vm</li><li>全分布式模式：使用1.x架构，具有secondary NN</li><li>全分布式hdfs NN ha模式：使用了QJM实现了hdfs的高可用架构</li><li>全分布式hdfs NN ha + yarn RS ha模式：使用了集群实现了yarn对DN的管理</li></ol><ul><li>对比可以发现1.x和2.x的共同点都是移动计算不移动数据</li><li>对比可以发现1.x和2.x的差异性在于1.x由client直接操作DN，但是2.x加入了yarn层管理DN</li><li>2.x有两个瓶颈NN和RS，因此需要ha处理</li><li>其中4中的QJM，包含了集群：</li></ul><ol><li>设计目标NN ha</li><li>从下到上：DN–&gt;NN active/NN standby–&gt;JN(edits管理)–&gt;zk中zkfc–&gt;zk</li></ol><ul><li>其中5包含了集群：</li></ul><ol><li>设计目标是yarn中的resource manager</li><li>yarn的管理包含了：resource manager（RS）–&gt;node manager</li><li>DN–&gt;NN active/NN standby–&gt;JN(edits管理)–&gt;zk中zkfc–&gt;zk–&gt;RS</li></ol>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> mapreduce </tag>
            
            <tag> yarn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PE hadoop 2.x hdfs availablity implement</title>
      <link href="/2018/04/30/PE%20hadoop%202.x%20hdfs%20availablity%20implement/"/>
      <url>/2018/04/30/PE%20hadoop%202.x%20hdfs%20availablity%20implement/</url>
      <content type="html"><![CDATA[<p>本文介绍如何在hdfs上实现namenode集群的高可用。</p><a id="more"></a><h2 id="集群配置需求"><a href="#集群配置需求" class="headerlink" title="集群配置需求"></a>集群配置需求</h2><ul><li>集群配置地点<pre><code>  NN  DN  JN  ZK  ZKFCn1  1           1   1n2  1   1   1   1   1n3      1   1   1   n4      1   1</code></pre></li></ul><h2 id="配置HDFS"><a href="#配置HDFS" class="headerlink" title="配置HDFS"></a>配置HDFS</h2><ul><li><p>hdfs-site.xml</p><pre><code>vim /root/app/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;      &lt;!-- 配置NN空间 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.nameservices&lt;/name&gt;              &lt;value&gt;sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.namenodes.sxt&lt;/name&gt;              &lt;value&gt;nn1,nn2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.rpc-address.sxt.nn1&lt;/name&gt;              &lt;value&gt;n1:8020&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.rpc-address.sxt.nn2&lt;/name&gt;              &lt;value&gt;n2:8020&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.http-address.sxt.nn1&lt;/name&gt;              &lt;value&gt;n1:50070&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.http-address.sxt.nn2&lt;/name&gt;              &lt;value&gt;n2:50070&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置JN能处理的Node，可以理解为DN --&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;              &lt;value&gt;qjournal://n2:8485;n3:8485;n4:8485/sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.client.failover.proxy.provider.sxt&lt;/name&gt;              &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置密钥 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;              &lt;value&gt;sshfence&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;              &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置JN的临时文件夹 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;              &lt;value&gt;/opt/journal/node/data&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 单节点故障自动迁移 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;              &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>core-site.xml</p><pre><code>vim /root/app/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;      &lt;property&gt;              &lt;name&gt;fs.defaultFS&lt;/name&gt;              &lt;value&gt;hdfs://sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;              &lt;value&gt;/opt/hadoop&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置Quorum Journal Manager的zk集群，JN管理集群 --&gt;      &lt;property&gt;              &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;              &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li></ul><h2 id="配置zookeeper"><a href="#配置zookeeper" class="headerlink" title="配置zookeeper"></a>配置zookeeper</h2><ul><li><p>环境变量</p><pre><code>cat /etc/profileexport ANT_HOME=/root/app/antexport HADOOP_HOME=/root/app/hadoopexport JAVA_HOME=/root/app/jdkexport JRE_HOME=/root/app/jdk/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$ANT_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/app/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin注意只有前面三台的zk路径加入环境变量</code></pre></li><li><p>配置文件</p><pre><code>cat /root/app/zookeeper/conf/zoo.cfg# The number of milliseconds of each tick# dataDir=/tmp/zookeeperdataDir=/opt/zookeeperserver.1=n1:2888:3888server.2=n2:2888:3888server.3=n3:2888:3888</code></pre></li><li><p>分别配置/opt/zookeeper目录，创建myid文件，加入1, 2, 3</p></li><li><p>同时启动zk</p><pre><code>zkServer.sh start</code></pre></li><li><p>查看状态</p><pre><code>zkServer.sh status</code></pre></li><li><p>启动输出</p><pre><code>root@n1:~# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower</code></pre></li><li><p>n1拷贝zookeeper到n2, n3, n4</p><pre><code>scp -r ./dir n1:`pwd`</code></pre></li></ul><h2 id="启动ZK"><a href="#启动ZK" class="headerlink" title="启动ZK"></a>启动ZK</h2><ul><li>启动并查看状态（1, 2, 3同时启动）<pre><code>zkServer.sh startzkServer.sh status</code></pre></li></ul><h2 id="启动JN"><a href="#启动JN" class="headerlink" title="启动JN"></a>启动JN</h2><ul><li><p>在n2. n3, n4启动JN</p><pre><code>hadoop-daemon.sh start journalnode</code></pre></li><li><p>输出</p><pre><code>root@n2:~# hadoop-daemon.sh start journalnodestarting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outroot@n2:~# jps1751 QuorumPeerMain1895 JournalNode1945 Jps</code></pre></li></ul><h2 id="启动NN，高可用HA操作"><a href="#启动NN，高可用HA操作" class="headerlink" title="启动NN，高可用HA操作"></a>启动NN，高可用HA操作</h2><ul><li><p>在一台NN格式化（NN:n1）</p><pre><code>hdfs namenode -format</code></pre></li><li><p>在没有格式化的另外一台hadoop执行standby操作（NN:n2）</p><pre><code>hdfs namenode -bootstrapStandby</code></pre></li><li><p>报错提示n1没有启动namenode，先启动namenode</p><pre><code>hadoop-daemon.sh start namenode</code></pre></li><li><p>在此执行standby成功（格式化+启动n1，standby另外n2）</p><pre><code>18/04/29 11:14:41 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/04/29 11:14:41 INFO namenode.NameNode: createNameNode [-bootstrapStandby]#=====================================================About to bootstrap Standby ID nn2 from:         Nameservice ID: sxt      Other Namenode ID: nn1Other NN&#39;s HTTP address: http://n1:50070Other NN&#39;s IPC  address: n1/192.168.44.100:8020           Namespace ID: 1765158274          Block pool ID: BP-1441163464-192.168.44.100-1525025632122             Cluster ID: CID-2e601647-294c-4e70-8e72-7a82bea94fa9         Layout version: -63     isUpgradeFinalized: true#=====================================================18/04/29 11:14:42 INFO common.Storage: Storage directory /opt/hadoop/dfs/name has been successfully formatted.18/04/29 11:14:43 INFO namenode.TransferFsImage: Opening connection to http://n1:50070/imagetransfer?getimage=1&amp;txid=0&amp;storageInfo=-63:1765158274:0:CID-2e601647-294c-4e70-8e72-7a82bea94fa918/04/29 11:14:43 INFO namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds18/04/29 11:14:43 INFO namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s18/04/29 11:14:43 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 321 bytes.18/04/29 11:14:43 INFO util.ExitUtil: Exiting with status 018/04/29 11:14:43 INFO namenode.NameNode: SHUTDOWN_MSG: SHUTDOWN_MSG: Shutting down NameNode at n2/192.168.44.101</code></pre></li></ul><h2 id="查看HA效果"><a href="#查看HA效果" class="headerlink" title="查看HA效果"></a>查看HA效果</h2><ul><li><p>在一个NN上格式化zookeeper（n1）</p><pre><code>hdfs zkfc -formatZK</code></pre></li><li><p>在单节点NN启动n1</p><pre><code>start-dfs.sh</code></pre></li><li><p>输出</p><pre><code>Starting namenodes on [n1 n2]n1: namenode running as process 2411. Stop it first.n2: namenode running as process 2239. Stop it first.n2: datanode running as process 2413. Stop it first.n4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outStarting journal nodes [n2 n3 n4]n3: journalnode running as process 2207. Stop it first.n2: journalnode running as process 1895. Stop it first.n4: journalnode running as process 1732. Stop it first.Starting ZK Failover Controllers on NN hosts [n1 n2]n2: zkfc running as process 2804. Stop it first.n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outroot@n1:~# </code></pre></li><li><p>进程查看</p><pre><code>root@n1:~# jps3091 DFSZKFailoverController3285 Jps2411 NameNode2188 QuorumPeerMain#root@n2:~# jps3184 Jps2804 DFSZKFailoverController1751 QuorumPeerMain1895 JournalNode2413 DataNode2239 NameNode#root@n3:~# jps2512 Jps2121 QuorumPeerMain2362 DataNode2207 JournalNode#root@n4:~# jps1732 JournalNode2039 Jps1887 DataNode#  NN  DN  JN  ZK  ZKFCn1  1           1   1n2  1   1   1   1   1n3      1   1   1   n4      1   1</code></pre></li><li><p>查看webUI</p><pre><code>http://n2:50070/dfshealth.html#tab-overviewOverview &#39;n2:8020&#39; (active)#http://n1:50070/dfshealth.html#tab-overviewOverview &#39;n1:8020&#39; (standby)#Datanode Information三个#http://n1:50070/explorer.html#/Operation category READ is not supported in state standby#http://n2:50070/explorer.html#/Browse Directory 可见</code></pre></li><li><p>故障测试</p><pre><code>hadoop-daemon.sh stop namenode（n2）查看网页，文件系统，DNhadoop-daemon.sh start namenode（n2）再次查看交换了状态Overview &#39;n1:8020&#39; (active)Overview &#39;n2:8020&#39; (standby)</code></pre></li></ul><h2 id="重新启动与停止"><a href="#重新启动与停止" class="headerlink" title="重新启动与停止"></a>重新启动与停止</h2><ul><li>启停操作<pre><code>stop-dfs.shroot@n1:~# stop-dfs.sh Stopping namenodes on [n1 n2]n1: stopping namenoden2: stopping namenoden3: stopping datanoden4: stopping datanoden2: stopping datanodeStopping journal nodes [n2 n3 n4]n2: stopping journalnoden3: stopping journalnoden4: stopping journalnodeStopping ZK Failover Controllers on NN hosts [n1 n2]n1: stopping zkfcn2: stopping zkfc#下次启动jps查看ZK是不是启动先启动ZK，在启动DFS#root@n1:~# zkServer.sh start（根据列表中的三台都同时启动zk）root@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower#root@n1:~# start-dfs.sh Starting namenodes on [n1 n2]n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.outn2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.outn4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outn2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.outStarting journal nodes [n2 n3 n4]n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outn3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.outn4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.outStarting ZK Failover Controllers on NN hosts [n1 n2]n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outn2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.out</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> ha </tag>
            
            <tag> hdfs </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hbase的基本构成与实践</title>
      <link href="/2018/04/24/hbase%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
      <url>/2018/04/24/hbase%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%E4%B8%8E%E5%AE%9E%E8%B7%B5/</url>
      <content type="html"><![CDATA[<p>本文介绍hbase的基本构成与实践。</p><a id="more"></a><h2 id="hbase的基本构成"><a href="#hbase的基本构成" class="headerlink" title="hbase的基本构成"></a>hbase的基本构成</h2><ul><li><p>表空间 namespace</p><pre><code>两个默认的表空间hbase： 系统默认表空间default： 不指定自动加入的表空间root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/dataFound 2 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/defaultdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase</code></pre></li><li><p>表 table</p><pre><code>表的存在形式：表以文件夹的形式存在于hdfs中表的基本组成是：RowKey, Column Family, Column, Value(Cell):Byte array表的物理属性：以RowKey进行字典排序，行的方向存在多个Region，Region是存储和负载均衡的最小单元，不同的Region分布到不同的RegionServer上#=============================root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbaseFound 2 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/metadrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/namespace#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/metaFound 3 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/meta/.tabledescdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/meta/.tmpdrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/meta/1588230740Found 4 items-rw-r--r--   1 root supergroup         32 2018-04-23 20:18 /hbase/data/hbase/meta/1588230740/.regioninfodrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/.tmpdrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/infodrwxr-xr-x   - root supergroup          0 2018-04-24 01:37 /hbase/data/hbase/meta/1588230740/recovered.edits#-----------------------------root@ubuntu:~/app/hbase/bin# ./hbase shell2018-04-24 06:29:19,776 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017hbase(main):001:0&gt;#-----------------------------hbase(main):001:0&gt; create &#39;maizi_hbase&#39;,&#39;f&#39;0 row(s) in 2.5670 seconds=&gt; Hbase::Table - maizi_hbasehbase(main):002:0&gt;#=============================访问 http://192.168.231.150:50070/explorer.html#/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458得到hbse的路径#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458Found 3 items-rw-r--r--   1 root supergroup         46 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/.regioninfodrwxr-xr-x   - root supergroup          0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/fdrwxr-xr-x   - root supergroup          0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/recovered.edits#-----------------------------上述操作中，6a028f21704f8fc6bf298598f6b8a458为Region的编号#=============================访问 http://192.168.231.150:16010/table.jsp?name=maizi_hbase 得到hbase的管理界面，可以看出路径结构</code></pre></li><li><p>列族 column family</p><pre><code>很多列的集合hbase中的每个列都属于一个column family每个column family存在于hdfs的单独文件中列名以column family为前缀， info:name, info:age#-----------------------------/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/f数据库/数据/表空间/表/region/列族#-----------------------------创建表的时候必须定义列族，因为在hdfs上必须要创建文件夹#-----------------------------何如设计RowKey是经典问题？</code></pre></li><li><p>列</p><pre><code>存放数据的地方</code></pre></li><li><p>RowKey</p><pre><code>可以理解为主键，最大长度为64k，RowKey保存为字节数组是非关系型数据库中key-value类型的数据的key自动字典排序散列原则，分布到不同的Region中，RegionServer的负载均衡问题</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> configuration </tag>
            
            <tag> 原理 </tag>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>how to understand habse components ?</title>
      <link href="/2018/04/24/how%20to%20understand%20habse%20component%20/"/>
      <url>/2018/04/24/how%20to%20understand%20habse%20component%20/</url>
      <content type="html"><![CDATA[<p>This article describes the basic components of hbase, including HMaster, HRegionServer, Region.</p><a id="more"></a><h2 id="principle-of-hbase"><a href="#principle-of-hbase" class="headerlink" title="principle of hbase"></a>principle of hbase</h2><ul><li><strong>features of HMaster (technical director)</strong></li></ul><ol><li>add, delete, and modify tables</li><li>Region load balancing</li><li>HMaster manages the distribution of data</li></ol><ul><li><strong>features of RegionServer (department manager)</strong></li></ul><ol><li>RegionServer is the service component of Hbase</li><li>RegionServer maintains Regions assigned by HMaster</li><li>RegionServer can divide big Regions</li></ol><ul><li><strong>features of Region (developers)</strong></li></ul><ol><li>Region is a partition</li><li>handling the tasks assigned by RegionServer</li></ol>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> configuration </tag>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>zookeeper working mechanism and installation</title>
      <link href="/2018/04/24/zooKeeper%20working%20mechanism%20and%20installation/"/>
      <url>/2018/04/24/zooKeeper%20working%20mechanism%20and%20installation/</url>
      <content type="html"><![CDATA[<p>This article describes the basic working principle and installation process of zookeeper.</p><a id="more"></a><h2 id="zookeeper-working-mechanism"><a href="#zookeeper-working-mechanism" class="headerlink" title="zookeeper working mechanism"></a>zookeeper working mechanism</h2><ul><li>zookeeper is a high-performance application coordination server that is mainly used to maintain a file system-like namespace.</li><li>zookeeper itself contains 2n+1 servers, and their roles are divided into leader and follower.</li><li>zookeeper maintains multiple server services and maintains data consistency to ensure that clients connecting to any server can get consistent data services.</li><li>zookeeper’s nodes have 4 life cycles.<pre><code>PERSISTENT (persistent node)PERSISTENT_SEQUENTIAL (Sequential automatic numbering of persistent nodes, this node automatically adds 1 based on the number of existing nodes)EPHEMERAL (temporary node, client session timeout such nodes will be automatically deleted)EPHEMERAL_SEQUENTIAL (temporary automatic numbering node)</code></pre></li></ul><h2 id="install-and-configure-zookeeper"><a href="#install-and-configure-zookeeper" class="headerlink" title="install and configure zookeeper"></a>install and configure zookeeper</h2><ul><li>download &gt; extract &gt; configure &gt; start/stop &gt; test</li><li>configure<pre><code>root@ubuntu:~/app/zookeeper# lltotal 1596drwxr-xr-x 10 1001 1001    4096 Mar 23  2017 ./drwxr-xr-x 13 root root    4096 Apr 23 23:50 ../drwxr-xr-x  2 1001 1001    4096 Mar 23  2017 bin/-rw-rw-r--  1 1001 1001   84725 Mar 23  2017 build.xmldrwxr-xr-x  2 1001 1001    4096 Mar 23  2017 conf/drwxr-xr-x 10 1001 1001    4096 Mar 23  2017 contrib/drwxr-xr-x  2 1001 1001    4096 Mar 23  2017 dist-maven/drwxr-xr-x  6 1001 1001    4096 Mar 23  2017 docs/-rw-rw-r--  1 1001 1001    1709 Mar 23  2017 ivysettings.xml-rw-rw-r--  1 1001 1001    5691 Mar 23  2017 ivy.xmldrwxr-xr-x  4 1001 1001    4096 Mar 23  2017 lib/-rw-rw-r--  1 1001 1001   11938 Mar 23  2017 LICENSE.txt-rw-rw-r--  1 1001 1001    3132 Mar 23  2017 NOTICE.txt-rw-rw-r--  1 1001 1001    1770 Mar 23  2017 README_packaging.txt-rw-rw-r--  1 1001 1001    1585 Mar 23  2017 README.txtdrwxr-xr-x  5 1001 1001    4096 Mar 23  2017 recipes/drwxr-xr-x  8 1001 1001    4096 Mar 23  2017 src/-rw-rw-r--  1 1001 1001 1456729 Mar 23  2017 zookeeper-3.4.10.jar-rw-rw-r--  1 1001 1001     819 Mar 23  2017 zookeeper-3.4.10.jar.asc-rw-rw-r--  1 1001 1001      33 Mar 23  2017 zookeeper-3.4.10.jar.md5-rw-rw-r--  1 1001 1001      41 Mar 23  2017 zookeeper-3.4.10.jar.sha1#==============================-rw-rw-r--  1 1001 1001  535 Mar 23  2017 configuration.xsl-rw-rw-r--  1 1001 1001 2161 Mar 23  2017 log4j.properties-rw-rw-r--  1 1001 1001  922 Mar 23  2017 zoo_sample.cfgroot@ubuntu:~/app/zookeeper/conf# cp zoo_sample.cfg zoo.cfgroot@ubuntu:~/app/zookeeper/conf# lltotal 24drwxr-xr-x  2 1001 1001 4096 Apr 24 00:17 ./drwxr-xr-x 10 1001 1001 4096 Mar 23  2017 ../-rw-rw-r--  1 1001 1001  535 Mar 23  2017 configuration.xsl-rw-rw-r--  1 1001 1001 2161 Mar 23  2017 log4j.properties-rw-r--r--  1 root root  922 Apr 24 00:17 zoo.cfg-rw-rw-r--  1 1001 1001  922 Mar 23  2017 zoo_sample.cfg#------------------------------vim zoo.cfgmkdir -p /root/app/zookeeper/zookdata# dataDir=/tmp/zookeeperdataDir=/root/app/zookeeper/zookdata# append the following:server.1=192.168.231.150:2888:3888#------------------------------root@ubuntu:~/app/zookeeper/zookdata# pwd/root/app/zookeeper/zookdataroot@ubuntu:~/app/zookeeper/zookdata# touch myid &amp;&amp; echo 1 &gt; myidroot@ubuntu:~/app/zookeeper/zookdata# cat myid1#==============================scp the zookeeper to other serverreset myid file in zookeeper on other server#==============================</code></pre></li></ul><h2 id="start-stop-zookeeper"><a href="#start-stop-zookeeper" class="headerlink" title="start/stop zookeeper"></a>start/stop zookeeper</h2><ul><li><p>start zookeeper</p><pre><code>root@ubuntu:~/app/zookeeper/bin# lltotal 52drwxr-xr-x  2 1001 1001 4096 Apr 24 00:35 ./drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../-rwxr-xr-x  1 1001 1001  232 Mar 23  2017 README.txt*-rwxr-xr-x  1 1001 1001 1937 Mar 23  2017 zkCleanup.sh*-rwxr-xr-x  1 1001 1001 1056 Mar 23  2017 zkCli.cmd*-rwxr-xr-x  1 1001 1001 1534 Mar 23  2017 zkCli.sh*-rwxr-xr-x  1 1001 1001 1628 Mar 23  2017 zkEnv.cmd*-rwxr-xr-x  1 1001 1001 2696 Mar 23  2017 zkEnv.sh*-rwxr-xr-x  1 1001 1001 1089 Mar 23  2017 zkServer.cmd*-rwxr-xr-x  1 1001 1001 6773 Mar 23  2017 zkServer.sh*-rw-r--r--  1 root root 5056 Apr 24 00:35 zookeeper.out#------------------------------Using config: /root/app/zookeeper/bin/../conf/zoo.cfgUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}#------------------------------root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh startzookeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED#------------------------------root@ubuntu:~/app/zookeeper/bin# jps38113 Jps34545 HQuorumPeer34757 HRegionServer12550 NodeManager12408 ResourceManager12024 DataNode34618 HMaster12235 SecondaryNameNode11852 NameNode#------------------------------root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh statuszookeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: standalone</code></pre></li><li><p>use zookeeper</p><pre><code>root@ubuntu:~/app/zookeeper/bin# ./zkCli.shConnecting to localhost:2181...2018-04-24 00:40:32,972 [myid:] - INFO  [main:Environment@100] - Client...2018-04-24 00:40:32,984 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2018-04-24 00:40:32,984 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp...WatchedEvent state:SyncConnected type:None path:null#------------------------------[zk: localhost:2181(CONNECTED) 0] helpzookeeper -server host:port cmd args  stat path [watch]  set path data [version]  ls path [watch]  delquota [-n|-b] path  ls2 path [watch]  setAcl path acl  setquota -n|-b val path  history  redo cmdno  printwatches on|off  delete path [version]  sync path  listquota path  rmr path  get path [watch]  create [-s] [-e] path data acl  addauth scheme auth  quit  getAcl path  close  connect host:port#------------------------------[zk: localhost:2181(CONNECTED) 1] create /data_test &#39;data_test&#39;       Created /data_test[zk: localhost:2181(CONNECTED) 2] ls /[data_test, zookeeper, hbase][zk: localhost:2181(CONNECTED) 3] get /data_testdata_testcZxid = 0x75ctime = Tue Apr 24 00:42:11 PDT 2018mZxid = 0x75mtime = Tue Apr 24 00:42:11 PDT 2018pZxid = 0x75cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 0#------------------------------[zk: localhost:2181(CONNECTED) 4] create /data_test/dir_2 &#39;123_value&#39;Created /data_test/dir_2[zk: localhost:2181(CONNECTED) 5] get /data_testdata_testcZxid = 0x75ctime = Tue Apr 24 00:42:11 PDT 2018mZxid = 0x75mtime = Tue Apr 24 00:42:11 PDT 2018pZxid = 0x76cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 1#------------------------------[zk: localhost:2181(CONNECTED) 6] quitQuitting...2018-04-24 00:47:19,034 [myid:] - INFO  [main:zookeeper@684] - Session: 0x162f5c0c60f0007 closed2018-04-24 00:47:19,037 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x162f5c0c60f0007</code></pre></li></ul><h2 id="view-the-log"><a href="#view-the-log" class="headerlink" title="view the log"></a>view the log</h2><ul><li>where is the log?<pre><code>root@ubuntu:~/app/zookeeper/bin# lltotal 52drwxr-xr-x  2 1001 1001 4096 Apr 24 00:35 ./drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../-rwxr-xr-x  1 1001 1001  232 Mar 23  2017 README.txt*-rwxr-xr-x  1 1001 1001 1937 Mar 23  2017 zkCleanup.sh*...-rw-r--r--  1 root root 5056 Apr 24 00:35 zookeeper.out</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> configuration </tag>
            
            <tag> hbase </tag>
            
            <tag> zookdata </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pseudo-distributed hbase configuration</title>
      <link href="/2018/04/24/pseudo-distributed%20hbase%20configuration/"/>
      <url>/2018/04/24/pseudo-distributed%20hbase%20configuration/</url>
      <content type="html"><![CDATA[<p>This article describes how to build a pseudo-distributed hbase on a virtual machine.</p><a id="more"></a><h2 id="preconditions"><a href="#preconditions" class="headerlink" title="preconditions"></a>preconditions</h2><ul><li>jdk environment<pre><code>root@ubuntu:~# echo $JAVA_HOME/root/app/jdk1.8.0_171</code></pre></li><li><p>a pseudo-distributed hadoop</p><pre><code>root@ubuntu:~/app/hadoop/sbin# jps12550 NodeManager34152 Jps12408 ResourceManager12024 DataNode12235 SecondaryNameNode11852 NameNode</code></pre></li><li><p>test</p><pre><code>http://localhost:50070http://192.168.231.150:8099http://192.168.231.150:8042</code></pre></li></ul><h2 id="configure-hbase"><a href="#configure-hbase" class="headerlink" title="configure hbase"></a>configure hbase</h2><ul><li>download hbase from <a href="http://mirror.bit.edu.cn/apache/" target="_blank" rel="noopener">apache mirrors</a></li><li>extract files from hbase-2.0.0-beta-2-bin.tar.gz</li><li>configure xml files<pre><code>hbase-env.shhbase-site.xmlregionservers#================================root@ubuntu:~/app/hbase/conf# pwd/root/app/hbase/conf#================================root@ubuntu:~/app/hbase/conf# lltotal 48drwxr-xr-x 2 root root 4096 Apr 23 20:15 ./drwxr-xr-x 8 root root 4096 Apr 23 20:17 ../-rw-r--r-- 1 root root 1811 Dec 26  2015 hadoop-metrics2-hbase.properties-rw-r--r-- 1 root root 4537 Jan 28  2016 hbase-env.cmd-rw-r--r-- 1 root root 7537 Apr 23 20:12 hbase-env.sh-rw-r--r-- 1 root root 2257 Dec 26  2015 hbase-policy.xml-rw-r--r-- 1 root root 1355 Apr 23 20:09 hbase-site.xml-rw-r--r-- 1 root root 4603 May 28  2017 log4j.properties-rw-r--r-- 1 root root   16 Apr 23 20:15 regionservers#================================vim hbase-env.sh# The java implementation to use.  Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/export JAVA_HOME=/root/app/jdk1.8.0_171#--------------------------------# Tell HBase whether it should manage it&#39;s own instance of Zookeeper or not.# export HBASE_MANAGES_ZK=trueexport HBASE_MANAGES_ZK=true#================================vim regionserversroot@ubuntu:~/app/hbase/conf# cat regionservers192.168.231.150#================================vim hbase-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;hbase.rootdir&lt;/name&gt;      &lt;value&gt;hdfs://192.168.231.150:9000/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;  &lt;value&gt;192.168.231.150&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;      &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre></li></ul><h2 id="run-stop-hbase"><a href="#run-stop-hbase" class="headerlink" title="run/stop hbase"></a>run/stop hbase</h2><ul><li>run hbase<pre><code>root@ubuntu:~/app/hbase/bin# ./start-hbase.shlocalhost: starting zookeeper, logging to /root/app/hbase/bin/../logs/hbase-root-zookeeper-ubuntu.outstarting master, logging to /root/app/hbase/bin/../logs/hbase-root-master-ubuntu.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0192.168.231.150: starting regionserver, logging to /root/app/hbase/bin/../logs/hbase-root-regionserver-ubuntu.out192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0root@ubuntu:~/app/hbase/bin# jps34545 HQuorumPeer34757 HRegionServer12550 NodeManager12408 ResourceManager12024 DataNode34618 HMaster12235 SecondaryNameNode11852 NameNode35053 Jps</code></pre></li><li>test hbase<pre><code>http://192.168.231.150:16010</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> configuration </tag>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>how to download 1080p videos from youtube</title>
      <link href="/2018/04/23/how%20to%20download%201080p%20videos%20from%20youtube/"/>
      <url>/2018/04/23/how%20to%20download%201080p%20videos%20from%20youtube/</url>
      <content type="html"><![CDATA[<p>the article describes how to download 1080p videos form youtube with your foreigen vps/ecs.</p><a id="more"></a><h2 id="general-method"><a href="#general-method" class="headerlink" title="general method"></a>general method</h2><ul><li>chrome browser</li><li>copy the link of the video from youtube</li><li>paste the link to the input box of <a href="https://en.savefrom.net/" target="_blank" rel="noopener">https://en.savefrom.net/</a></li><li>download videos though chrome</li></ul><h2 id="advance-method"><a href="#advance-method" class="headerlink" title="advance method"></a>advance method</h2><ul><li>login in your foreign vps</li><li>such as: ubuntu from digitalocean</li><li>use the following cmd to download the video<pre><code>apt-get install youtube-dl -yyoutube-dl -f 22 your_video_link</code></pre></li></ul><h2 id="ultimate-method"><a href="#ultimate-method" class="headerlink" title="ultimate method"></a>ultimate method</h2><ul><li><p>use the following cmd to analysis all videos and audios about your_video_link</p><pre><code>youtube-dl -F your_video_link</code></pre></li><li><p>use the following cmd to download videos and audios with the specified code</p><pre><code>youtube-dl -f 10 your_video_link</code></pre></li><li><p>install the tools of video and audio</p><pre><code>apt-get install ffmpeg -y</code></pre></li><li><p>merge the video and audio</p><pre><code>ffmpeg -i /tmp/a.wav -i /tmp/a.avi /tmp/out.avi</code></pre></li></ul><h2 id="how-to-download-playlist-from-youtube"><a href="#how-to-download-playlist-from-youtube" class="headerlink" title="how to download playlist from youtube"></a>how to download playlist from youtube</h2><ul><li>cmd<pre><code>youtube-dl -citk –format mp4 –yes-playlist VIDEO_PLAYLIST_LINKyoutube-dl -citk –format mp4 –yes-playlist https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfuayoutube-dl -cit &quot;https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfua&quot;</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> vps </tag>
            
            <tag> youtube </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>usage of scp</title>
      <link href="/2018/04/23/usage%20of%20scp/"/>
      <url>/2018/04/23/usage%20of%20scp/</url>
      <content type="html"><![CDATA[<p>this article describes how download/upload files from server using scp command line.</p><a id="more"></a><h2 id="usage-of-scp"><a href="#usage-of-scp" class="headerlink" title="usage of scp"></a>usage of scp</h2><ol><li><p>download file from server</p><pre><code>scp root@servername:/path/filename /tmp/local_destinationscp root@192.168.0.101:/home/kimi/test.txt /home/kimi/test.txt</code></pre></li><li><p>upload file to server</p><pre><code>scp /path/local_filename root@servername:/path  scp /var/www/test.php root@192.168.0.101:/var/www/</code></pre></li><li><p>download directory to client</p><pre><code>scp -r root@servername:remote_dir/ /tmp/local_dir scp -r root@192.168.0.101:/home/kimi/test /tmp/local_dir</code></pre></li><li><p>upload directory to server</p><pre><code>scp -r /tmp/local_dir root@servername:remote_dirscp -P 22 -r test root@192.168.0.101:/var/www/</code></pre></li></ol>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> linux </tag>
            
            <tag> cmd </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>write wordcount program on eclipse and run on hadoop in win10</title>
      <link href="/2018/04/23/write%20wordcount%20program%20on%20eclipse%20and%20run%20on%20hadoop%20in%20win10/"/>
      <url>/2018/04/23/write%20wordcount%20program%20on%20eclipse%20and%20run%20on%20hadoop%20in%20win10/</url>
      <content type="html"><![CDATA[<p>This article describes how to write the wordcount program on eclipse and run it on local hadoop.</p><a id="more"></a><h2 id="prerequisites"><a href="#prerequisites" class="headerlink" title="prerequisites"></a><strong>prerequisites</strong></h2><ul><li><strong>server</strong></li></ul><ol><li>win10</li><li>hadoop 2.7.6</li></ol><ul><li><strong>client</strong></li></ul><ol><li>win10</li><li>eclipse neon</li><li>hadoop-eclipse-plugin-2.7.2.jar</li></ol><h2 id="create-project-and-coding"><a href="#create-project-and-coding" class="headerlink" title="create project and coding"></a><strong>create project and coding</strong></h2><ul><li><p><strong>create project</strong></p><pre><code>new &gt; other &gt; map reduce program &gt; fix the boxes with name, ${HADOOP_HOME} &gt; finish</code></pre></li><li><p><strong>coding</strong></p><pre><code>package com.hikvision.bigdata.hadoop.hadoop_wordcount;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.log4j.BasicConfigurator;/*** wordcount*/public class WordCount {  public static void main(String[] args) throws Exception {      BasicConfigurator.configure();      System.out.println(&quot;Hello World!&quot;);      Configuration conf = new Configuration();      @SuppressWarnings(&quot;deprecation&quot;)      Job job = new Job(conf, &quot;wordcount&quot;);      job.setJarByClass(WordCount.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);      job.setMapperClass(Map.class);      job.setReducerClass(Reduce.class);      job.setInputFormatClass(TextInputFormat.class);      job.setOutputFormatClass(TextOutputFormat.class);      FileInputFormat.addInputPath(job, new Path(args[0]));      FileOutputFormat.setOutputPath(job, new Path(args[1]));      job.waitForCompletion(true);  }  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {      private final static IntWritable one = new IntWritable(1);      private Text word = new Text();      public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String line = value.toString();          StringTokenizer tokenizer = new StringTokenizer(line);          while (tokenizer.hasMoreTokens()) {              word.set(tokenizer.nextToken());              context.write(word, one);          }      }  }  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {      public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)              throws IOException, InterruptedException {          int sum = 0;          for (IntWritable val : values) {              sum += val.get();          }          context.write(key, new IntWritable(sum));      }  }}</code></pre></li></ul><h2 id="configure-program"><a href="#configure-program" class="headerlink" title="configure program"></a><strong>configure program</strong></h2><ul><li><p><strong>configure hadoop</strong></p><pre><code>step 1:core-site.xml&lt;configuration&gt; &lt;property&gt;    &lt;name&gt;fs.default.name&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 2:hdfs-site.xml&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;dfs.name.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.data.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;    &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 3:mapred-site.xml&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;    &lt;value&gt;localhost:10020&lt;/value&gt;    &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 4:yarn-site.xml&lt;configuration&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;        &lt;value&gt;localhost:8099&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p><strong>format namenode</strong></p><pre><code>cd binhdfs namenode -format# format only once</code></pre></li><li><p><strong>start hadoop and upload data to hdfs</strong></p><pre><code>cd ${HADOOP_HOME}input cmd to open cmd linecd sbinstart-all.cmd##hadoop fs -mkdir /datahadoop fs -put D:\a.txt /data/a.txt</code></pre></li><li><strong>configure input and output path for program in eclipse</strong><pre><code>1 project name &gt; right clieck &gt; run as &gt; run configuration &gt; java application &gt; new2 fix the boxes with the program name, main class, run name, and arguments3 the arguments as follows:hdfs://localhost:9000/data/a.txt hdfs://localhost:9000/data/output</code></pre></li></ul><h2 id="run-program"><a href="#run-program" class="headerlink" title="run program"></a><strong>run program</strong></h2><ul><li><strong>precondition</strong></li></ul><ol><li>delete the output floder on hdfs</li><li>data is ready</li><li>input/output path is configured in eclipse</li><li>hadoop is running</li></ol><ul><li><strong>run program</strong><pre><code>project name &gt; right clieck &gt; run as &gt; run on hadoop &gt; select the main class &gt; enjoy the ouput</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> configuration </tag>
            
            <tag> coding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>configure hadoop on ubuntu and connect to eclipse on ubuntu or win10</title>
      <link href="/2018/04/22/configure%20hadoop%20on%20ubuntu%20and%20connect%20to%20eclipse%20on%20ubuntu%20or%20win10/"/>
      <url>/2018/04/22/configure%20hadoop%20on%20ubuntu%20and%20connect%20to%20eclipse%20on%20ubuntu%20or%20win10/</url>
      <content type="html"><![CDATA[<p>This article describes how to configure hadoop2.7.6 on ubuntu and connect to hadoop2.7.6 using eclipse on ubuntu and win10 respectively.</p><a id="more"></a><h2 id="prerequisites"><a href="#prerequisites" class="headerlink" title="prerequisites"></a><strong>prerequisites</strong></h2><ul><li><strong>server</strong></li></ul><ol><li>win10</li><li>vmware workstation pro 12</li><li>Ubuntu 16.04.4 LTS</li><li>hadoop 2.7.6 <a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">download</a></li></ol><ul><li><strong>ubuntu clinet</strong></li></ul><ol><li>eclipse neon <a href="http://www.eclipse.org/downloads/packages/release/Neon/3" target="_blank" rel="noopener">download</a></li><li>hadoop-eclipse-plugin-2.7.2.jar <a href="https://download.csdn.net/download/tondayong1981/9432425" target="_blank" rel="noopener">download</a></li></ol><ul><li><strong>win10 client</strong></li></ul><ol><li>eclipse neon <a href="http://www.eclipse.org/downloads/packages/release/Neon/3" target="_blank" rel="noopener">download</a></li><li>hadoop-eclipse-plugin-2.7.2.jar <a href="https://download.csdn.net/download/tondayong1981/9432425" target="_blank" rel="noopener">download</a></li></ol><h2 id="pretreatment-for-ubuntu-virtual-machine"><a href="#pretreatment-for-ubuntu-virtual-machine" class="headerlink" title="pretreatment for ubuntu virtual machine"></a><strong>pretreatment for ubuntu virtual machine</strong></h2><ul><li><strong>enable the user-passwd input box on the login screen</strong><pre><code>sudo passwd rootsu rootcd /usr/share/lightdm/lightdm.conf.d/vim 50-unity-greeter.conf# adduser-session=ubuntugreeter-show-manual-login=trueall-guest=false# rebootreboot# login in ubuntu with root and get a error reportvim /root/.profile# locate tomesg n || true# change totty -s &amp;&amp; mesg n || true</code></pre></li><li><strong>configure jdk for ubuntu</strong></li><li>jdk 1.8 <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">download</a><pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binsource /etc/profilejava -version</code></pre></li></ul><h2 id="configure-hadoop2-7-6"><a href="#configure-hadoop2-7-6" class="headerlink" title="configure hadoop2.7.6"></a><strong>configure hadoop2.7.6</strong></h2><ul><li><p><strong>append <em>JAVA_HOME</em> to stat script</strong></p><pre><code>step 1:vim /root/app/hadoop/etc/hadoop/hadoop-env.sh# export JAVA_HOME=${JAVA_HOME}export JAVA_HOME=/root/app/jdk1.8.0_171###########################################step 2:vim /root/app/hadoop/etc/hadoop/yarn-env.sh# some Java parameters# export JAVA_HOME=/home/y/libexec/jdk1.6.0/export JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p><strong>configure core-site.xml, hdfs-site.xml,  mapred-site.xml, yarn-site.xml</strong></p><pre><code>step 1:vim /root/app/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;   &lt;property&gt;      &lt;name&gt;fs.default.name&lt;/name&gt;      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 2:vim /root/app/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;dfs.name.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.data.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt;  &lt;/property&gt; &lt;property&gt;      &lt;name&gt;dfs.permissions&lt;/name&gt;      &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;      &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 3:vim /root/app/hadoop/etc/hadoop/mapred-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;      &lt;value&gt;yarn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;      &lt;value&gt;localhost:10020&lt;/value&gt;      &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 4:vim /root/app/hadoop/etc/hadoop/yarn-site.xml&lt;configuration&gt;  &lt;property&gt;          &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;          &lt;value&gt;mapreduce_shuffle&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;          &lt;value&gt;localhost:8099&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p><strong>create floder for hadoop work directory</strong></p><pre><code>cd /root/app/hadoopmkdir hdfs -p hdfs/data hdfs/namemkdir tmp</code></pre></li><li><p><strong>format hdfs and start/stop hadoop</strong></p></li></ul><ol><li>format hdfs<pre><code># keep all hadoop process stopped./sbin/stop-all.sh# remove tmp directoryrm -rdf tmp/# format hdfs only oncebin/hdfs namenode -format# see if the format is successfultree hdfs/</code></pre></li><li>start/stop hadoop<pre><code># start hadoop./sbin/start-all.shroot@ubuntu:~/app/hadoop# jps63809 Jps63474 NodeManager62950 DataNode63335 ResourceManager62775 NameNode63163 SecondaryNameNode# stop hadoop./sbin/stop-all.sh</code></pre></li></ol><h2 id="connect-to-hadoop-with-eclipse"><a href="#connect-to-hadoop-with-eclipse" class="headerlink" title="connect to hadoop with eclipse"></a><strong>connect to hadoop with eclipse</strong></h2><ul><li><strong>install plugin in eclipse on ubuntu</strong><pre><code>1 copy to hadoop-eclipse-plugin-2.7.2.jar to ${eclipse_home}/dropins;2 open eclipse;3 open menu &gt; windows &gt; show view &gt; other &gt; mapreduce tools &gt; map/reduce locations;4 map/reduce locations &gt; right click &gt; edit hadoop location;location name: XXXmap/reduce master:  host: localhost  port: 50020dfs master:  host: localhost  port: 90005 open dfs locations, you will find the file in hdfs.</code></pre></li><li><strong>install plugin in eclipse on win10</strong><pre><code>1 on win10, your eclipse serves as a clinet, you can connect your server with ip, so you should firstly replace *localhost* with your server ip in all etc files, such as core-site.xml, hdfs-site.xml,  mapred-site.xml, yarn-site.xml;2 repeat the step 1,2,3,4 above;3 map/reduce locations &gt; right click &gt; edit hadoop location &gt; advace parameters, replace *hadoop.tmp.dir* with your own address in /hdfs-site.xml;4 enjoy the local developing and the remote debuging.</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> configuration </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
            <tag> eclipse </tag>
            
            <tag> configuration </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hexo构建博客搜索框加载中的解决方案</title>
      <link href="/2018/04/19/hexo%E6%9E%84%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%90%9C%E7%B4%A2%E6%A1%86%E5%8A%A0%E8%BD%BD%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>/2018/04/19/hexo%E6%9E%84%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%90%9C%E7%B4%A2%E6%A1%86%E5%8A%A0%E8%BD%BD%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      <content type="html"><![CDATA[<p>本文介绍nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。</p><a id="more"></a><h2 id="问题与现象"><a href="#问题与现象" class="headerlink" title="问题与现象"></a>问题与现象</h2><ul><li>nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。</li></ul><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><ul><li>开发的markdown中出现了非utf-8的字符。</li><li>访问可以查找错误出现的位置：<a href="https://leebin.top/search.xml" target="_blank" rel="noopener">https://leebin.top/search.xml</a></li></ul><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ul><li>逐个排查每个markdown文件，直到找到非utf-8字符，删除，重新部署，点击搜索框测试。</li><li>访问：<a href="https://leebin.top/search.xml" target="_blank" rel="noopener">https://leebin.top/search.xml</a> 发现可以解析成源文件。</li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop中wordcount程序开发</title>
      <link href="/2018/04/19/Hadoop%E4%B8%ADwordcount%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/"/>
      <url>/2018/04/19/Hadoop%E4%B8%ADwordcount%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/</url>
      <content type="html"><![CDATA[<p>本文介绍如何利用java和hadoop组件开发wordcount程序。</p><a id="more"></a><h2 id="开发与测试环境"><a href="#开发与测试环境" class="headerlink" title="开发与测试环境"></a>开发与测试环境</h2><ul><li>windows</li><li>eclipse</li><li>maven，常见的组件如下：</li></ul><ol><li>Apache Hadoop Common 3.1</li><li>Apache Hadoop Client Aggregator 3.1</li><li>Hadoop Core 1.2</li><li>Apache Hadoop HDFS 3.1</li><li>Apache Hadoop MapReduce Core 3.1</li></ol><ul><li>ubuntu中hadoop单机模式，搭建过程参考: <a href="https://leebin.top/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">如何hadoop单机版</a></li></ul><h2 id="添加依赖后maven报错"><a href="#添加依赖后maven报错" class="headerlink" title="添加依赖后maven报错"></a>添加依赖后maven报错</h2><ul><li><p>报错</p><pre><code>Buiding Hadoop with Eclipse / Maven - Missing artifact jdk.tools:jdk.tools:jar:1.6</code></pre></li><li><p>解决</p><pre><code># cmdC:\Users\BinLee&gt;java -versionjava version &quot;1.8.0_144&quot;Java(TM) SE Runtime Environment (build 1.8.0_144-b01)Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)# 添加下面的依赖到maven的pom.xml&lt;dependency&gt;  &lt;groupId&gt;jdk.tools&lt;/groupId&gt;  &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;  &lt;version&gt;1.8.0_144&lt;/version&gt;  &lt;scope&gt;system&lt;/scope&gt;  &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt;</code></pre></li></ul><h2 id="wordcount程序开发"><a href="#wordcount程序开发" class="headerlink" title="wordcount程序开发"></a>wordcount程序开发</h2><ul><li><p>pom.xml</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.jordiburgos&lt;/groupId&gt;  &lt;artifactId&gt;wordcount&lt;/artifactId&gt;  &lt;packaging&gt;jar&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;name&gt;wordcount&lt;/name&gt;  &lt;url&gt;http://jordiburgos.com&lt;/url&gt;  &lt;properties&gt;      &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;  &lt;/properties&gt;  &lt;repositories&gt;      &lt;repository&gt;          &lt;id&gt;cloudera&lt;/id&gt;          &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;      &lt;/repository&gt;  &lt;/repositories&gt;  &lt;dependencies&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;          &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;          &lt;version&gt;2.2.0&lt;/version&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;          &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;          &lt;version&gt;1.2.1&lt;/version&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;jdk.tools&lt;/groupId&gt;          &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;          &lt;version&gt;1.7&lt;/version&gt;          &lt;scope&gt;system&lt;/scope&gt;          &lt;systemPath&gt;${java.home}/../lib/tools.jar&lt;/systemPath&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;junit&lt;/groupId&gt;          &lt;artifactId&gt;junit&lt;/artifactId&gt;          &lt;version&gt;3.8.1&lt;/version&gt;          &lt;scope&gt;test&lt;/scope&gt;      &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;      &lt;plugins&gt;          &lt;plugin&gt;              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;              &lt;version&gt;2.4&lt;/version&gt;              &lt;executions&gt;                  &lt;execution&gt;                      &lt;id&gt;distro-assembly&lt;/id&gt;                      &lt;phase&gt;package&lt;/phase&gt;                      &lt;goals&gt;                          &lt;goal&gt;single&lt;/goal&gt;                      &lt;/goals&gt;                      &lt;configuration&gt;                          &lt;descriptors&gt;                              &lt;descriptor&gt;assembly.xml&lt;/descriptor&gt;                          &lt;/descriptors&gt;                      &lt;/configuration&gt;                  &lt;/execution&gt;              &lt;/executions&gt;          &lt;/plugin&gt;      &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre></li><li><p>wordcount.java</p><pre><code>package com.jordiburgos;import java.io.IOException;import java.util.*;import org.apache.hadoop.fs.Path;import org.apache.hadoop.conf.*;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.*;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;public class WordCount {  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {      private final static IntWritable one = new IntWritable(1);      private Text word = new Text();      public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String line = value.toString();          StringTokenizer tokenizer = new StringTokenizer(line);          while (tokenizer.hasMoreTokens()) {              word.set(tokenizer.nextToken());              context.write(word, one);          }      }  }  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {      public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)              throws IOException, InterruptedException {          int sum = 0;          for (IntWritable val : values) {              sum += val.get();          }          context.write(key, new IntWritable(sum));      }  }  public static void main(String[] args) throws Exception {      Configuration conf = new Configuration();      Job job = new Job(conf, &quot;wordcount&quot;);      job.setJarByClass(WordCount.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);      job.setMapperClass(Map.class);      job.setReducerClass(Reduce.class);      job.setInputFormatClass(TextInputFormat.class);      job.setOutputFormatClass(TextOutputFormat.class);      FileInputFormat.addInputPath(job, new Path(args[0]));      FileOutputFormat.setOutputPath(job, new Path(args[1]));      job.waitForCompletion(true);  }}</code></pre></li></ul><h2 id="使用maven打包程序"><a href="#使用maven打包程序" class="headerlink" title="使用maven打包程序"></a>使用maven打包程序</h2><ul><li><p>打包命令</p><pre><code>项目右键&gt;run as&gt;maven build</code></pre></li><li><p>打包后jar的结构</p><pre><code>C:.└─wordcount  ├─com  │  └─jordiburgos  └─META-INF      └─maven          └─com.jordiburgos              └─wordcount</code></pre></li></ul><h2 id="在hadoop上运行程序"><a href="#在hadoop上运行程序" class="headerlink" title="在hadoop上运行程序"></a>在hadoop上运行程序</h2><ul><li><p>上传待分析的文本到hdfs</p><pre><code># 本地创建input文件夹和a.txt文件cd /root/app/hadoop-3.1.0mkdir inputvim a.txt## 创建文件夹hadoop fs -mkdir hdfs://localhost:9001/tmp## 上传文件到hdfshadoop fs -put /root/app/hadoop-3.1.0/input hdfs://127.0.0.1:9001/tmp</code></pre></li><li><p>运行jar程序</p><pre><code>cd /root/app/hadoop-3.1.0bin/hadoop jar wordcount.jar com.jordiburgos.WordCount hdfs://localhost:9001/tmp/input/ file:///root/app/hadoop-3.1.0/output/</code></pre></li><li><p>在linux中查看输出文件</p><pre><code>cd /root/app/hadoop-3.1.0/outputroot@ubuntu:~/app/hadoop-3.1.0/output# lspart-r-00000  _SUCCESSroot@ubuntu:~/app/hadoop-3.1.0/output# cat part-r-000000000    1aaaa    1ddfh    1ff    1ggg    1hj    1iiiii    1sss    1</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> maven </tag>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ubuntu中hadoop单机模式和伪分布式搭建</title>
      <link href="/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<p>本文介绍如何搭建hadoop单机版本/独立模式/standalone模式？<br><a id="more"></a></p><h2 id="ubuntu开启root用户登录的方法"><a href="#ubuntu开启root用户登录的方法" class="headerlink" title="ubuntu开启root用户登录的方法"></a>ubuntu开启root用户登录的方法</h2><ul><li>设置密码、添加信息<pre><code>sudo passwd -u rootsudo passwd rootsu rootcd /usr/share/lightdm/lightdm.conf.d/vim 50-unity-greeter.conf# 添加user-session=ubuntugreeter-show-manual-login=trueall-guest=false# 重启reboot# 使用user和passwd进入root报错vim /root/.profile# 找到mesg n || true# 改为tty -s &amp;&amp; mesg n || true</code></pre></li></ul><h2 id="ubuntu中的java环境变量配置"><a href="#ubuntu中的java环境变量配置" class="headerlink" title="ubuntu中的java环境变量配置"></a>ubuntu中的java环境变量配置</h2><ul><li>编辑 sudo vim /etc/profile<pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binsource /etc/profile</code></pre></li><li>验证<pre><code>java -version</code></pre></li></ul><h2 id="单机版hadoop配置"><a href="#单机版hadoop配置" class="headerlink" title="单机版hadoop配置"></a>单机版hadoop配置</h2><ul><li><p><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation" target="_blank" rel="noopener">官方文档</a></p></li><li><p>生成ssh密钥</p><pre><code>cd ~ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keysssh localhost</code></pre></li><li><p>配置java环境</p><pre><code>root@ubuntu:~# vim /etc/profileexport JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</code></pre></li><li><p>配置hadoop环境 vim /etc/profile</p><pre><code>#HADOOP VARIABLES STARTexport JAVA_HOME=/root/app/jdk1.8.0_171export HADOOP_HOME=/root/app/hadoop-3.1.0export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#HADOOP VARIABLES ENDsource /etc/profile</code></pre></li><li><p>单机版测试</p><pre><code>root@ubuntu:~# /root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jarAn example program must be given as the first argument.Valid program names are:aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.dbcount: An example job that count the pageview counts from a database.distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.grep: A map/reduce program that counts the matches of a regex in the input.join: A job that effects a join over sorted, equally partitioned datasetsmultifilewc: A job that counts words from several files.pentomino: A map/reduce tile laying program to find solutions to pentomino problems.pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.randomwriter: A map/reduce program that writes 10GB of random data per node.secondarysort: An example defining a secondary sort to the reduce.sort: A map/reduce program that sorts the data written by the random writer.sudoku: A sudoku solver.teragen: Generate data for the terasortterasort: Run the terasortteravalidate: Checking results of terasortwordcount: A map/reduce program that counts the words in the input files.wordmean: A map/reduce program that counts the average length of the words in the input files.wordmedian: A map/reduce program that counts the median length of the words in the input files.wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.</code></pre></li><li><p>实例测试</p><pre><code>/root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar grep ./input ./output &#39;dfs[a-z.]+&#39;cat ./output/*  # 查看结果rm -r ./output  # 删除结果# 结果root@ubuntu:~/app/hadoop-3.1.0# cat ./output/*  1    dfsadmin</code></pre></li></ul><h2 id="伪分布式hadoop配置"><a href="#伪分布式hadoop配置" class="headerlink" title="伪分布式hadoop配置"></a>伪分布式hadoop配置</h2><ul><li><p>格式化hdfs</p><pre><code>cd /root/app/hadoop-3.1.0./bin/hdfs namenode -format</code></pre></li><li><p>添加变量到 vim /etc/profile</p><pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin## HADOOP VARIABLES STARTexport JAVA_HOME=/root/app/jdk1.8.0_171export HADOOP_HOME=/root/app/hadoop-3.1.0#export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin#export HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOME#export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#export HDFS_DATANODE_USER=rootexport HDFS_NAMENODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=root#export YARN_RESOURCEMANAGER_USER=rootexport HADOOP_SECURE_DN_USER=yarnexport YARN_NODEMANAGER_USER=root# HADOOP VARIABLES END</code></pre></li><li><p>编辑/root/app/hadoop-3.1.0/etc/hadoop/core-site.xml</p><pre><code>&lt;configuration&gt;     &lt;property&gt;          &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;          &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;     &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>编辑/root/app/hadoop-3.1.0/etc/hadoop/hdfs-site.xml</p><pre><code>&lt;configuration&gt;      &lt;property&gt;           &lt;name&gt;dfs.replication&lt;/name&gt;           &lt;value&gt;2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;           &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/name&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;           &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/data&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.http.address&lt;/name&gt;          &lt;value&gt;0.0.0.0:50070&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>编辑 hadoop-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/hadoop-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p>编辑 yarn-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/yarn-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p>编辑 mapred-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/mapred-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li></ul><h2 id="hadoop的使用"><a href="#hadoop的使用" class="headerlink" title="hadoop的使用"></a>hadoop的使用</h2><ul><li><p>启动与停止</p><pre><code>cd /root/app/hadoop-3.1.0./sbin/start-all.sh./sbin/stop-all.sh</code></pre></li><li><p>查看服务</p><pre><code>root@ubuntu:~/app/hadoop-3.1.0# jps23058 NameNode23491 SecondaryNameNode23753 ResourceManager23225 DataNode24427 Jps24030 NodeManager</code></pre></li><li>Resource Manager <a href="http://localhost:8088" target="_blank" rel="noopener">http://localhost:8088</a></li><li>Web UI of the NameNode daemon <a href="http://localhost:50070" target="_blank" rel="noopener">http://localhost:50070</a></li><li>HDFS NameNode web interface <a href="http://localhost:8042" target="_blank" rel="noopener">http://localhost:8042</a></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>常见的环境变量配置</title>
      <link href="/2018/04/17/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/04/17/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>本文介绍常见的环境变量配置方法。</p><a id="more"></a><h2 id="windows常见的环境变量配置"><a href="#windows常见的环境变量配置" class="headerlink" title="windows常见的环境变量配置"></a>windows常见的环境变量配置</h2><ul><li><p>CLASSPATH</p><pre><code>.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</code></pre></li><li><p>JAVA_HOME</p><pre><code>C:\app3\Java\jdk1.8.0_144</code></pre></li><li><p>JRE_HOME</p><pre><code>C:\app3\Java\jre1.8.0_144</code></pre></li><li><p>MVN_HOME<br>C:\app3\apache-maven-3.5.3</p></li><li><p>Path</p><pre><code>C:\app3\Python35\Scripts\;C:\app3\Python35\;C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%JAVA_HOME%\bin;%JRE_HOME%\bin;C:\app3\Git\cmd;C:\app3\MinGW\bin;C:\app3\nodejs\;C:\app3\MATLAB\R2017b\runtime\win64;C:\app3\MATLAB\R2017b\bin;%MVN_HOME%\bin;</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> windows </tag>
            
            <tag> 环境变量 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>apache maven的配置与使用</title>
      <link href="/2018/04/17/maven%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/04/17/maven%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
      <content type="html"><![CDATA[<p>本文介绍了apache maven的配置与使用过程，【清理项目】→【编译项目】→【测试项目】→【生成测试报告】→【打包项目】→【部署项目】，maven详细讲解：<a href="https://www.yiibai.com/maven/" target="_blank" rel="noopener">他山之石</a></p><a id="more"></a><h2 id="需要先配置java和maven环境变量"><a href="#需要先配置java和maven环境变量" class="headerlink" title="需要先配置java和maven环境变量"></a><strong>需要先配置java和maven环境变量</strong></h2><ul><li><p>CLASSPATH</p><pre><code>.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</code></pre></li><li><p>JAVA_HOME</p><pre><code>C:\app3\Java\jdk1.8.0_144</code></pre></li><li><p>JRE_HOME</p><pre><code>C:\app3\Java\jre1.8.0_144</code></pre></li><li><p>MVN_HOME</p><pre><code>C:\app3\apache-maven-3.5.3</code></pre></li><li><p>Path</p><pre><code>C:\app3\Python35\Scripts\;C:\app3\Python35\;C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%JAVA_HOME%\bin;%JRE_HOME%\bin;C:\app3\Git\cmd;C:\app3\MinGW\bin;C:\app3\nodejs\;C:\app3\MATLAB\R2017b\runtime\win64;C:\app3\MATLAB\R2017b\bin;%MVN_HOME%\bin;</code></pre></li></ul><h2 id="更换maven的仓库为自定义的仓库"><a href="#更换maven的仓库为自定义的仓库" class="headerlink" title="更换maven的仓库为自定义的仓库"></a><strong>更换maven的仓库为自定义的仓库</strong></h2><ul><li>创建目标位置如，d:\maven\repo</li><li>拷贝C:\app3\apache-maven-3.5.3\conf\settings.xml文件到d:\maven</li><li>修改两处的settings.xml文件</li><li>定位到localRepository<pre><code>&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;# 修改为：&lt;localRepository&gt;d:\maven\repo&lt;/localRepository&gt;</code></pre></li></ul><h2 id="maven手动创建项目"><a href="#maven手动创建项目" class="headerlink" title="maven手动创建项目"></a><strong>maven手动创建项目</strong></h2><ul><li><p><a href="https://www.cnblogs.com/yjmyzz/p/3495762.html" target="_blank" rel="noopener">他山之石</a></p></li><li><p>创建项目</p><pre><code># cmdcd /d d:\testmvn archetype:generate# logChoose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): 1169:（和eclipse上的maven插件有关系，直接回车）Choose org.apache.maven.archetypes:maven-archetype-quickstart version:1: 1.0-alpha-12: 1.0-alpha-23: 1.0-alpha-34: 1.0-alpha-45: 1.06: 1.17: 1.3Choose a number: 7:(直接回车)Define value for property &#39;groupId&#39;: com.hikvision.ai_data.data（从大往小填写自己公司的名字）Define value for property &#39;artifactId&#39;: test_mvn（项目的名字）Define value for property &#39;version&#39; 1.0-SNAPSHOT: :（默认就行）Define value for property &#39;package&#39; com.hikvision.ai_data.data: : test_mvn_pkg（将class打包的jar文件的名称）Confirm properties configuration:groupId: com.hikvision.ai_data.dataartifactId: test_mvnversion: 1.0-SNAPSHOTpackage: test_mvn_pkgY: :(直接回车)[INFO] ----------------------------------------------------------------------------[INFO] Using following parameters for creating project from Archetype: maven-archetype-quickstart:1.3[INFO] ----------------------------------------------------------------------------[INFO] Parameter: groupId, Value: com.hikvision.ai_data.data[INFO] Parameter: artifactId, Value: test_mvn[INFO] Parameter: version, Value: 1.0-SNAPSHOT[INFO] Parameter: package, Value: test_mvn_pkg[INFO] Parameter: packageInPathFormat, Value: test_mvn_pkg[INFO] Parameter: package, Value: test_mvn_pkg[INFO] Parameter: version, Value: 1.0-SNAPSHOT[INFO] Parameter: groupId, Value: com.hikvision.ai_data.data[INFO] Parameter: artifactId, Value: test_mvn[INFO] Project created from Archetype in dir: D:\003---WorkSpace\06---testmaven\test_mvn[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 13:57 min[INFO] Finished at: 2018-04-17T14:54:37+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>创建项目后查看文件</p><pre><code>D:\003---WorkSpace\06---testmaven&gt;tree卷 工厂 的文件夹 PATH 列表卷序列号为 0000006C BAA7:827CD:.└─test_mvn  └─src      ├─main      │  └─java      │      └─test_mvn_pkg      └─test          └─java              └─test_mvn_pkg</code></pre></li><li><p>编译项目</p><pre><code># cmdcd test_mvnmvn clean compile# logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean compile[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn ---[INFO][INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.570 s[INFO] Finished at: 2018-04-17T14:58:59+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>单元测试</p><pre><code># cmdmvn clean test# logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean test[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn ---[INFO] Deleting D:\003---WorkSpace\06---testmaven\test_mvn\target[INFO][INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes[INFO][INFO] --- maven-resources-plugin:3.0.2:testResources (default-testResources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\test\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\test-classes[INFO][INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ test_mvn ---[INFO][INFO] -------------------------------------------------------[INFO]  T E S T S[INFO] -------------------------------------------------------[INFO] Running test_mvn_pkg.AppTest[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.057 s - in test_mvn_pkg.AppTest[INFO][INFO] Results:[INFO][INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 3.227 s[INFO] Finished at: 2018-04-17T15:00:33+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>打包项目</p><pre><code># cmdmvn clean package# logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean package[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn ---[INFO] Deleting D:\003---WorkSpace\06---testmaven\test_mvn\target[INFO][INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes[INFO][INFO] --- maven-resources-plugin:3.0.2:testResources (default-testResources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\test\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\test-classes[INFO][INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ test_mvn ---[INFO][INFO] -------------------------------------------------------[INFO]  T E S T S[INFO] -------------------------------------------------------[INFO] Running test_mvn_pkg.AppTest[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.052 s - in test_mvn_pkg.AppTest[INFO][INFO] Results:[INFO][INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO][INFO][INFO] --- maven-jar-plugin:3.0.2:jar (default-jar) @ test_mvn ---[INFO] Building jar: D:\003---WorkSpace\06---testmaven\test_mvn\target\test_mvn-1.0-SNAPSHOT.jar[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 3.599 s[INFO] Finished at: 2018-04-17T15:02:35+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>运行项目</p><pre><code># cmd# 1.无参数，类在target下面test_mvn\target\classes\test_mvn_pkg\App.classmvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot;# 即mvn exec:java -Dexec.mainClass=&quot;test_mvn_pkg.App&quot;## 2.有参数mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.args=&quot;arg0 arg1 arg2&quot;## 3.指定对classpath的运行时依赖mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.classpathScope=runtime## logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn exec:java -Dexec.mainClass=&quot;test_mvn_pkg.App&quot;[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- exec-maven-plugin:1.6.0:java (default-cli) @ test_mvn ---Hello World![INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.010 s[INFO] Finished at: 2018-04-17T15:09:49+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>项目部署</p><pre><code># 前提是jboss web server已经成功启动# cmdmvn clean jboss-as:deploy</code></pre></li></ul><h2 id="eclipse上的maven项目"><a href="#eclipse上的maven项目" class="headerlink" title="eclipse上的maven项目"></a><strong>eclipse上的maven项目</strong></h2><ul><li><p>打开eclipse进行java配置，然后关闭</p><pre><code>windows &gt; preferences &gt; java &gt; installed jres &gt; add jdk floder and jre floderselect jdk floder &gt; apply</code></pre></li><li><p>eclipse上的maven插件M2Eclipse</p><pre><code>help menu &gt; install new software &gt; input the url as followhttp://download.eclipse.org/technology/m2e/releases/# 备注插件官网http://www.eclipse.org/m2e/# 该插件可以解决mvn install报错问题</code></pre></li><li><p>eclipse中的maven配置</p><pre><code>windows &gt; preferences &gt; maven &gt; installations &gt; maven &gt; $(maven_home) &gt; applywindows &gt; preferences &gt; maven &gt; users seting &gt; user setting &gt; C:\app3\apache-maven-3.5.3\conf\settings.xmlwindows &gt; preferences &gt; maven &gt; users seting &gt; local repository &gt; C:\Users\BinLee\.m2\repository</code></pre></li><li><p>创建maven项目</p></li></ul><ol><li>创建 New-&gt;Other…-&gt;Maven-&gt;Maven Project</li><li>use default workspace location</li><li>archetypes maven-archetype-quickstart</li><li><p>new maven project</p><pre><code>com.hikvision.big_data.datatest_eclipse_maven0.0.1-SNAPSHOTcom.hikvision.big_data.data.test_eclipse_maven</code></pre></li><li><p>其中pom.xml</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.hikvision.big_data.data&lt;/groupId&gt;&lt;artifactId&gt;test_eclipse_maven&lt;/artifactId&gt;&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;name&gt;test_eclipse_maven&lt;/name&gt;&lt;url&gt;http://maven.apache.org&lt;/url&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt;   &lt;groupId&gt;junit&lt;/groupId&gt;   &lt;artifactId&gt;junit&lt;/artifactId&gt;   &lt;version&gt;3.8.1&lt;/version&gt;   &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;/project&gt;</code></pre></li><li><p>在网站上找到自己需要的依赖 <a href="http://mvnrepository.com/" target="_blank" rel="noopener">http://mvnrepository.com/</a></p><pre><code># 比如：我需要找到time相关的操作，直接mavenrepository中搜索time， 得到的Joda Time# 再用google搜索Joda Time，查看其用法# mavenrepository中的Joda Time依赖添加到pom.xml&lt;!-- https://mvnrepository.com/artifact/org.webjars.npm/d3-array --&gt;&lt;!-- https://mvnrepository.com/artifact/joda-time/joda-time --&gt;&lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;version&gt;2.9.9&lt;/version&gt;&lt;/dependency&gt;# 保存自动下载# 使用everything搜索Joda Time发现已经在C:\Users\BinLee\.m2\repository\joda-time\joda-time\2.9.9\joda-time-2.9.9.jar下面</code></pre></li><li><p>使用依赖</p><pre><code># 在窗口上project explorer&gt;maven dependencies查看需要的依赖类# 在需要地方直接插入# codepackage com.hikvision.big_data.data.test_eclipse_maven;import org.joda.time.DateTime;import org.joda.time.Days;import org.joda.time.LocalDateTime;/*** Hello world!**/public class App { public static void main(String[] args) {     System.out.println(&quot;Hello World!&quot;);     DateTime now = DateTime.now();     System.out.println(now);     Days maxValue = Days.MAX_VALUE;     System.out.println(maxValue);     System.out.println(LocalDateTime.now()); }}# outputHello World!2018-04-17T16:15:02.211+08:00P2147483647D2018-04-17T16:15:02.289</code></pre></li></ol><h2 id="关于maven源码打包"><a href="#关于maven源码打包" class="headerlink" title="关于maven源码打包"></a><strong>关于maven源码打包</strong></h2><ul><li><p>命令行方式，<a href="https://blog.csdn.net/symgdwyh/article/details/4407945" target="_blank" rel="noopener">他山之石</a></p><pre><code>cd {项目目录下}mvn source:jarmvn source:test-jar</code></pre></li><li><p>eclipse中pom.xml结尾加入插件，然后执行maven install</p><pre><code>...  &lt;/dependencies&gt;  &lt;build&gt;      &lt;plugins&gt;          &lt;plugin&gt;              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;              &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;              &lt;executions&gt;                  &lt;execution&gt;                      &lt;id&gt;attach-sources&lt;/id&gt;                      &lt;goals&gt;                          &lt;goal&gt;jar&lt;/goal&gt;                      &lt;/goals&gt;                  &lt;/execution&gt;              &lt;/executions&gt;          &lt;/plugin&gt;      &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre></li></ul><h2 id="利用idea运行maven-install报错的问题解决"><a href="#利用idea运行maven-install报错的问题解决" class="headerlink" title="利用idea运行maven install报错的问题解决"></a><strong>利用idea运行maven install报错的问题解决</strong></h2><ul><li>参考 <a href="http://tieba.baidu.com/p/4810060893?traceid=" target="_blank" rel="noopener">他山之石</a></li><li>右边maven projects &gt; lifecycle &gt; install</li><li>不要点击plugins &gt; install会报错</li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> maven </tag>
            
            <tag> 配置 </tag>
            
            <tag> windows </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>eclipse中的git基本配置</title>
      <link href="/2018/04/16/eclipse%E4%B8%AD%E7%9A%84git%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/04/16/eclipse%E4%B8%AD%E7%9A%84git%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>本文介绍了如何使用git和eclipse进行代码的版本控制。<br><a id="more"></a></p><h2 id="命令行模式操作"><a href="#命令行模式操作" class="headerlink" title="命令行模式操作"></a>命令行模式操作</h2><ul><li>服务端注册github或者giteee账号</li><li>客户端下载git软件</li><li>使用命令生成本地的密钥</li><li>将秘钥添加到服务端的git中</li><li>服务端新建git仓库，客户端克隆到本地</li><li>客户端添加文件到仓库中，使用各种命令对该仓库进行版本控制</li><li>上述的属于git的基本操作详细步骤参考 <a href="https://leebin.top/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ubuntu%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89git%E6%9C%8D%E5%8A%A1%E7%AB%AF-%E9%99%84ssh%E6%93%8D%E4%BD%9C%EF%BC%9F/" target="_blank" rel="noopener">如何利用ubuntu实现私有git服务端-附ssh操作？</a></li></ul><h2 id="eclipse中git上传代码"><a href="#eclipse中git上传代码" class="headerlink" title="eclipse中git上传代码"></a>eclipse中git上传代码</h2><ul><li>服务端已经添加了客户端的ssh密钥</li><li>服务端已经新建了仓库</li><li>客户端eclipse新建项目</li><li>在路径eclipse&gt;windows&gt;preference&gt;team&gt;git&gt;configuration下查看user和passwd的配置</li><li>在路径package explorer&gt;项目右键&gt;share project&gt;repository&gt;create，新建本地的仓库名字要和服务端的名字一致，如：d:\test.git，完成了新建仓库</li><li>在路径package explorer&gt;项目右键&gt;team&gt;add to index，完成文件的add</li><li>在路径package explorer&gt;项目右键&gt;team&gt;commit或者Ctrl+#，提交</li><li>接上一步，先填写commit message</li><li>接上一步，填写服务器地址<pre><code>remote name: originurl: git@github.com:xjdlb/testgit.git # git 地址hostname: github.com # 域名repository path: xjdlb/testgit.git</code></pre></li><li>一路next就好了</li><li><a href="https://blog.csdn.net/u014079773/article/details/51595127" target="_blank" rel="noopener">他山之石</a></li></ul><h2 id="eclipse中git下载代码"><a href="#eclipse中git下载代码" class="headerlink" title="eclipse中git下载代码"></a>eclipse中git下载代码</h2><ul><li>在路径package explorer&gt;空白右键&gt;import&gt;Git&gt;Projects from Git，next</li><li>接上步，选择URI，包含了远程和本地</li><li>主要的分支</li><li>新建本地的仓库，如：d:\test.git</li><li>继续coding</li><li>返回上面上传代码操作</li></ul><h2 id="eclipse-push-出现了-rejected-non-fast-forward错误"><a href="#eclipse-push-出现了-rejected-non-fast-forward错误" class="headerlink" title="eclipse push 出现了 rejected-non-fast-forward错误"></a>eclipse push 出现了 rejected-non-fast-forward错误</h2><ul><li><a href="https://blog.csdn.net/chenshun123/article/details/46756087" target="_blank" rel="noopener">他山之石</a></li><li>打开windows&gt;show view&gt;other&gt;git repositories</li><li>git repositories&gt;remote&gt;origin&gt;绿色分支&gt;右键&gt;configure fetch&gt;save and fetch</li><li>此时可以看见remote tracking&gt;origin/mater&gt;右键&gt;merge</li><li>问题解决，可以上传了</li><li>add&gt;commit&gt;push</li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> eclipse </tag>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用kcptun加速ss服务</title>
      <link href="/2018/04/15/%E4%BD%BF%E7%94%A8kcptun%E5%8A%A0%E9%80%9Fss%E6%9C%8D%E5%8A%A1/"/>
      <url>/2018/04/15/%E4%BD%BF%E7%94%A8kcptun%E5%8A%A0%E9%80%9Fss%E6%9C%8D%E5%8A%A1/</url>
      <content type="html"><![CDATA[<p>本文介绍如何使用kcptun加速ss服务。<br><a id="more"></a></p><h2 id="软件准备"><a href="#软件准备" class="headerlink" title="软件准备"></a>软件准备</h2><ul><li>安装组件<pre><code>apt-get updateapt-get upgradeapt-get install build-essential python-pip m2crypto supervisor</code></pre></li><li>安装ss<pre><code>pip install shadowsocks</code></pre></li><li>安装加密用软件 libsodium<pre><code>wget https://github.com/jedisct1/libsodium/releases/download/1.0.11/libsodium-1.0.11.tar.gztar zxvf libsodium-1.0.11.tar.gzcd libsodium-1.0.11./configuremake &amp;&amp; make checkmake installecho /usr/local/lib &gt; /etc/ld.so.conf.d/usr_local_lib.confldconfig</code></pre></li><li><p>[可选] 配置supervisor, vi /etc/supervisor/conf.d/shadowsocks.conf</p><pre><code>[program:shadowsocks]command=ssserver -c /etc/shadowsocks.jsonautorestart=trueuser=root</code></pre></li><li><p>[可选] 使用supervisor</p><pre><code>supervisorctl reloadsupervisorctl status</code></pre></li></ul><h2 id="ss配置"><a href="#ss配置" class="headerlink" title="ss配置"></a>ss配置</h2><ul><li>ss服务器配置ss_config.json<pre><code>{&quot;server&quot;: &quot;127.0.0.1&quot;,&quot;port_password&quot;: {&quot;10001&quot;: &quot;helloworld&quot;,&quot;10002&quot;: &quot;helloworld&quot;,&quot;10003&quot;: &quot;helloworld&quot;},&quot;local_port&quot;: 1080,&quot;timeout&quot;: 600,&quot;method&quot;: &quot;chacha20&quot;,&quot;auth&quot;: true}</code></pre></li><li>启动和停止脚本<pre><code>ssserver -c /root/shadowsocks/ss_config.json -d startssserver -c /root/shadowsocks/ss_config.json -d stop</code></pre></li></ul><h2 id="kcptun配置"><a href="#kcptun配置" class="headerlink" title="kcptun配置"></a>kcptun配置</h2><ul><li>kcptun官网 <a href="https://github.com/xtaci/kcptun/releases" target="_blank" rel="noopener">https://github.com/xtaci/kcptun/releases</a></li><li>其中 kcptun-linux-amd64-20180316.tar.gz 为Linux版本</li><li>其中 kcptun-windows-amd64-20180316.tar.gz 为Windows版本</li><li>安装 kcptun<pre><code>mkdir /root/kcptuncd /root/kcptunln -sf /bin/bash /bin/shwget https://github.com/xtaci/kcptun/releases/download/v20161118/kcptun-linux-amd64-20161118.tar.gztar -zxf kcptun-linux-amd64-*.tar.gz</code></pre></li><li>配置三个脚本start.sh, stop.sh, server-config.json</li></ul><ol><li><p>启动脚本vi /root/kcptun/start.sh</p><pre><code>#!/bin/bashcd /root/kcptun/./server_linux_amd64 -c /root/kcptun/server-config.json &gt; kcptun.log 2&gt;&amp;1 &amp;echo &quot;Kcptun started.&quot;</code></pre></li><li><p>停止脚本 vi /root/kcptun/stop.sh</p><pre><code>#!/bin/bashecho &quot;Stopping Kcptun...&quot;PID=`ps -ef | grep server_linux_amd64 | grep -v grep | awk &#39;{print $2}&#39;`if [ &quot;&quot; !=  &quot;$PID&quot; ]; thenecho &quot;killing $PID&quot;kill -9 $PIDfiecho &quot;Kcptun stoped.&quot;</code></pre></li><li><p>kcptun配置文件 vi /root/kcptun/server-config.json</p><pre><code>{&quot;listen&quot;: &quot;:443&quot;,&quot;target&quot;: &quot;127.0.0.1:10001&quot;,&quot;key&quot;: &quot;helloworld&quot;,&quot;crypt&quot;: &quot;salsa20&quot;,&quot;mode&quot;: &quot;fast2&quot;,&quot;mtu&quot;: 1350,&quot;sndwnd&quot;: 1024,&quot;rcvwnd&quot;: 1024,&quot;datashard&quot;: 5,&quot;parityshard&quot;: 5,&quot;dscp&quot;: 46,&quot;nocomp&quot;: true,&quot;acknodelay&quot;: false,&quot;nodelay&quot;: 0,&quot;interval&quot;: 40,&quot;resend&quot;: 0,&quot;nc&quot;: 0,&quot;sockbuf&quot;: 4194304,&quot;keepalive&quot;: 10}</code></pre></li></ol><ul><li>启动或停止kcptun<pre><code>sh /root/kcptun/start.shsh /root/kcptun/stop.sh</code></pre></li></ul><hr><h2 id="客户端windows环境中的kcptun配置"><a href="#客户端windows环境中的kcptun配置" class="headerlink" title="客户端windows环境中的kcptun配置"></a>客户端windows环境中的kcptun配置</h2><ul><li>kcptun官网 <a href="https://github.com/xtaci/kcptun/releases" target="_blank" rel="noopener">https://github.com/xtaci/kcptun/releases</a></li><li>client_windows_amd64.exe 放在全部英文目录下</li><li>创建下面的三个文件：run.vbs, client-config.json, stop.sh</li></ul><ol><li>在当前文件夹下，创建 run.vbs<pre><code>Dim RunKcptunSet fso = CreateObject(&quot;Scripting.FileSystemObject&quot;)Set WshShell = WScript.CreateObject(&quot;WScript.Shell&quot;)currentPath = fso.GetFile(Wscript.ScriptFullName).ParentFolder.Path &amp; &quot;\&quot;configFile = currentPath &amp; &quot;client-config.json&quot;logFile = currentPath &amp; &quot;kcptun.log&quot;exeConfig = currentPath &amp; &quot;client_windows_amd64.exe -c &quot; &amp; configFilecmdLine = &quot;cmd /c &quot; &amp; exeConfig &amp; &quot; &gt; &quot; &amp; logFile &amp; &quot; 2&gt;&amp;1&quot;WshShell.Run cmdLine, 0, False&#39;WScript.Sleep 1000&#39;Wscript.echo cmdLineSet WshShell = NothingSet fso = NothingWScript.quit</code></pre></li><li>在当前文件夹下，创建client-config.json<pre><code>{&quot;localaddr&quot;: &quot;:12345&quot;,&quot;remoteaddr&quot;: &quot;165.227.213.57:443&quot;,&quot;key&quot;: &quot;helloworld&quot;,&quot;crypt&quot;: &quot;salsa20&quot;,&quot;mode&quot;: &quot;fast2&quot;,&quot;conn&quot;: 1,&quot;autoexpire&quot;: 60,&quot;mtu&quot;: 1350,&quot;sndwnd&quot;: 128,&quot;rcvwnd&quot;: 1024,&quot;datashard&quot;: 5,&quot;parityshard&quot;: 5,&quot;dscp&quot;: 46,&quot;nocomp&quot;: true,&quot;acknodelay&quot;: false,&quot;nodelay&quot;: 0,&quot;interval&quot;: 40,&quot;resend&quot;: 0,&quot;nc&quot;: 0,&quot;sockbuf&quot;: 4194304,&quot;keepalive&quot;: 10}</code></pre></li><li>在当前文件夹下，创建stop.sh<pre><code>taskkill /f /im client_windows_amd64.exe</code></pre></li></ol><h2 id="客户端windows环境中的ss配置"><a href="#客户端windows环境中的ss配置" class="headerlink" title="客户端windows环境中的ss配置"></a>客户端windows环境中的ss配置</h2><ul><li>使用本地的配置<pre><code>127.0.0.112345helloworld(服务端ss的密码，不是kcptun的密码)chacha20</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> kcptun </tag>
            
            <tag> ss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hexo如何开启语法高亮？</title>
      <link href="/2018/04/03/hexo%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%EF%BC%9F/"/>
      <url>/2018/04/03/hexo%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文介绍hexo有关语法高亮的配置方案。<br><a id="more"></a></p><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><ul><li>配置主站点下的配置文件<pre><code>highlight:enable: trueline_number: trueauto_detect: truetab_replace:</code></pre></li><li>代码后面添加名称，如```java code ```</li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LC_001_TwoSum_HashMap</title>
      <link href="/2018/04/03/LC_001_TwoSum_HashMap/"/>
      <url>/2018/04/03/LC_001_TwoSum_HashMap/</url>
      <content type="html"><![CDATA[<p>leetcode第001题，主要用到了hashmap数据结构。<br><a id="more"></a></p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><pre><code class="java">package LC;import java.util.Arrays;import java.util.HashMap;/** * https://leetcode.com/problems/two-sum/description/ * Given an array of integers, * return indices of the two numbers such that they add up to a specific target. * You may assume that each input would have exactly one solution, * and you may not use the same element twice. * Example: * Given nums = [2, 7, 11, 15], target = 9, * Because nums[0] + nums[1] = 2 + 7 = 9, * return [0, 1]. */public class LC_001_TwoSum_HashMap {    public static void main(String[] args) {        int[] a = {1, 2, 3, 4, 5, 7};        int t = 10;        System.out.println(Arrays.toString(twoSum(a, t)));    }    private static int[] twoSum(int[] nums, int target) {        HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();        for (int i = 0; i &lt; nums.length; i++) {            int diff = target - nums[i];            if (map.containsKey(diff)) return new int[]{map.get(diff), i};            map.put(nums[i], i);        }        throw new IllegalArgumentException(&quot;-1&quot;);    }}</code></pre>]]></content>
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> leetcode </tag>
            
            <tag> java </tag>
            
            <tag> basic algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何去掉valine的Powered By信息？</title>
      <link href="/2018/04/03/%E5%A6%82%E4%BD%95%E5%8E%BB%E6%8E%89valine%E7%9A%84Powered%20By%E4%BF%A1%E6%81%AF%EF%BC%9F/"/>
      <url>/2018/04/03/%E5%A6%82%E4%BD%95%E5%8E%BB%E6%8E%89valine%E7%9A%84Powered%20By%E4%BF%A1%E6%81%AF%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文介绍如何去掉valine页面上的Powered By信息。<br><a id="more"></a></p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul><li>找到配置文件<pre><code>blog/themes/next/layout/_third-party/comments/valine.swig</code></pre></li><li>配置如下<pre><code>{% if theme.valine.enable and theme.valine.appid and theme.valine.appkey %}  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>  <script type="text/javascript">    var GUEST = ['nick','mail','link'];    var guest = '{{ theme.valine.guest_info }}';    guest = guest.split(',').filter(item=>{      return GUEST.indexOf(item)>-1;    });    new Valine({        el: '#comments' ,        verify: {{ theme.valine.verify }},        notify: {{ theme.valine.notify }},        appId: '{{ theme.valine.appid }}',        appKey: '{{ theme.valine.appkey }}',        placeholder: '{{ theme.valine.placeholder }}',        avatar:'{{ theme.valine.avatar }}',        guest_info:guest,        pageSize:'{{ theme.valine.pageSize }}' || 10,    });//新增以下代码即可，可以移除.info下所有子节点。var infoEle = document.querySelector('#comments .info');if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){  infoEle.childNodes.forEach(function(item) {item.parentNode.removeChild(item);  });}  </script>{% endif %}</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> valine </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何在next配置站内的搜索引擎？</title>
      <link href="/2018/04/03/%E5%A6%82%E4%BD%95%E5%9C%A8next%E9%85%8D%E7%BD%AE%E7%AB%99%E5%86%85%E7%9A%84%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%9F/"/>
      <url>/2018/04/03/%E5%A6%82%E4%BD%95%E5%9C%A8next%E9%85%8D%E7%BD%AE%E7%AB%99%E5%86%85%E7%9A%84%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文介绍如何在next配置站内的搜索引擎。<br><a id="more"></a></p><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><ul><li>安装hexo-generator-searchdb插件，以管理员身份打开cmd进入项目目录下，运行<pre><code>npm install hexo-generator-searchdb --save</code></pre></li><li>在站点的-config.yml文件中增加<pre><code>search:path: search.xmlfield: postformat: htmllimit: 10000</code></pre></li><li>配置theme/next/-config.yml文件<pre><code># Algolia Searchalgolia_search:enable: falsehits:  per_page: 10labels:  input_placeholder: Search for Posts  hits_empty: &quot;We didn&#39;t find any results for the search: ${query}&quot;  hits_stats: &quot;${hits} results found in ${time} ms&quot;## Local search# Dependencies: https://github.com/flashlab/hexo-generator-searchlocal_search:enable: true# if auto, trigger search by changing input# if manual, trigger search by pressing enter key or search buttontrigger: auto# show top n results per article, show all results by setting to -1top_n_per_article: 1</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> next </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>linux shell入门练习</title>
      <link href="/2018/04/03/linux%20shell%E5%85%A5%E9%97%A8%E7%BB%83%E4%B9%A0/"/>
      <url>/2018/04/03/linux%20shell%E5%85%A5%E9%97%A8%E7%BB%83%E4%B9%A0/</url>
      <content type="html"><![CDATA[<p>本文介绍linux shell的入门程序，后期会陆续更新。<br><a id="more"></a></p><h2 id="基本程序实例"><a href="#基本程序实例" class="headerlink" title="基本程序实例"></a>基本程序实例</h2><ul><li><p>计算文件夹下的文件数量</p><pre><code class="bash">#!/bin/bashecho &quot;this is a shell print file&#39;s number in the local dir.&quot;ls &gt; filename.logy=1for i in $( cat filename.log )  do      echo &quot;the file number is $y&quot;      y=$(( $y + 1 ))  donerm -rf filename.log</code></pre></li><li><p>简单求和程序</p><pre><code class="bash">#!/bin/bash# author: leebins=0for(( i=1; i&lt;=100; i=i+1 ))  do      s=$(( $s+$i ))  doneecho &quot;the sum of 1+2+3+...+100 is $s&quot;</code></pre></li><li><p>使用数组</p><pre><code class="bash">#!/bin/bashfor x in morning noon afternoon evening  do      echo &quot;This time is $x&quot;  done</code></pre></li><li><p>使用函数</p><pre><code class="bash">#!/bin/bashfunction func1(){  echo AAA}func1echo this is the end of the loopecho Now this is the end of the script</code></pre></li><li><p>判断</p><pre><code class="bash">#!/bin/bashif [ -d /etc/mysql ]  then      echo &quot;the path is right!!&quot;  else      echo &quot;the path is not right&quot;fi</code></pre></li><li><p>判断硬盘是否已经满了</p><pre><code class="bash">#!/bin/bash# Author: LeeBinrate=$( df | grep &quot;sda&quot; | awk &#39;{print $5}&#39;| cut -d &quot;%&quot; -f 1 )if [ $rate -ge 80 ]  then      echo &quot;Warning! /dev/sda1 is full!!&quot;  else      echo &quot;/dev/sda1 is not full!!&quot;fi</code></pre></li><li><p>until循环</p><pre><code class="bash">#!/bin/bash# Author:LeeBini=1s=0until [ $i -gt 100 ]  do      s=$(( $s+$i ))      i=$(( $i+1 ))  doneecho &quot;the sum is $s&quot;</code></pre></li><li><p>while循环</p><pre><code class="bash">#!/bin/bashfunction func1(){    echo this is an example of a function}count=1while [ $count -le 5 ]do  func1  count=$[ $count+1 ]doneecho end of loopfunc1echo end of script</code></pre></li><li><p>while循环求和</p><pre><code class="bash">#!/bin/bash# Author:LeeBini=1s=0#while [ $i -le 100 ]  do      s=$(( $s+$i ))      i=$(( $i+1 ))  doneecho &quot;the sum is $s&quot;</code></pre></li><li><p>备份脚本</p><pre><code class="bash">#!/bin/sh# auto mail for system info# time/bin/date +%F &gt;&gt; ~/app/shell/sysinfoecho &gt;&gt; ~/app/shell/sysinfo# disk infoecho &quot;disk info:&quot; &gt;&gt; ~/app/shell/sysinfo/bin/df -h &gt;&gt; ~/app/shell/sysinfoecho &gt;&gt; ~/app/shell/sysinfoecho &quot;online users&quot; &gt;&gt; ~/app/shell/sysinfo/usr/bin/who | /bin/grep -v root &gt;&gt; ~/app/shell/sysinfoecho &gt;&gt; ~/app/shell/sysinfoecho &quot;memory info:&quot; &gt;&gt; ~/app/shell/sysinfo/usr/bin/free -m &gt;&gt; ~/app/shell/sysinfoecho &gt;&gt; ~/app/shell/sysinfo</code></pre></li><li><p>case语句</p><pre><code class="bash">#!/bin/bash# author: leebinread -p &quot;Please choose yes/no: &quot; -t 30 cho#case $cho in  &quot;yes&quot;)      echo &quot;Your choose is yes!!&quot;      ;;  &quot;no&quot;)      echo &quot;Your choose is no!!&quot;      ;;  *)      echo &quot;Your choose is error!!&quot;      ;;esac#</code></pre></li><li><p>批量解压缩</p><pre><code class="bash">#!/bin/bash# author: leebincd /lampls *.tar.gz &gt; ls.log#for i in $( cat ls.log )  do      tar -zxvf $i &amp;&gt; /dev/null  done#rm -rf /lamp/ls.log</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> shell </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何将hexo的git pages项目部署vps？</title>
      <link href="/2018/03/29/%E5%A6%82%E4%BD%95%E5%B0%86hexo%E7%9A%84git%20pages%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2vps%EF%BC%9F/"/>
      <url>/2018/03/29/%E5%A6%82%E4%BD%95%E5%B0%86hexo%E7%9A%84git%20pages%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2vps%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文介绍如何在vps上搭建自己的blog。<br><a id="more"></a></p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul><li>digitalocean上ubuntu的vps一台</li><li>window10+nodejs+hexo软件环境，参考<a href="https://leebin.top/2018/03/27/%E4%BD%BF%E7%94%A8github%20pages%E5%92%8Chexo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">hexo搭建博客</a></li></ul><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ul><li>方案一 vps上使用本地模式搭建hexo博客，使用Nginx将域名指向 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a></li><li>方案二 在客户端写blog，git推送到服务端，服务端用Nginx解析网页文件</li></ul><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><ul><li>下文分为众多的详细步骤</li></ul><h3 id="安装git和nginx"><a href="#安装git和nginx" class="headerlink" title="安装git和nginx"></a>安装git和nginx</h3><ul><li>安装软件git和nginx<pre><code>apt-get updateapt-get install gitapt-get install nginx</code></pre></li></ul><h3 id="配置git用户和仓库"><a href="#配置git用户和仓库" class="headerlink" title="配置git用户和仓库"></a>配置git用户和仓库</h3><ul><li>git用户权限设定（可以不需要）<pre><code class="bash">chmod 740 /etc/sudoersvim /etc/sudoers#在root ALL=(ALL:ALL) ALL下面新增一行git ALL=(ALL:ALL) ALLchmod 440 /etc/sudoers</code></pre></li><li>配置git用户和仓库, 参考<a href="https://leebin.top/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ubuntu%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89git%E6%9C%8D%E5%8A%A1%E7%AB%AF-%E9%99%84ssh%E6%93%8D%E4%BD%9C%EF%BC%9F/" target="_blank" rel="noopener">在vps上构建私有git服务器</a></li></ul><h3 id="配置git-hooks"><a href="#配置git-hooks" class="headerlink" title="配置git hooks"></a>配置git hooks</h3><ul><li>在hexo.git/hooks/目录下修改post-update.sample为post-update，并覆盖加入<pre><code class="bash">#!/bin/bashGIT_REPO=/home/git/hexo.gitTMP_GIT_CLONE=/tmp/hexoPUBLIC_WWW=/var/www/hexorm -rf ${TMP_GIT_CLONE}git clone $GIT_REPO $TMP_GIT_CLONErm -rf ${PUBLIC_WWW}/*cp -rf ${TMP_GIT_CLONE}/* ${PUBLIC_WWW}</code></pre></li><li>保证post-update有执行权限<pre><code class="bash">chmod +x post-receive</code></pre></li></ul><h3 id="nginx配置"><a href="#nginx配置" class="headerlink" title="nginx配置"></a>nginx配置</h3><ul><li>新建站点文件夹<pre><code class="bash">mkdir -p /var/www/blogchmod -R 775 /var/www/blogchown -R git /var/www/blogchgrp -R git /var/www/blog</code></pre></li><li>配nginx的站点文件2处<pre><code class="bash">#配置1vim /etc/nginx/conf.d/hexo.confserver {  listen  80 ;  listen [::]:80;  root /var/www/blog;  server_name clearsky.me www.clearsky.me; #server_ip  access_log  /var/log/nginx/hexo_access.log;  error_log   /var/log/nginx/hexo_error.log;  error_page 404 =  /404.html;  location ~* ^.+\.(ico|gif|jpg|jpeg|png)$ {      root /var/www/blog;      access_log   off;      expires      1d;  }  location ~* ^.+\.(css|js|txt|xml|swf|wav)$ {      root /var/www/blog;      access_log   off;      expires      10m;  }  location / {      root /var/www/blog;      if (-f $request_filename) {          rewrite ^/(.*)$  /$1 break;      }  }  location /nginx_status {      stub_status on;      access_log off;  }}#配置2vim /etc/nginx/sites-available/defaultroot /var/www/html;</code></pre></li><li>重启nginx服务器<pre><code>service nginx restart#或者/etc/init.d/nginx stop/etc/init.d/nginx start</code></pre><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2></li><li>修改本地的blog源文件，配置推送git服务器，推送到vps服务器上</li><li>参考<a href="https://leebin.top/2018/03/29/git%20pages%E7%9A%84%E8%BF%81%E7%A7%BB%E5%92%8C%E5%A4%9A%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">git pages多服务器部署</a></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> git </tag>
            
            <tag> vps </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>git pages的迁移和多服务器部署</title>
      <link href="/2018/03/29/git%20pages%E7%9A%84%E8%BF%81%E7%A7%BB%E5%92%8C%E5%A4%9A%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/03/29/git%20pages%E7%9A%84%E8%BF%81%E7%A7%BB%E5%92%8C%E5%A4%9A%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2/</url>
      <content type="html"><![CDATA[<p>本文介绍如何将github上的项目迁移到gitee上，如何实现源文件多服务器的部署。<br><a id="more"></a></p><h2 id="github-pages迁移到gitee服务器上"><a href="#github-pages迁移到gitee服务器上" class="headerlink" title="github pages迁移到gitee服务器上"></a>github pages迁移到gitee服务器上</h2><ul><li>在gitee上新建一个和gitee用户名一样的git仓库，并且在pages标签开启pages服务</li><li>克隆github pages仓库到本地，安装必要的插件，保证github pages能够与github服务器正常上传部署</li><li>更改github仓库/.git/config文件<pre><code>[core]  repositoryformatversion = 0  filemode = false  bare = false  logallrefupdates = true  symlinks = false  ignorecase = true[remote &quot;origin&quot;]  url = git@github.com:xjdlb/xjdlb.github.io.git  fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;]  remote = origin  merge = refs/heads/master</code></pre></li><li>更改url为gitee仓库的url</li><li>更改github仓库/-config.yml文件<pre><code>deploy:type: gitrepo:      gitee: git@gitee.com:bin_lee/bin_lee.gitbranch: master</code></pre></li><li>更改repo为gitee仓库的url</li><li>然后使用下面的脚本提交、推送、发布到gitee仓库，迁移就成功了<pre><code class="bash">echo &quot;hello&quot;yy=$(date +%y)mm=$(date +%m)dd=$(date +%d)HH=$(date +%H)MM=$(date +%M)SS=$(date +%S)xW=$(date +%U)we=$(date +%a)xD=$(date +%j)git statusgit add .git commit -m &quot;$yy/$mm/$dd-$HH:$MM:$SS 把github服务器上的pages迁移到gitee上&quot;echo &quot;==================================&quot;git push git@gitee.com:bin_lee/bin_lee.git hexogit  log --oneline | headecho &quot;==================================&quot;hexo clean &amp;&amp; hexo g -d</code></pre></li></ul><h2 id="迁移完成，实现多服务器部署"><a href="#迁移完成，实现多服务器部署" class="headerlink" title="迁移完成，实现多服务器部署"></a>迁移完成，实现多服务器部署</h2><ul><li>更改github仓库/.git/config文件，改为主要的服务器地址<pre><code>[core]  repositoryformatversion = 0  filemode = false  bare = false  logallrefupdates = true  symlinks = false  ignorecase = true[remote &quot;origin&quot;]  url = git@github.com:xjdlb/xjdlb.github.io.git  fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;]  remote = origin  merge = refs/heads/master</code></pre></li><li>更改url为gitee仓库的url</li><li>更改github仓库/-config.yml文件<pre><code>deploy:type: gitrepo:      github: git@github.com:xjdlb/xjdlb.github.io.git      gitee: git@gitee.com:bin_lee/bin_lee.gitbranch: master</code></pre></li><li>更改repo为gitee仓库的url</li><li>然后使用下面的脚本提交、推送、发布到gitee仓库和github仓库，多服务器部署就成功了<pre><code class="bash">echo &quot;hello&quot;yy=$(date +%y)mm=$(date +%m)dd=$(date +%d)HH=$(date +%H)MM=$(date +%M)SS=$(date +%S)xW=$(date +%U)we=$(date +%a)xD=$(date +%j)git statusgit add .git commit -m &quot;$yy/$mm/$dd-$HH:$MM:$SS 同时部署到两个服务器上测试&quot;git push git@github.com:xjdlb/xjdlb.github.io.git hexogit  log --oneline | headecho &quot;==================================&quot;git push git@gitee.com:bin_lee/bin_lee.git hexogit  log --oneline | headecho &quot;==================================&quot;hexo clean &amp;&amp; hexo g -d</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何将hexo源文件整合到部署的git中？</title>
      <link href="/2018/03/28/%E5%A6%82%E4%BD%95%E5%B0%86hexo%E6%BA%90%E6%96%87%E4%BB%B6%E6%95%B4%E5%90%88%E5%88%B0%E9%83%A8%E7%BD%B2%E7%9A%84git%E4%B8%AD%EF%BC%9F/"/>
      <url>/2018/03/28/%E5%A6%82%E4%BD%95%E5%B0%86hexo%E6%BA%90%E6%96%87%E4%BB%B6%E6%95%B4%E5%90%88%E5%88%B0%E9%83%A8%E7%BD%B2%E7%9A%84git%E4%B8%AD%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文介绍如何将hexo源文件整合到部署的git中，实现不携带源文件也能写博客，其中，发布和部署实现了自动化脚本操作。</p><a id="more"></a><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><ul><li>已经使用了hexo部署了自己的blog</li><li>源文件没有丢失</li></ul><h2 id="克隆"><a href="#克隆" class="headerlink" title="克隆"></a>克隆</h2><ul><li>在github上克隆部署后的文件到本地客户端</li><li>使用git bash here进入git仓库</li><li>新建hexo分支并转换到hexo分支<pre><code class="bash">git checkout -b hexo</code></pre></li></ul><h2 id="拷贝"><a href="#拷贝" class="headerlink" title="拷贝"></a>拷贝</h2><ul><li>git仓库转换到hexo分支</li><li>将源文件blog文件夹下的所有文件拷贝到上述git仓库中</li></ul><h2 id="创建自动化脚本"><a href="#创建自动化脚本" class="headerlink" title="创建自动化脚本"></a>创建自动化脚本</h2><ul><li>在git仓库的根目录下创建脚本</li><li>脚本1 create_new_page.sh<pre><code class="bash">echo &quot;hello&quot;yy=$(date +%Y)mm=$(date +%m)dd=$(date +%d)HH=$(date +%H)MM=$(date +%M)SS=$(date +%S)filename=&quot;11111&quot;filepostfix=&quot;.md&quot;cd source/_poststouch $filename$filepostfixecho &gt; $filename$filepostfixecho &quot;---&quot; &gt;&gt; $filename$filepostfixecho &quot;title: $filename&quot; &gt;&gt; $filename$filepostfixecho &quot;date: $yy-$mm-$dd $HH:$MM:$SS&quot; &gt;&gt; $filename$filepostfixecho &quot;tags: [列表,2222,3333,4444]&quot; &gt;&gt; $filename$filepostfixecho &quot;categories: 5555&quot; &gt;&gt; $filename$filepostfixecho &quot;toc: true&quot; &gt;&gt; $filename$filepostfixecho &quot;mathjax: true&quot; &gt;&gt; $filename$filepostfixecho &quot;---&quot; &gt;&gt; $filename$filepostfixecho &quot;&quot; &gt;&gt; $filename$filepostfixecho &quot;&lt;!-- more --&gt;&quot; &gt;&gt; $filename$filepostfixcd ../..</code></pre></li><li>文本1 commit.txt<pre><code class="bash">echo &quot;hello&quot;yy=$(date +%y)mm=$(date +%m)dd=$(date +%d)HH=$(date +%H)MM=$(date +%M)SS=$(date +%S)xW=$(date +%U)we=$(date +%a)xD=$(date +%j)git statusgit add .git commit -m &quot;$yy/$mm/$dd-$HH:$MM:$SS 新增了列表标签&quot;git push origin hexogit  log --oneline | headecho &quot;==================================&quot;hexo clean &amp;&amp; hexo g -d</code></pre></li><li>脚本2 upload_and_deploy.sh<pre><code class="bash">ehco &quot;push and deploy...&quot;sh commit.txt</code></pre></li><li>本次修改完成直接在commit.txt中修改commit，然后运行upload_and_deploy.sh，即可上传代码到hexo分支，发布blog到master分支</li></ul><h2 id="换电脑，安装环境，继续写作"><a href="#换电脑，安装环境，继续写作" class="headerlink" title="换电脑，安装环境，继续写作"></a>换电脑，安装环境，继续写作</h2><ul><li>先决条件：电脑+网络+nodejs+hexo</li><li>克隆仓库到本地</li><li>在仓库中建立hexo配置脚本 init_hexo_after_clone.sh<pre><code>git checkout hexonpm install hexonpm installnpm install hexo-deployer-git</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> git </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hexo部署在github和gitee上的坑</title>
      <link href="/2018/03/28/hexo%E9%83%A8%E7%BD%B2%E5%9C%A8github%E5%92%8Cgitee%E4%B8%8A%E7%9A%84%E5%9D%91/"/>
      <url>/2018/03/28/hexo%E9%83%A8%E7%BD%B2%E5%9C%A8github%E5%92%8Cgitee%E4%B8%8A%E7%9A%84%E5%9D%91/</url>
      <content type="html"><![CDATA[<p>本文介绍了hexo博客github和gitee上部署时候遇到的坑。<br><a id="more"></a></p><h2 id="遇到的坑列举如下"><a href="#遇到的坑列举如下" class="headerlink" title="遇到的坑列举如下"></a>遇到的坑列举如下</h2><ul><li>gitee不支持个性化的域名绑定，所以不要试图申请阿里云的域名，将域名指向gitee pages。</li><li>github pages支持个性化域名的绑定，需要在blog/source目录下新建CNAME文件，并写入自己域名。</li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hexo </tag>
            
            <tag> github </tag>
            
            <tag> gitee </tag>
            
            <tag> 部署 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hexo的主题相关的配置</title>
      <link href="/2018/03/27/hexo%E7%9A%84%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/03/27/hexo%E7%9A%84%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>本文将逐渐介绍blog和themes相关的配置方法.<br><a id="more"></a></p><h2 id="导航上的首页、标签、分类、关于等配置"><a href="#导航上的首页、标签、分类、关于等配置" class="headerlink" title="导航上的首页、标签、分类、关于等配置"></a>导航上的首页、标签、分类、关于等配置</h2><ul><li>保留blog下的配置文件中的首页、标签、分类、关于的目录正确</li><li>在theme下配置文件打开menu相关的导航</li><li>博文前面文件为：<pre><code>title: 使用github pages和hexo搭建自己的博客date: 2018-03-27 13:56:08tags: [githubpages,hexo,配置]categories: 配置toc: truemathjax: true</code></pre></li><li>或者<pre><code>title: 如何利用ubuntu云服务器实现私有git服务端-附ssh常见操作？date: 2018-03-27 18:37:32tags:  - git  - 配置categories: 配置toc: truemathjax: true</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> hexo </tag>
            
            <tag> githubpages </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用github pages和hexo搭建自己的博客</title>
      <link href="/2018/03/27/%E4%BD%BF%E7%94%A8github%20pages%E5%92%8Chexo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84blog/"/>
      <url>/2018/03/27/%E4%BD%BF%E7%94%A8github%20pages%E5%92%8Chexo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84blog/</url>
      <content type="html"><![CDATA[<p>本文描述了如何使用github pages和hexo搭建自己的博客。<br><a id="more"></a></p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h2><ul><li><a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js下载地址</a></li><li>下载node.js，并安装</li></ul><h2 id="安装git并配置ssh密钥"><a href="#安装git并配置ssh密钥" class="headerlink" title="安装git并配置ssh密钥"></a>安装git并配置ssh密钥</h2><ul><li>在客户端下载<a href="https://git-scm.com/downloads/" target="_blank" rel="noopener">git下载地址</a></li><li>安装git</li><li>在客户端右键打开git bash here</li><li>设置user.name和user.email<pre><code>git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot;</code></pre></li><li>生成ssh密钥<pre><code>ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;</code></pre></li><li>显示密钥<pre><code>cat ~/.ssh/id_rsa.pub</code></pre></li><li>添加密钥到github服务器中密钥管理<a href="https://github.com/settings/keys" target="_blank" rel="noopener">添加地址</a></li></ul><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><ul><li>安装hexo、安装扩展插件<pre><code># 安装hexonpm install hexo-cli -g# 初始化博客文件夹hexo init blog# 切换到该路径cd blog# 安装hexo的扩展插件npm install# 安装其它插件npm install hexo-server --savenpm install hexo-admin --savenpm install hexo-generator-archive --savenpm install hexo-generator-feed --savenpm install hexo-generator-search --savenpm install hexo-generator-tag --savenpm install hexo-deployer-git --savenpm install hexo-generator-sitemap --save</code></pre></li></ul><h2 id="本地开发blog与本地测试"><a href="#本地开发blog与本地测试" class="headerlink" title="本地开发blog与本地测试"></a>本地开发blog与本地测试</h2><ul><li>添加自己的markdown到 blog/source/posts目录下</li><li>生成静态页面并开启服务器<pre><code># 生成静态页面hexo generate# 开启本地服务器hexo s# 或者hexo s -p 指定的port</code></pre></li><li>打开浏览器，地址栏中输入：<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a></li></ul><h2 id="服务端新建自己的博客仓库"><a href="#服务端新建自己的博客仓库" class="headerlink" title="服务端新建自己的博客仓库"></a>服务端新建自己的博客仓库</h2><ul><li>在 <a href="https://github.com/new" target="_blank" rel="noopener">https://github.com/new</a> 中新建自己的仓库</li><li>其中Repository name要和Owner是一致的</li></ul><h2 id="客户端将hexo博客部署到github上"><a href="#客户端将hexo博客部署到github上" class="headerlink" title="客户端将hexo博客部署到github上"></a>客户端将hexo博客部署到github上</h2><ul><li>修改配置文件blog/config.yml，修改deploy项的内容<pre><code># Deployment 注释## Docs: https://hexo.io/docs/deployment.htmldeploy:# 类型type: git# 仓库repo: git@github.com:xjdlb/xjdlb.github.io.git# 分支branch: master</code></pre></li><li>注意：type: git中的冒号后面由空格</li><li>注意：将xjdlb换成自己的用户名</li></ul><h2 id="客户端将自己的blog部署hexo"><a href="#客户端将自己的blog部署hexo" class="headerlink" title="客户端将自己的blog部署hexo"></a>客户端将自己的blog部署hexo</h2><ul><li>将自己的项目部署到github pages中<pre><code># 清空静态页面hexo clean# 生成静态页面hexo generate# 部署hexo deploy</code></pre></li><li>打开网页，输入 <a href="http://github_username.github.io" target="_blank" rel="noopener">http://github_username.github.io</a> 打开github上托管的博客</li><li>如我的博客地址是：<a href="http://xjdlb.github.io" target="_blank" rel="noopener">http://xjdlb.github.io</a></li></ul><h2 id="hexo命令缩写与组合"><a href="#hexo命令缩写与组合" class="headerlink" title="hexo命令缩写与组合"></a>hexo命令缩写与组合</h2><ul><li>含义<pre><code>hexo g：hexo generatehexo c：hexo cleanhexo s：hexo serverhexo d：hexo deploy</code></pre></li><li>组合<pre><code># 清除、生成、启动hexo clean &amp;&amp; hexo g -s# 清除、生成、部署hexo clean &amp;&amp; hexo g -d</code></pre></li></ul><h2 id="主题相关配置"><a href="#主题相关配置" class="headerlink" title="主题相关配置"></a>主题相关配置</h2><ul><li>在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">hexo themes</a>中下载相关的主题</li><li>下载方法在blog目录中克隆<pre><code>git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre></li><li>在blog/config.yml中配置主题<pre><code>theme: next</code></pre></li></ul><h2 id="新建blog文件"><a href="#新建blog文件" class="headerlink" title="新建blog文件"></a>新建blog文件</h2><ul><li>hexo new “Hexo教程”</li><li>添加标题及其分类信息<pre><code>title: Hello Worlddate: 2016-01-15 20:19:32tags: [SayHi]categories: SayHitoc: truemathjax: true</code></pre></li><li>或者 在blog目录下可以写成脚本<pre><code>yy=$(date +%Y)mm=$(date +%m)dd=$(date +%d)HH=$(date +%H)MM=$(date +%M)SS=$(date +%S)filename=&quot;11111&quot;filepostfix=&quot;.md&quot;cd source/_poststouch $filename$filepostfixecho &gt; $filename$filepostfixecho &quot;---&quot; &gt;&gt; $filename$filepostfixecho &quot;title: $filename&quot; &gt;&gt; $filename$filepostfixecho &quot;date: $yy-$mm-$dd $HH:$MM:$SS&quot; &gt;&gt; $filename$filepostfixecho &quot;tags: [2222,3333,4444]&quot; &gt;&gt; $filename$filepostfixecho &quot;categories: 5555&quot; &gt;&gt; $filename$filepostfixecho &quot;toc: true&quot; &gt;&gt; $filename$filepostfixecho &quot;mathjax: true&quot; &gt;&gt; $filename$filepostfixecho &quot;---&quot; &gt;&gt; $filename$filepostfixcd ../..</code></pre></li></ul><h2 id="将github-pages绑定自己的域名"><a href="#将github-pages绑定自己的域名" class="headerlink" title="将github pages绑定自己的域名"></a>将github pages绑定自己的域名</h2><ul><li>在阿里云控制台找到域名管理</li><li>在阿里云上购买自己的域名<a href="https://help.aliyun.com/product/35473.html" target="_blank" rel="noopener">注册地址</a></li><li>在xjdlb/xjdlb.github.io/settings中Custom domain处添加自己的域名，不要http://和www</li><li>ping <a href="https://xjdlb.github.io/" target="_blank" rel="noopener">https://xjdlb.github.io/</a> 查看github pages的ip</li><li>添加解析</li></ul><table><thead><tr><th>记录类型</th><th>主机记录</th><th>解析线路</th><th>记录值</th><th>TTL值</th></tr></thead><tbody><tr><td>A</td><td>@</td><td>默认</td><td>151.101.41.147</td><td>600</td></tr><tr><td>A</td><td>www</td><td>默认</td><td>151.101.41.147</td><td>600</td></tr></tbody></table><ul><li>使用自己的域名测试</li></ul><h2 id="CNAME问题"><a href="#CNAME问题" class="headerlink" title="CNAME问题"></a>CNAME问题</h2><p>问题：每次hexo deploy之后，<a href="https://www.leebin.top" target="_blank" rel="noopener">https://www.leebin.top</a> 都会出现404错误<br>一般解决：Github pages–&gt;Settings–&gt;Custom domain<br>最优解决：在将CNAME文件放在source目录下，CNAME文件内容为：leebin.top</p><h2 id="环境变更"><a href="#环境变更" class="headerlink" title="环境变更"></a>环境变更</h2><ul><li>换电脑，安装环境<pre><code>git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot;node -vnpm -vgit --versionnpm intsall hexo -g 或 npm install hexo-cli ghexo -vnpm installnpm install hexo-deployer-git --save# 下面是全部组件，源git仓库不需要全部用上npm install hexo-server --savenpm install hexo-admin --savenpm install hexo-generator-archive --savenpm install hexo-generator-feed --savenpm install hexo-generator-search --savenpm install hexo-generator-tag --savenpm install hexo-deployer-git --savenpm install hexo-generator-sitemap --save</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> hexo </tag>
            
            <tag> github </tag>
            
            <tag> pages </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何利用ubuntu实现私有git服务端-附ssh操作？</title>
      <link href="/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ubuntu%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89git%E6%9C%8D%E5%8A%A1%E7%AB%AF-%E9%99%84ssh%E6%93%8D%E4%BD%9C%EF%BC%9F/"/>
      <url>/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ubuntu%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89git%E6%9C%8D%E5%8A%A1%E7%AB%AF-%E9%99%84ssh%E6%93%8D%E4%BD%9C%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文介绍如何利用云服务器实现私有git服务端，包含了git新建仓库、本地与服务器的ssh互连、保留gitlog迁移git的方法、以及创建仓库的自动化脚本。<br><a id="more"></a></p><h2 id="在服务端下载git"><a href="#在服务端下载git" class="headerlink" title="在服务端下载git"></a>在服务端下载git</h2><ul><li>下载安装git<pre><code>apt-get updateapt-get install git -y</code></pre></li></ul><h2 id="配置git用户"><a href="#配置git用户" class="headerlink" title="配置git用户"></a>配置git用户</h2><ul><li>添加git用户<pre><code>useradd gitpasswd git</code></pre></li></ul><h2 id="通过ssh客户端和服务器互连"><a href="#通过ssh客户端和服务器互连" class="headerlink" title="通过ssh客户端和服务器互连"></a>通过ssh客户端和服务器互连</h2><ul><li>客户端生成ssh密钥<pre><code class="bash">git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot;ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;cat ~/.ssh/id_rsa.pub</code></pre></li><li>或者 上述操作可以集成为无交互的脚本在本地直接执行即可<pre><code class="bash">y=$(date +%y)m=$(date +%m)d=$(date +%d)H=$(date +%H)M=$(date +%M)S=$(date +%S)path=$(pwd)cd ~git config --global user.name &quot;bin_lee&quot;git config --global user.email &quot;xjd.binlee@qq.com&quot;#cd ~/.sshtar -zcvf ssh_binlee_backup_$y-$m-$d-$H-$M-$S.tar.gz .sshrm -rfd ~/.ssh# ssh-keygen -t rsa -C &quot;xjd.binlee@qq.com&quot;ssh-keygen -t rsa -P &quot;&quot; -C &quot;xjd.binlee@qq.com&quot; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pubcd $path</code></pre></li><li><p>服务端安装ssh并实现xshell连接</p><pre><code class="bash">#安装sudo apt-get install openssh-server -ysudo ps -e |grep sshsudo service ssh startsudo passwd rootsudo vi /etc/ssh/sshd_configPermitRootLogin prohibit-passwordPermitRootLogin yessudo service ssh restart#在服务器的指定用户目录下mkdir -p /root/.sshtouch authorized_keys</code></pre></li><li><p>将上述生成的密钥文件添加到服务端</p><pre><code class="bash">echo &quot;密钥&quot; &gt;&gt; /root/.ssh/authorized_keys</code></pre></li><li>客户端测试连通性<pre><code class="bash">ssh -T git@gitee.com或者ssh -T git@server_ip</code></pre></li></ul><h2 id="新建git仓库并使用"><a href="#新建git仓库并使用" class="headerlink" title="新建git仓库并使用"></a>新建git仓库并使用</h2><ul><li>新建git仓库<pre><code>mkdir -p /srv/git/repos/xxx.gitcd /srv/git/repos</code></pre></li><li>初始化git仓库<pre><code>git init --bare /srv/git/repos/xxx.git</code></pre></li><li>设置git仓库的访问权限<pre><code class="bash">cd /srv/git/reposchmod -R 775 xxx.gitchown -R git xxx.gitchgrp -R git xxx.git</code></pre></li><li>克隆git仓库并测试<pre><code class="bash">git clone git@server_ip:/srv/git/repos/xxx.git</code></pre><h2 id="大招-将上述操作合并为git脚本"><a href="#大招-将上述操作合并为git脚本" class="headerlink" title="大招 将上述操作合并为git脚本"></a>大招 将上述操作合并为git脚本</h2></li><li>合并如下：<pre><code class="bash">apt-get updateecho &quot;----------------------------------------&quot;echo &quot;&gt;&gt;&gt; update finished...&quot;echo &quot;----------------------------------------&quot;apt-get install git -yecho &quot;----------------------------------------&quot;echo &quot;&gt;&gt;&gt; install finished...&quot;echo &quot;----------------------------------------&quot;#useradd git#passwd git#or#openssl passwd -stdinuseradd -p &quot;8iENHwQTXrdZM&quot; git#change passwdtouch chpass.txtecho &quot;git:hest&quot; &gt;&gt; chpass.txtchpasswd &lt; chpass.txtrm -rf chpass.txtecho &quot;----------------------------------------&quot;echo &quot;&gt;&gt;&gt; useradd and reset passwd finished...&quot;echo &quot;----------------------------------------&quot;key=&quot;ssh-rsa AAA.......&quot;mkdir -p /home/git/.sshtouch /home/git/.ssh/authorized_keys#vim /home/git/.ssh/authorized_keysecho &quot;${key}&quot; &gt;&gt; /home/git/.ssh/authorized_keyscat /home/git/.ssh/authorized_keysecho &quot;----------------------------------------&quot;echo &quot;&gt;&gt;&gt; add authorized_keys finished...&quot;echo &quot;----------------------------------------&quot;respos_path=&quot;/srv/git/respos/&quot;project_name=&quot;test.git&quot;project_path=${respos_path}${project_name}mkdir -p ${project_path}git init --bare ${project_path}chmod -R 775 ${project_path}chown -R git ${project_path}chgrp -R git ${project_path}echo &quot;----------------------------------------&quot;echo &quot;init git respos finished...&quot;my_ip=$(/sbin/ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk &#39;{print $2}&#39;|tr -d &quot;addr:&quot;)echo &quot;git clone git@${my_ip}:${project_path}&quot;echo &quot;----------------------------------------&quot;</code></pre><h2 id="如果出错销毁服务端git"><a href="#如果出错销毁服务端git" class="headerlink" title="如果出错销毁服务端git"></a>如果出错销毁服务端git</h2></li><li>删除用户和仓库<pre><code>userdel -r gitrm -rdf /srv/git/</code></pre><h2 id="如果服务器出现问题，保留gitlog迁移git的方法"><a href="#如果服务器出现问题，保留gitlog迁移git的方法" class="headerlink" title="如果服务器出现问题，保留gitlog迁移git的方法"></a>如果服务器出现问题，保留gitlog迁移git的方法</h2></li><li>使用镜像克隆保留gitlog<pre><code class="bash">#在源服务器上裸克隆git clone --bare git://github.com/username/project.gitcd project.git#镜像上传到新的服务器上git push --mirror git@gitcafe.com/username/newproject.gitcd ..rm -rf project.git#克隆新服务器下的工程到客户端git clone git@gitcafe.com/username/newproject.git#设置新的上传url为新服务器的地址git remote set-url origin remote_git_address  </code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何利用VPS搭建自己的ss服务器？</title>
      <link href="/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8VPS%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84ss%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%9F/"/>
      <url>/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8VPS%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84ss%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%9F/</url>
      <content type="html"><![CDATA[<p>本文描述了如何在ubuntu服务器上快速搭建自己的shadowcoks代理服务器。<br><a id="more"></a></p><h2 id="以详细步骤安装配置启动过程"><a href="#以详细步骤安装配置启动过程" class="headerlink" title="以详细步骤安装配置启动过程"></a>以详细步骤安装配置启动过程</h2><h3 id="1-安装ss"><a href="#1-安装ss" class="headerlink" title="1.安装ss"></a>1.安装ss</h3><pre><code class="bash">apt-get updatesudo apt-get install python-pip -ysudo pip install shadowsocks</code></pre><h3 id="2-配置"><a href="#2-配置" class="headerlink" title="2.配置"></a>2.配置</h3><pre><code class="bash">mkdir /etc/shadowsockstouch /etc/shadowsocks/ss_config.jsonvim /etc/shadowsocks/ss_config.json{  &quot;server&quot;: &quot;165.227.213.57&quot;,  &quot;port_password&quot;: {      &quot;10001&quot;: &quot;112345678a!&quot;,      &quot;10002&quot;: &quot;112345678a!&quot;,      &quot;10003&quot;: &quot;112345678a!&quot;  },  &quot;local_port&quot;: 1080,  &quot;timeout&quot;: 600,  &quot;method&quot;: &quot;aes-256-cfb&quot;}</code></pre><h3 id="3-启动"><a href="#3-启动" class="headerlink" title="3.启动"></a>3.启动</h3><pre><code class="bash">cd ~touch start.shchmod 775 start.shvim start.shssserver -c /etc/shadowsocks/ss_config.json -d startssserver -c /etc/shadowsocks/ss_config_multiple.json -d startnetstat -ntlp | grep pythontouch stop.shchmod 775 stop.shvim stop.shssserver -c /etc/shadowsocks/ss_config.json -d stopssserver -c /etc/shadowsocks/ss_config_multiple.json -d stopnetstat -ntlp | grep python</code></pre><h2 id="用脚本实现一键安装"><a href="#用脚本实现一键安装" class="headerlink" title="用脚本实现一键安装"></a>用脚本实现一键安装</h2><h3 id="1-创建配置启动脚本"><a href="#1-创建配置启动脚本" class="headerlink" title="1.创建配置启动脚本"></a>1.创建配置启动脚本</h3><pre><code class="bash">创建x脚本touch x &amp;&amp; chmod 775 x &amp;&amp; vim x直接复制到x脚本里面cd ~ &amp;&amp; touch ss_cfg.jsonip=&quot;162.243.161.150&quot;echo &quot;{&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;server\&quot;: \&quot;${ip}\&quot;,&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;port_password\&quot;: {&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;10001\&quot;: \&quot;helloworld\&quot;,&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;10002\&quot;: \&quot;helloworld\&quot;,&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;10003\&quot;: \&quot;helloworld\&quot;&quot; &gt;&gt; ss_cfg.jsonecho &quot;},&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;local_port\&quot;: 1080,&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;timeout\&quot;: 600,&quot; &gt;&gt; ss_cfg.jsonecho &quot;\&quot;method\&quot;: \&quot;aes-256-cfb\&quot;&quot; &gt;&gt; ss_cfg.jsonecho &quot;}&quot; &gt;&gt; ss_cfg.jsoncd ~touch sta.sh &amp;&amp; chmod 775 sta.shecho &quot;ssserver -c ~/ss_cfg.json -d start&quot; &gt;&gt; ~/sta.shecho &quot;netstat -ntlp | grep python&quot; &gt;&gt; ~/sta.shtouch sto.sh &amp;&amp; chmod 775 sto.shecho &quot;ssserver -c ~/ss_cfg.json -d stop&quot; &gt;&gt; ~/sto.shecho &quot;netstat -ntlp | grep python&quot; &gt;&gt; ~/sto.shecho &quot;-------------report-------------------&quot;echo &quot;the fie list as follows:&quot;lsecho &quot;-------------start ss-----------------&quot;./sta.shecho &quot;-------------your ss config-----------&quot;echo &quot;ip=${ip}&quot;echo &quot;port=10001, password=helloworld&quot;echo &quot;port=10002, password=helloworld&quot;echo &quot;port=10003, password=helloworld&quot;echo &quot;local_port=1080&quot;echo &quot;timeout=600&quot;echo &quot;method=aes-256-cfb&quot;echo &quot;-------------end---------------------&quot;</code></pre><h3 id="2-启动服务"><a href="#2-启动服务" class="headerlink" title="2.启动服务"></a>2.启动服务</h3><pre><code class="bash">运行x脚本./x启动服务./sta.sh关闭服务./sto.sh删除文件rm -rf x ss_cfg.json sta.sh sto.sh &amp;&amp; touch x &amp;&amp; chmod 775 x &amp;&amp; vim x</code></pre>]]></content>
      
      <categories>
          
          <category> 配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> 配置 </tag>
            
            <tag> ss </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/03/27/hello-world/"/>
      <url>/2018/03/27/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      <categories>
          
          <category> SayHi </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 列表 </tag>
            
            <tag> SayHi </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
