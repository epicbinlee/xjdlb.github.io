<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HBase基础开发]]></title>
    <url>%2F2018%2F05%2F05%2FHBase%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[本文介绍了如何利用eclipse进行HBase相关的开发。 模拟场景 运营商通话记录查询通话详单本机号码 主叫/被叫 通话时长 时间 对方号码 号码属性 归属地 第1次启动分布式hadoop和hbase 删除所有的临时文件夹(重要) 启动zk集群，三台，最好在xshell下面一起启动，并且查看状态status edits交给JN,手动启动三个JN 格式化一台NN，并启动这一台(只能格式化一次，不然重新来) 另外一台NN执行同步standby 格式化zk 启动dfs(可以直接全部启动) 最后启动RSrm -rdf /opt/hadoop/* /opt/journal/* /opt/zookeeper/v* /opt/zookeeper/z* zkServer.sh start(xshell同时启动) vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode) hadoop-daemon.sh start journalnode(根据hdfs配置文件，n2 n3 n4) hdfs namenode -format(选一个NN格式化，一次机会，失败重头再来) hadoop-daemon.sh start namenode(格式化的NN上) hdfs namenode -bootstrapStandby(n2,没有格式化的NN上) hdfs zkfc -formatZK(n1) start-all.sh(n1) yarn-daemon.sh start resourcemanager(指定的机器，n3 n4) start-hbase.sh 第2次及以后启动分布式hadoop和hbase 启动过程zkServer.sh start(xshell同时启动) vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode) start-all.sh(n1) yarn-daemon.sh start resourcemanager(指定的机器，n3 n4) start-hbase.sh eclipse连接hadoop 连接过程具体参考 新建项目 新建项目 到入hbase安装包下面的lib文件夹到项目目录下 写代码创建表 package com.hikvision.hbase; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HColumnDescriptor; import org.apache.hadoop.hbase.HTableDescriptor; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.HBaseAdmin; import org.apache.log4j.BasicConfigurator; import org.junit.After; import org.junit.Before; import org.junit.Test; public class HBase_test { HBaseAdmin hAdmin; String TABLE_NAME = &quot;phone&quot;; // 列族 一般是1到2个 String COLUMN_FAMILY = &quot;cf1&quot;; @SuppressWarnings(&quot;deprecation&quot;) @Before public void begin() throws Exception { BasicConfigurator.configure(); Configuration conf = new Configuration(); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;n1,n2,n3&quot;); conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); conf.set(&quot;hbase.master&quot;, &quot;n5:16020&quot;); hAdmin = new HBaseAdmin(conf); } @After public void end() { if (hAdmin != null) { try { hAdmin.close(); } catch (IOException e) { e.printStackTrace(); } } } @Test public void createTable() throws Exception { // 0.容错,先判断是不是存在 if (hAdmin.tableExists(TABLE_NAME)) { hAdmin.disableTable(TABLE_NAME); hAdmin.deleteTable(TABLE_NAME); } // 1.表描述 HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(TABLE_NAME)); // 2.列族描述 HColumnDescriptor family = new HColumnDescriptor(COLUMN_FAMILY); // 2.1读缓存 family.setBlockCacheEnabled(true); family.setInMemory(true); // 2.2设定最大版本数默认为1 family.setMaxVersions(1); // 为表指定列族 desc.addFamily(family); hAdmin.createTable(desc); } } 查看hbase shell root@n5:~# hbase shell SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/root/app/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/root/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands. Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0&gt; list TABLE phone t3 tab-1 3 row(s) in 0.2560 seconds =&gt; [&quot;phone&quot;, &quot;t3&quot;, &quot;tab-1&quot;] hbase(main):002:0&gt; describe &quot;phone&quot; Table phone is ENABLED phone COLUMN FAMILIES DESCRIPTION {NAME =&gt; &#39;cf1&#39;, BLOOMFILTER =&gt; &#39;ROW&#39;, VERSIONS =&gt; &#39;1&#39;, IN_MEMORY =&gt; &#39;true&#39;, KEEP_DELETED_CELLS =&gt; &#39;FALSE&#39;, DATA_BLOCK_ENCODING =&gt; &#39;NONE&#39;, TTL =&gt; &#39;FOREVER&#39;, COMPRESSION =&gt; &#39;NONE&#39;, MIN_VERSIONS =&gt; &#39;0&#39;, BLOCKCACH E =&gt; &#39;true&#39;, BLOCKSIZE =&gt; &#39;65536&#39;, REPLICATION_SCOPE =&gt; &#39;0&#39;} 1 row(s) in 0.1350 seconds]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>HBase</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase原理搭建高可用与基本使用]]></title>
    <url>%2F2018%2F05%2F03%2FHBase%E5%8E%9F%E7%90%86%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B8%8E%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文介绍如何安装并使用HBase。 伪分布式的HBase环境搭建Pseudo-Distributed Local Install 参考 配置 &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt; &lt;/property&gt; 完全分布式安装 架构 VM NN DN JN ZK ZKFC RS HM HRS n1 y n n y y n y(b) n n2 y y y y y n n y n3 n y y y n y n y n4 n y y n n y n y n5 n y n n n n y(m) n n6 n y n n n n n n hbase-env.sh export JAVA_HOME=/root/app/jdk export HBASE_MANAGES_ZK=false (不启动自带的zookeeper) hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://sxt:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;n1,n2,n3&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; conf/regionservers #根据架构把n5设定为HMaster，下面的为HReginServer n2 n3 n4 conf/backup-masters file(master的HA) vim backup-masters n1 #根据架构将n1设定为HMaster从节点 HDFS Client Configuration(hbase访问hdfs) copy hdfs-site.xml ${HBASE_HOME}/conf root@n1:~/app/hbase/conf# pwd /root/app/hbase/conf cp /root/app/hadoop/etc/hadoop/hdfs-site.xml ./ 启动 #----------------------------------- 在n5 HMaster主节点启动 root@n5:~/app/hbase/conf# jps 1058 NodeManager 962 DataNode 2706 HMaster 3083 Jps #----------------------------------- 在n2,n3,n4查看HRegionServer root@n4:~# jps 1041 DataNode 949 JournalNode 2618 Jps 2396 HRegionServer 1198 NodeManager 1263 ResourceManager webUI查看 主节点 http://n5:16010/master-status 从节点 http://n1:16010/master-status root@n5:~/app/hbase/conf# netstat -ntlp | grep &quot;java&quot; tcp 0 0 0.0.0.0:50010 0.0.0.0:* LISTEN 962/java tcp 0 0 0.0.0.0:50075 0.0.0.0:* LISTEN 962/java tcp 0 0 0.0.0.0:50020 0.0.0.0:* LISTEN 962/java tcp 0 0 127.0.0.1:42933 0.0.0.0:* LISTEN 962/java tcp6 0 0 :::13562 :::* LISTEN 1058/java tcp6 0 0 192.168.44.105:16000 :::* LISTEN 2706/java tcp6 0 0 :::8040 :::* LISTEN 1058/java tcp6 0 0 :::16010 :::* LISTEN 2706/java tcp6 0 0 :::8042 :::* LISTEN 1058/java tcp6 0 0 :::34187 :::* LISTEN 1058/java 常见的命令 增删改查 hbase shell help #----------------------------------- status table_help version whoami ddl（disable drop enable exist is_disableed is_enabled list show_filters） namespace（alter_namespace ...） dml（append put get delete scan） tools（balancer flush:menorystore2storefile） replication snapshots #----------------------------------- create create &#39;person&#39;,&#39;cf1&#39;,&#39;cf2&#39; #----------------------------------- lsit desc &#39;person&#39; TTL =&gt; &#39;forever&#39; #----------------------------------- put put &#39;person&#39;,&#39;r1&#39;,&#39;c1&#39;,&#39;value&#39; put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;,&#39;xiaoming&#39; put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:sex&#39;,&#39;boy&#39; #----------------------------------- scan scan &#39;person&#39; #----------------------------------- get get &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39; #----------------------------------- 插入数据 put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;,&#39;xiaoming2&#39; get &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39; #----------------------------------- #查看命名空间 list_namespace default hbase #----------------------------------- #先禁用再删除 create &#39;tbl&#39;,&#39;cf&#39; list disable disabled &#39;tbl&#39; list drop &#39;tbl&#39; 查看hbase的文件单机模式文件在Linux上伪分布式模式在Linux上分布式模式在hdfs上 D:\HBASE目录结构 └─namespace ├─主节点HRegionMaster └─域节点HRegionServer └─表table ├─域HRegion │ ├─存储Store │ │ ├─内存存储memoryStore │ │ ├─内存存储memoryStore1 │ │ ├─磁盘储存storeFile │ │ └─磁盘储存storeFile1 │ └─存储Store1 │ ├─内存存储memoryStore │ └─内存存储memoryStore1 └─域HRegion1 ├─存储Store │ ├─内存存储memoryStore │ └─内存存储memoryStore1 └─存储Store1 ├─内存存储memoryStore └─内存存储memoryStore1 webUI查看 n1:60010 HBase 的ha验证 创建表 #-------------------------------------------- create &#39;tab-1&#39; ,&#39;cf-1&#39; put &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:name&#39;,&#39;xiaoli&#39; #-------------------------------------------- hbase(main):007:0&gt; scan &#39;tab-1&#39; ROW COLUMN+CELL rk-0001 column=cf-1:name, timestamp=1525506703143, value=xiaoli 1 row(s) in 0.2510 seconds #-------------------------------------------- hbase(main):008:0&gt; get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:name&#39; COLUMN CELL cf-1:name timestamp=1525506703143, value=xiaoli 1 row(s) in 0.0480 seconds #-------------------------------------------- put &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:age&#39;,&#39;18&#39; #-------------------------------------------- hbase(main):010:0&gt; scan &#39;tab-1&#39; ROW COLUMN+CELL rk-0001 column=cf-1:age, timestamp=1525507159178, value=18 rk-0001 column=cf-1:name, timestamp=1525506703143, value=xiaoli 1 row(s) in 0.0320 seconds #-------------------------------------------- get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1&#39; #-------------------------------------------- hbase(main):011:0&gt; get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1&#39; COLUMN CELL cf-1:age timestamp=1525507159178, value=18 cf-1:name timestamp=1525506703143, value=xiaoli 2 row(s) in 0.0200 seconds 验证 #-------------------------------------------- root@n5:~# jps 1058 NodeManager 962 DataNode 5444 Jps 3625 HMaster #-------------------------------------------- kill -9 3625 查看n5:16010 查看n1:16010 查看hbase shell scan &#39;tab-1&#39; #-------------------------------------------- 重新启动n5上的master #--------------------------------------------]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>HBase</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MR实例温度]]></title>
    <url>%2F2018%2F05%2F01%2FMR%E5%AE%9E%E4%BE%8B%E6%B8%A9%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[本文介绍如何分析具体的MR实例。 数据与需求 数据 1949-10-01 14:21:02 34c 1949-10-02 14:01:02 36c 1950-01-01 11:21:02 32c 1950-10-01 12:21:02 37c 1951-12-01 12:21:02 23c 1950-10-02 12:21:02 41c 1950-10-03 12:21:02 27c 1951-07-01 12:21:02 45c 1951-07-02 12:21:02 46c 1951-07-03 12:21:03 47c 需求 输出：得出每个年月下，温度最高的前两天 年月：升序 温度：降序 技术点分析 sort需要进行二次排序，需要从写sort方法 reduce个数设定为3个，可能有%3操作，进行负载均衡 全部的MR模型中包含了map,reduce,part,sort,group,combine 写代码 代码涉及到了MapReduce的整个生命周期 每个周期的代码如下 TQJob package com.hikvision.tq; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class TQJob { public static void main(String[] args) throws Exception { // 默认加载根目录src目录下的配置文件 Configuration conf = new Configuration(); // 运行方式1：手动指定提交服务器的地址 // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;); // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;); // 运行方式2：使用配置文件指定，屏蔽上面的手动指定,报错的需要指定跨平台，指定本地jar的位置 conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;); conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\tq.jar&quot;); // 运行方式3：手动上传到服务器上，屏蔽上的面的conf指定 // System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;); // 开始job相关的设定 Job job = Job.getInstance(conf); // 设定MR主类 job.setJarByClass(TQJob.class); // 设定mapper job.setMapperClass(TQMapper.class); job.setOutputKeyClass(Weather.class); job.setMapOutputValueClass(IntWritable.class); // 设定reducer job.setReducerClass(TQReducer.class); job.setPartitionerClass(TQPartition.class); job.setSortComparatorClass(TQSort.class); job.setGroupingComparatorClass(TQGroup.class); job.setNumReduceTasks(3); // 要分析的文件 FileInputFormat.addInputPath(job, new Path(&quot;/weather/input/tq.txt&quot;)); // 输出路径 Path outPath = new Path(&quot;/weather/output&quot;); FileSystem fs = FileSystem.get(conf); if (fs.exists(outPath)) { fs.delete(outPath, true); } FileOutputFormat.setOutputPath(job, outPath); // 提交job作业 boolean flag = job.waitForCompletion(true); if (flag) { System.out.println(&quot;job commit successfully...&quot;); } } } TQMapper package com.hikvision.tq; import java.io.IOException; import java.text.ParseException; import java.text.SimpleDateFormat; import java.util.Calendar; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.util.StringUtils; public class TQMapper extends Mapper&lt;LongWritable, Text, Weather, IntWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] strs = StringUtils.split(value.toString(), &#39;\t&#39;); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); Calendar cal = Calendar.getInstance(); try { cal.setTime(sdf.parse(strs[0])); Weather weather = new Weather(); weather.setYear(cal.get(Calendar.YEAR)); weather.setMonth(cal.get(Calendar.MONTH) + 1); weather.setDay(cal.get(Calendar.DAY_OF_MONTH)); int temperature = Integer.parseInt(strs[1].substring(0, strs[1].lastIndexOf(&#39;c&#39;))); weather.setTemperature(temperature); context.write(weather, new IntWritable(temperature)); } catch (ParseException e) { e.printStackTrace(); } } } TQPartition package com.hikvision.tq; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner; public class TQPartition extends HashPartitioner&lt;Weather, IntWritable&gt; { @Override public int getPartition(Weather key, IntWritable value, int numReduceTasks) { // 重写partition的过程，需要满足业务的规则 // 越简单越好，会影响计算速度 return (key.getYear() - 1949) % numReduceTasks; // return super.getPartition(key, value, numReduceTasks); } } TQSort package com.hikvision.tq; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; public class TQSort extends WritableComparator { // 重写构造方法 public TQSort() { super(Weather.class, true); } @SuppressWarnings(&quot;rawtypes&quot;) @Override public int compare(WritableComparable a, WritableComparable b) { Weather w1 = (Weather) a; Weather w2 = (Weather) b; int c1 = Integer.compare(w1.getYear(), w2.getYear()); if (c1 == 0) { int c2 = Integer.compare(w1.getMonth(), w2.getMonth()); if (c2 == 0) { return -Integer.compare(w1.getTemperature(), w2.getTemperature()); } return c2; } return c1; } } TQGroup package com.hikvision.tq; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; public class TQGroup extends WritableComparator { public TQGroup() { super(Weather.class, true); } @SuppressWarnings(&quot;rawtypes&quot;) @Override public int compare(WritableComparable a, WritableComparable b) { Weather w1 = (Weather) a; Weather w2 = (Weather) b; int c1 = Integer.compare(w1.getYear(), w2.getYear()); if (c1 == 0) { return Integer.compare(w1.getMonth(), w2.getMonth()); } return c1; } } TQReducer package com.hikvision.tq; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class TQReducer extends Reducer&lt;Weather, IntWritable, Text, NullWritable&gt; { @Override protected void reduce(Weather weather, Iterable&lt;IntWritable&gt; iterable, Context context) throws IOException, InterruptedException { int flag = 0; for (IntWritable i : iterable) { flag++; if (flag &gt; 2) { break; } String msg = weather.getYear() + &quot;-&quot; + weather.getMonth() + &quot;-&quot; + weather.getDay() + &quot;-&quot; + i.get(); context.write(new Text(msg), NullWritable.get()); } } } 提交job操作 新建文件夹，上传文件到hdfs 已经有了配置文件所以，使用方式2进行提交，需要手动打包，直接运行 // 运行方式2：使用配置文件指定，屏蔽上面的手动指定,报错的需要指定跨平台，指定本地jar的位置 conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;); conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\tq.jar&quot;); 查看运行结果 控制台日志 2018-05-02 15:22:31,473 WARN mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this. 2018-05-02 15:22:34,937 INFO input.FileInputFormat (FileInputFormat.java:listStatus(283)) - Total input paths to process : 1 2018-05-02 15:22:35,103 INFO mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:1 2018-05-02 15:22:35,119 INFO Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - mapred.jar is deprecated. Instead, use mapreduce.job.jar 2018-05-02 15:22:35,237 INFO mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1525168992297_0007 2018-05-02 15:22:35,674 INFO impl.YarnClientImpl (YarnClientImpl.java:submitApplication(273)) - Submitted application application_1525168992297_0007 2018-05-02 15:22:35,825 INFO mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://n3:8088/proxy/application_1525168992297_0007/ 2018-05-02 15:22:35,826 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1525168992297_0007 2018-05-02 15:22:48,438 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1525168992297_0007 running in uber mode : false 2018-05-02 15:22:48,439 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1367)) - map 0% reduce 0% 2018-05-02 15:22:58,537 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1367)) - map 100% reduce 0% 2018-05-02 15:23:14,820 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1367)) - map 100% reduce 33% 2018-05-02 15:23:15,837 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1367)) - map 100% reduce 100% 2018-05-02 15:23:16,853 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_1525168992297_0007 completed successfully 2018-05-02 15:23:17,092 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 50 File System Counters FILE: Number of bytes read=238 FILE: Number of bytes written=501967 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=347 HDFS: Number of bytes written=101 HDFS: Number of read operations=12 HDFS: Number of large read operations=0 HDFS: Number of write operations=6 Job Counters Killed reduce tasks=1 Launched map tasks=1 Launched reduce tasks=3 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=7783 Total time spent by all reduces in occupied slots (ms)=43298 Total time spent by all map tasks (ms)=7783 Total time spent by all reduce tasks (ms)=43298 Total vcore-milliseconds taken by all map tasks=7783 Total vcore-milliseconds taken by all reduce tasks=43298 Total megabyte-milliseconds taken by all map tasks=7969792 Total megabyte-milliseconds taken by all reduce tasks=44337152 Map-Reduce Framework Map input records=10 Map output records=10 Map output bytes=200 Map output materialized bytes=238 Input split bytes=96 Combine input records=0 Combine output records=0 Reduce input groups=5 Reduce shuffle bytes=238 Reduce input records=10 Reduce output records=8 Spilled Records=20 Shuffled Maps =3 Failed Shuffles=0 Merged Map outputs=3 GC time elapsed (ms)=498 CPU time spent (ms)=8400 Physical memory (bytes) snapshot=570830848 Virtual memory (bytes) snapshot=7779991552 Total committed heap usage (bytes)=175120384 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=251 File Output Format Counters Bytes Written=101 job commit successfully... webUI和输出的目录 输出目录下分为三个文件,和设定的三个reducer有关 1949-10-2-36 1949-10-1-34 # 1950-1-1-32 1950-10-2-41 1950-10-1-37 # 1951-7-3-47 1951-7-2-46 1951-12-1-23 查看webUI http://n3:8088/cluster 显示已经执行成功]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工欲善其事必先利其器(持续更新中)]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%B7%A5%E6%AC%B2%E5%96%84%E5%85%B6%E4%BA%8B%E5%BF%85%E5%85%88%E5%88%A9%E5%85%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[本文介绍所有的优化配置过程，设计到操作系统，开发工具，常用的小工具等。 win10 系统优化 安装win10: 搜索关键词：msdn我告诉你，下载原生的win10系统下载魔方，用魔方组件将ISO文件写入到U盘重启，开机即可安装win10用户名可以设置为root，很多开发场景中需要windows的用户名为root 安装驱动： 360官网 &gt; 驱动大师安装驱动大师 &gt; 安装驱动 安装office 2016 搜索关键词：msdn我告诉你，下载office2016安装 激活win10 office2016 小马激活OEM10.exe激活win10KMSpico_setup.exe激活office 2016 关闭/打开windows BISO启动入口 控制面板\硬件和声音\电源选项\系统设置更改不可用设置 &gt; 启用快速启动 关闭掉windows update 在中文输入法下 &gt; 按win键 &gt; 输入服务windows update &gt; 停止 &gt; 禁止启动 关闭掉windows defender 设置 &gt; 更新和安全 &gt; Windows Defender &gt; 关闭实时保护 关闭windows 防火墙 计算机右键属性 &gt; windows防火墙 &gt; 启用和关闭windows防火墙 备份系统 http://www.uqidong.com/ 下载U启动工具，制作U盘启动查找电脑主板 &gt; 查找进入bios的方法 &gt; 重启按键 &gt; 从U盘启动选择备份系统 &gt; 备份C盘 windows 常用的工具优化 everything+wox 实现mac的sportlight搜索框功能 autohotkey实现workflow功能 ahk常用的快捷键 ahk win ^ Ctrl键 + Shift键 ! Alt键 井 Win键 Up 上箭头键 Down 下箭头键 Left 左箭头键 Right 右箭头键 PgUp PageUp键 PgDn PageDn键 F1-F12 功能键 a-z a-z键 LButton 鼠标左键 RButton 鼠标右键 MButton 鼠标中键 WheelUp 鼠标滑轮向上 WheelDown 鼠标滑轮向下 Del Del删除 Enter Enter回车 Tab Table制表符 Space Space空格 eclipse优化 eclipse快捷键 参考 key meaning ctrl+o 打开类 alt+shift+s v 重写方法 alt+shift+s source菜单 alt+shift+l 补全变量名 ctrl+2+l 补全变量名 依赖包的导入方式 手动创建lib，然后导入添加用户依赖 设置代码显示最大宽度 Window &gt; Preferences &gt; Java &gt; Code Style &gt; Formatter &gt; new ProfileLine Wrapping &gt; Maximum line width eclipse插件 maven插件M2Eclipse 找到maven链接 http://www.eclipse.org/m2e/软件 &gt; 安装其他软件 hadoop插件 连接远程hadoop集群，需要本地的windows用户为root用户 ubuntu设置 启动用户名和密码的登录界面 sudo passwd -u root sudo passwd root su root cd /usr/share/lightdm/lightdm.conf.d/ vim 50-unity-greeter.conf # 添加 user-session=ubuntu greeter-show-manual-login=true all-guest=false # 重启 reboot # 使用user和passwd进入root报错 vim /root/.profile # 找到mesg n || true # 改为tty -s &amp;&amp; mesg n || true 更换国内的源(fq除外) sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup vi /etc/apt/sources.list #Ubuntu 官方源 deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse #网易163 deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse sudo apt-get update sudo apt-get dist-upgrade desktop桌面版本vm下的ubuntu更改为固定的IP 查看vm的IP地址 &gt; 网段和DNS编辑ubutnu的网卡配置 &gt; 设置在合理的IP范围重启网卡 server服务器版本vm下的ubuntu更改为固定的IP vmware &gt; 编辑 &gt; 虚拟网络编辑器 &gt; 点击vmnet8 &gt; NET设置 &gt; 查看由vmnet8设定的子网先关参数 &gt; 也可以自己更改子网的IP和网段 配置固定IP三个步骤 首先ifconfig查看自己ubuntu工作网卡名称，替换下面的eth0 #---------------------------------------- /etc/network/interfaces 原来为： auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp #---------------------------------------- 改为： auto lo iface lo inet loopback auto eth0 iface eth0 inet static #定义为静态IP address 192.168.2.29 #所要设置的IP地址 netmask 255.255.255.0 #子网掩码 gateway 192.168.2.1 #网关（路由地址） #---------------------------------------- 手动设动网关和DNS /etc/resolv.conf nameserver 192.168.2.1 #网关（同上） #nameserver 202.106.0.20 #DNS服务器地址（参考其他电脑，VM上的ubuntu可以不用设定） #---------------------------------------- 永久性更改网关和DNS /etc/resolvconf/resolv.conf.d/base nameserver 192.168.2.1 #网关 #nameserver 202.106.0.20 #DNS #---------------------------------------- /etc/init.d/networking restart reboot]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop开发代码三种提交方式]]></title>
    <url>%2F2018%2F05%2F01%2Fhadoop%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本文介绍如何在本地编写wordcount程序，在远程的hadoop集群上运行测试代码，重点介绍了hadoop本地开发后的三种提交到服务器的方式，分别涉及到了项目开发，项目测试，项目交付时期的不同提交方式。 新建项目 新建java项目 添加依赖 maven：下载设置maven，新建maven项目，在maven repository找到坐标手动添加依赖：用户依赖，直接把hadoop/share中lib包逐个引用过来手动添加依赖：lib依赖，将所有的jar放在一个目录下 关联源代码：ctrl+单击打开api，管理源码，打开解压后的源码文件夹 JDK基本的源码在JDK安装路径中src.ziphadoop源码在官网下载解压后关联文件夹到eclipse 代码分为三个部分：主程序设置job任务，mapper，reducer mainpackage com.hikvision.wc; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class WCJob { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { // 默认加载根目录src目录下的配置文件 Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;); conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;); Job job = Job.getInstance(conf); // 设定MR主类 job.setJarByClass(WCJob.class); // 设定mapper job.setMapperClass(WCMapper.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设定reducer job.setReducerClass(WCReducer.class); // 要分析的文件 FileInputFormat.addInputPath(job, new Path(&quot;/wc/input/data.txt&quot;)); // 输出路径 Path outPath = new Path(&quot;/wc/output&quot;); FileSystem fs = FileSystem.get(conf); if (fs.exists(outPath)) { fs.delete(outPath, true); } FileOutputFormat.setOutputPath(job, outPath); // 提交job作业 boolean flag = job.waitForCompletion(true); if (flag) { System.out.println(&quot;job commit successfully...&quot;); } } } mapperpackage com.hikvision.wc; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.util.StringUtils; public class WCMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String str = value.toString(); String[] strs = StringUtils.split(str, &#39; &#39;); for (String s : strs) { context.write(new Text(s), new IntWritable(1)); } } } reducerpackage com.hikvision.wc; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { @Override protected void reduce(Text text, Iterable&lt;IntWritable&gt; iterable, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable i : iterable) { sum += i.get(); } context.write(text, new IntWritable(sum)); } } 运行项目 测试数据放在src下 三种运行方式 本地测试环境（企业开发过程中测试）提交到服务器（测试代码的执行能力）自动打包手动打包 提交1：不打包本地提交到远程服务器直接执行（功能开发过程） 没有配置文件+必要的conf设置 本地eclipse提交代码 解压hadoop，配置环境变量安装windows中的hadoop依赖工具win-utils的%HADOOP_HOME%\bin拷贝hadoop源码到自己的工程中，检查包是不是报错配置本地hosts文件启动zkServer.sh start启动start-all.sh(dfs yarn)启动resource manager(RS)测试50070，找到active的dfs节点测试8088，找到active的resourcemanager节点配置本地的hadoop连接创建输入文件夹，上传数据文件配置conf提交地址在main中填写输入输出路径直接以java application运行代码 实验结果 2018-05-01 18:11:01,551 INFO Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - session.id is deprecated. Instead, use dfs.metrics.session-id 2018-05-01 18:11:01,559 INFO jvm.JvmMetrics (JvmMetrics.java:init(76)) - Initializing JVM Metrics with processName=JobTracker, sessionId= 2018-05-01 18:11:02,328 WARN mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this. 2018-05-01 18:11:02,401 WARN mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(171)) - No job jar file set. User classes may not be found. See Job or Job#setJar(String). 2018-05-01 18:11:02,466 INFO input.FileInputFormat (FileInputFormat.java:listStatus(283)) - Total input paths to process : 1 2018-05-01 18:11:02,601 INFO mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:1 2018-05-01 18:11:02,776 INFO mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_local2042689909_0001 2018-05-01 18:11:03,239 INFO mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://localhost:8080/ 2018-05-01 18:11:03,242 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_local2042689909_0001 2018-05-01 18:11:03,251 INFO mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(471)) - OutputCommitter set in config null 2018-05-01 18:11:03,260 INFO output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 1 2018-05-01 18:11:03,265 INFO mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(489)) - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 2018-05-01 18:11:03,438 INFO mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for map tasks 2018-05-01 18:11:03,445 INFO mapred.LocalJobRunner (LocalJobRunner.java:run(224)) - Starting task: attempt_local2042689909_0001_m_000000_0 2018-05-01 18:11:03,493 INFO output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 1 2018-05-01 18:11:03,502 INFO util.ProcfsBasedProcessTree (ProcfsBasedProcessTree.java:isAvailable(192)) - ProcfsBasedProcessTree currently is supported only on Linux. 2018-05-01 18:11:03,567 INFO mapred.Task (Task.java:initialize(614)) - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5b5d72a5 2018-05-01 18:11:03,574 INFO mapred.MapTask (MapTask.java:runNewMapper(756)) - Processing split: hdfs://n1:8020/wc/input/data.txt:0+44 2018-05-01 18:11:03,666 INFO mapred.MapTask (MapTask.java:setEquator(1205)) - (EQUATOR) 0 kvi 26214396(104857584) 2018-05-01 18:11:03,666 INFO mapred.MapTask (MapTask.java:init(998)) - mapreduce.task.io.sort.mb: 100 2018-05-01 18:11:03,666 INFO mapred.MapTask (MapTask.java:init(999)) - soft limit at 83886080 2018-05-01 18:11:03,666 INFO mapred.MapTask (MapTask.java:init(1000)) - bufstart = 0; bufvoid = 104857600 2018-05-01 18:11:03,666 INFO mapred.MapTask (MapTask.java:init(1001)) - kvstart = 26214396; length = 6553600 2018-05-01 18:11:03,671 INFO mapred.MapTask (MapTask.java:createSortingCollector(403)) - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer 2018-05-01 18:11:04,185 INFO mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 2018-05-01 18:11:04,189 INFO mapred.MapTask (MapTask.java:flush(1460)) - Starting flush of map output 2018-05-01 18:11:04,190 INFO mapred.MapTask (MapTask.java:flush(1482)) - Spilling map output 2018-05-01 18:11:04,190 INFO mapred.MapTask (MapTask.java:flush(1483)) - bufstart = 0; bufend = 70; bufvoid = 104857600 2018-05-01 18:11:04,191 INFO mapred.MapTask (MapTask.java:flush(1485)) - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600 2018-05-01 18:11:04,212 INFO mapred.MapTask (MapTask.java:sortAndSpill(1667)) - Finished spill 0 2018-05-01 18:11:04,217 INFO mapred.Task (Task.java:done(1046)) - Task:attempt_local2042689909_0001_m_000000_0 is done. And is in the process of committing 2018-05-01 18:11:04,234 INFO mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - map 2018-05-01 18:11:04,234 INFO mapred.Task (Task.java:sendDone(1184)) - Task &#39;attempt_local2042689909_0001_m_000000_0&#39; done. 2018-05-01 18:11:04,242 INFO mapred.Task (Task.java:done(1080)) - Final Counters for attempt_local2042689909_0001_m_000000_0: Counters: 22 File System Counters FILE: Number of bytes read=150 FILE: Number of bytes written=335288 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=44 HDFS: Number of bytes written=0 HDFS: Number of read operations=6 HDFS: Number of large read operations=0 HDFS: Number of write operations=1 Map-Reduce Framework Map input records=8 Map output records=8 Map output bytes=70 Map output materialized bytes=92 Input split bytes=97 Combine input records=0 Spilled Records=8 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=0 Total committed heap usage (bytes)=233832448 File Input Format Counters Bytes Read=44 2018-05-01 18:11:04,243 INFO mapred.LocalJobRunner (LocalJobRunner.java:run(249)) - Finishing task: attempt_local2042689909_0001_m_000000_0 2018-05-01 18:11:04,243 INFO mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete. 2018-05-01 18:11:04,246 INFO mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for reduce tasks 2018-05-01 18:11:04,249 INFO mapred.LocalJobRunner (LocalJobRunner.java:run(302)) - Starting task: attempt_local2042689909_0001_r_000000_0 2018-05-01 18:11:04,249 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_local2042689909_0001 running in uber mode : false 2018-05-01 18:11:04,250 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1367)) - map 100% reduce 0% 2018-05-01 18:11:04,259 INFO output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 1 2018-05-01 18:11:04,260 INFO util.ProcfsBasedProcessTree (ProcfsBasedProcessTree.java:isAvailable(192)) - ProcfsBasedProcessTree currently is supported only on Linux. 2018-05-01 18:11:04,317 INFO mapred.Task (Task.java:initialize(614)) - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@368baef3 2018-05-01 18:11:04,328 INFO mapred.ReduceTask (ReduceTask.java:run(362)) - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@42ca90ce 2018-05-01 18:11:04,347 INFO reduce.MergeManagerImpl (MergeManagerImpl.java:&lt;init&gt;(205)) - MergerManager: memoryLimit=1987680640, maxSingleShuffleLimit=496920160, mergeThreshold=1311869312, ioSortFactor=10, memToMemMergeOutputsThreshold=10 2018-05-01 18:11:04,349 INFO reduce.EventFetcher (EventFetcher.java:run(61)) - attempt_local2042689909_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events 2018-05-01 18:11:04,428 INFO reduce.LocalFetcher (LocalFetcher.java:copyMapOutput(144)) - localfetcher#1 about to shuffle output of map attempt_local2042689909_0001_m_000000_0 decomp: 88 len: 92 to MEMORY 2018-05-01 18:11:04,446 INFO reduce.InMemoryMapOutput (InMemoryMapOutput.java:shuffle(100)) - Read 88 bytes from map-output for attempt_local2042689909_0001_m_000000_0 2018-05-01 18:11:04,455 INFO reduce.MergeManagerImpl (MergeManagerImpl.java:closeInMemoryFile(319)) - closeInMemoryFile -&gt; map-output of size: 88, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;88 2018-05-01 18:11:04,461 INFO reduce.EventFetcher (EventFetcher.java:run(76)) - EventFetcher is interrupted.. Returning 2018-05-01 18:11:04,462 INFO mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied. 2018-05-01 18:11:04,462 INFO reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(691)) - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs 2018-05-01 18:11:04,482 INFO mapred.Merger (Merger.java:merge(606)) - Merging 1 sorted segments 2018-05-01 18:11:04,482 INFO mapred.Merger (Merger.java:merge(705)) - Down to the last merge-pass, with 1 segments left of total size: 82 bytes 2018-05-01 18:11:04,483 INFO reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(758)) - Merged 1 segments, 88 bytes to disk to satisfy reduce memory limit 2018-05-01 18:11:04,484 INFO reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(788)) - Merging 1 files, 92 bytes from disk 2018-05-01 18:11:04,484 INFO reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(803)) - Merging 0 segments, 0 bytes from memory into reduce 2018-05-01 18:11:04,485 INFO mapred.Merger (Merger.java:merge(606)) - Merging 1 sorted segments 2018-05-01 18:11:04,486 INFO mapred.Merger (Merger.java:merge(705)) - Down to the last merge-pass, with 1 segments left of total size: 82 bytes 2018-05-01 18:11:04,486 INFO mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied. 2018-05-01 18:11:04,546 INFO Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords 2018-05-01 18:11:04,924 INFO mapred.Task (Task.java:done(1046)) - Task:attempt_local2042689909_0001_r_000000_0 is done. And is in the process of committing 2018-05-01 18:11:04,935 INFO mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied. 2018-05-01 18:11:04,935 INFO mapred.Task (Task.java:commit(1225)) - Task attempt_local2042689909_0001_r_000000_0 is allowed to commit now 2018-05-01 18:11:05,044 INFO output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task &#39;attempt_local2042689909_0001_r_000000_0&#39; to hdfs://n1:8020/wc/output/_temporary/0/task_local2042689909_0001_r_000000 2018-05-01 18:11:05,051 INFO mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - reduce &gt; reduce 2018-05-01 18:11:05,052 INFO mapred.Task (Task.java:sendDone(1184)) - Task &#39;attempt_local2042689909_0001_r_000000_0&#39; done. 2018-05-01 18:11:05,060 INFO mapred.Task (Task.java:done(1080)) - Final Counters for attempt_local2042689909_0001_r_000000_0: Counters: 29 File System Counters FILE: Number of bytes read=366 FILE: Number of bytes written=335380 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=44 HDFS: Number of bytes written=32 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=3 Map-Reduce Framework Combine input records=0 Combine output records=0 Reduce input groups=5 Reduce shuffle bytes=92 Reduce input records=8 Reduce output records=5 Spilled Records=8 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=28 Total committed heap usage (bytes)=267386880 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Output Format Counters Bytes Written=32 2018-05-01 18:11:05,060 INFO mapred.LocalJobRunner (LocalJobRunner.java:run(325)) - Finishing task: attempt_local2042689909_0001_r_000000_0 2018-05-01 18:11:05,061 INFO mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - reduce task executor complete. 2018-05-01 18:11:05,253 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1367)) - map 100% reduce 100% 2018-05-01 18:11:06,254 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_local2042689909_0001 completed successfully 2018-05-01 18:11:06,267 INFO mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 35 File System Counters FILE: Number of bytes read=516 FILE: Number of bytes written=670668 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=88 HDFS: Number of bytes written=32 HDFS: Number of read operations=15 HDFS: Number of large read operations=0 HDFS: Number of write operations=4 Map-Reduce Framework Map input records=8 Map output records=8 Map output bytes=70 Map output materialized bytes=92 Input split bytes=97 Combine input records=0 Combine output records=0 Reduce input groups=5 Reduce shuffle bytes=92 Reduce input records=8 Reduce output records=5 Spilled Records=16 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=28 Total committed heap usage (bytes)=501219328 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=44 File Output Format Counters Bytes Written=32 job commit successfully... 查看结果 aaa 2 apple 3 ooo 1 ttt 1 yyy 1 提交2：打包到本地，直接运行（项目性能测试） 配置文件+必要的conf设置 使用配置文件 拷贝四个配置文件到源代码下面 core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml 在主代码中指定打包的位置，并注释掉代码配置 // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;); // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;); conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;); 手动打jar：项目右键 &gt; export &gt; &gt; java jar file &gt; 指定路径选择main calss运行下java程序刷新 http://n3:8088/cluster 是不是有新的任务运行任务时，常见的报错如下解决链接1:conf设置夸平台提交解决链接2:修改源码 问题： org.apache.hadoop.util.Shell$ExitCodeException: /bin/bash: line 0: fg: no job control 解决1： 这是由于跨平台造成的，经过Google搜索口在StackOverflow上发现答案 conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;); 解决2： 需要手动修改hadoop源码，目前还没有弄明白 处理后，现在的main如下 // 默认加载根目录src目录下的配置文件 Configuration conf = new Configuration(); // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;); // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;); conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;); conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;); 以java application的方式运行程序在控制台和active节点 http://n3:8088/cluster 查看运行的日志，WebUI会显示提交的任务在eclipse-hadoop文件系统查看是不是有数据输出 提交3：打包到本地，手动上传（项目上线时候） 配置文件+删除不必要的conf设置 屏蔽不必要的cond指定 // 默认加载根目录src目录下的配置文件 Configuration conf = new Configuration(); // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;); // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;); // conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;); // conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;); 手动打包，指定main class 通过命令运行程序 hadoop jar jar路径 程序的入口(主程序的入口) hadoop jar wc-1.jar com.hikvision.wc.WCJob 查看输出和webUI信息]]></content>
      <categories>
        <category>程序</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PE hadoop高可用集群搭建总结]]></title>
    <url>%2F2018%2F04%2F30%2FPE%20hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文简单记录生产环境中的hadoop高可用集群搭建总结。 hdfs ha理论总结 NN内存受到限制：federation NN单点故障：edits和fsimage原理，在没有格式化的NN上standby操作 edits记录日志文件，保障一致性，交给第三方JNs来管理，奇数个，不用NFS不可靠 zookeeper，奇数个，它的ZKFC心跳监控NN；管理和切换zkfc yarn ha搭建过程总结 删除所有的临时文件夹(重要) 启动zk集群，三台，最好在xshell下面一起启动，并且查看状态status edits交给JN,手动启动三个JN 格式化一台NN，并启动这一台(只能格式化一次，不然重新来) 另外一台NN执行同步standby 格式化zk 启动dfs(可以直接全部启动) 最后启动RSrm -rdf /opt/hadoop/* /opt/journal/* /opt/zookeeper/v* /opt/zookeeper/z* zkServer.sh start(xshell同时启动) vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode) hadoop-daemon.sh start journalnode(根据hdfs配置文件，n2 n3 n4) hdfs namenode -format(选一个NN格式化，一次机会，失败重头再来) hadoop-daemon.sh start namenode(格式化的NN上) hdfs namenode -bootstrapStandby(n2,没有格式化的NN上) hdfs zkfc -formatZK(n1) start-all.sh(n1) yarn-daemon.sh start resourcemanager(指定的机器，n3 n4) mr yarn理论 四个阶段：split，map,shuffle,reduce, 最小的MR程序包含了前面两个 split块 map task按行读&lt;K,V&gt;排序 分区partition，memory buffer，sort &amp; combine（手动实现，不能代替reduce，可以提升效率） reduce&lt;K,V&gt;迭代器取数据 整合mr到hadoop，实现RS ha 先停止hdfs 配置mr，配置yarn高可用 resource manager需要单独启动]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地eclipse链接远程hadoop编写hdfs测试代码]]></title>
    <url>%2F2018%2F04%2F30%2F%E6%9C%AC%E5%9C%B0eclipse%E9%93%BE%E6%8E%A5%E8%BF%9C%E7%A8%8Bhadoop%E7%BC%96%E5%86%99hdfs%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用eclipse插件连接远程的hadoop，并且编写hdfs测试代码。 开发环境 local: win10 + eclipse neon + hadoop-eclipse-2.7.3.jar remote: hadoop 2.7.6 + hdfs namenode ha 架构 + yarn resource manager ha 架构 本地eclipse连接远程的hadoop集群 后面所有开发的前提是windows用户名必须为root，win+x+G &gt; 本地用户和组 &gt; 用户 &gt; 修改系统的用户名为root 上面的步骤，我没有成功，后来重新安装系统设置为root用户 安装hadoop-eclipse插件，copy jar到插件目录，启动，选择windows &gt; show view &gt; other &gt; hadoop/mr 需要在windows本地安装配置和服务器相同版本的hadoop，并且下载关键字为&lt;hadoo2.7.3的hadoop.dll和winutils.exe&gt;的组件放在本地hadoop/bin下 在MR locations窗口下配置连接远程hadoop服务器，其中dfs master的端口为 50070 webUI能够访问的且显示为active的端口 比如：我的n2:50070 webUI上面显示8020端口 active, 所以hadoop配置为8020 暂时没有用到MR，所以默认就好 完成上述步骤即可连接远程服务器成功 可以创建文件进行测试：eclipse窗口创建，webUI显示，或者终端上使用hdfs shell查看文件列表 创建项目手动导入依赖包 创建项目，直接常见一个Java项目；不着急创建hadoop/mapreduce项目 手动添加依赖；不着急使用maven构建项目 build path &gt; libraries窗口 &gt; add library &gt; user library &gt; user libraries &gt; new name随意hadoop2.7.6 &gt; and external jars 导入hadoop安装路径下面的C:\app2\hadoop-2.7.6\share\hadoop下面全部组件的lib下面的所有jars 最后可以看到窗口有了外部依赖hadoop2.7.6 最后还需要以上面的方式导入junit测试包 编写连接hdfs的测试代码 代码文件为/hdfs-test/src/com/hik/hdfs/HdfsDemo.java，下面的代码包含了连接, 创建文件夹，上传下载文件，合并小文件，下载小文件等操作，简单实现网盘的功能 package com.hik.hdfs; import java.io.File; import java.io.FileNotFoundException; import java.io.IOException; import org.apache.commons.io.FileUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileStatus; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.SequenceFile; import org.apache.hadoop.io.SequenceFile.Writer; import org.apache.hadoop.io.Text; import org.junit.After; import org.junit.Before; import org.junit.Test; public class HdfsDemo { FileSystem fs; Configuration configuration; @Before public void begin() throws Exception { // load configuration file from src configuration = new Configuration(); // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n2:8020&quot;); fs = FileSystem.get(configuration); } @After public void end() throws Exception { fs.close(); } @Test public void mkdir() throws Exception { Path path = new Path(&quot;/tmp_innerConfig&quot;); boolean newdir = fs.mkdirs(path); System.out.println(newdir); } @Test public void upload() throws IOException { Path path = new Path(&quot;/tmp/a2.txt&quot;); FSDataOutputStream outputStream = fs.create(path); FileUtils.copyFile(new File(&quot;D:\\test.txt&quot;), outputStream); } @Test public void list() throws FileNotFoundException, IOException { Path path = new Path(&quot;/tmp/&quot;); FileStatus[] listStatus = fs.listStatus(path); for (FileStatus x : listStatus) { String str = x.getPath() + &quot;--&quot; + x.getLen() + &quot;--&quot; + x.getAccessTime(); System.out.println(str); } } @Test public void uploadSmalltoBig() throws Exception { Path path = new Path(&quot;/seq.txt&quot;); @SuppressWarnings(&quot;deprecation&quot;) Writer writer = SequenceFile.createWriter(fs, configuration, path, Text.class, Text.class); File file = new File(&quot;D:\\file&quot;); for (File f : file.listFiles()) { Text name = new Text(f.getName()); Text content = new Text(FileUtils.readFileToString(f, &quot;UTF-8&quot;)); writer.append(name, content); } } @Test public void downloadBig() throws Exception { Path path = new Path(&quot;/seq.txt&quot;); @SuppressWarnings({ &quot;resource&quot;, &quot;deprecation&quot; }) SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, configuration); Text key = new Text(); Text value = new Text(); while (reader.next(key, value)) { System.out.println(key); System.out.println(value); } } } 运行测试程序 配置本地windows的hosts文件，直接用everything搜索找到c盘下面的hosts,C:\Windows\System32\drivers\etc\hosts, 加入ip和主机名的映射关系192.168.44.100 n1 192.168.44.101 n2 192.168.44.102 n3 192.168.44.103 n4 运行代码前，需要导入连接先关的配置文件，将hadoop/etc/hadoop下的hdfs-site.xml和core-site.xml拷贝到src文件夹下，直接在eclipse里面粘贴 上面的步骤也可以在程序的configuration中设置对应的rpc接口地址 直接在函数名上右键run as &gt; junit test 即可 如果成功绿色状态，查看hdfs文件系统是不是生成相应的文件]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PE mr configuration and yarn resource manager ha]]></title>
    <url>%2F2018%2F04%2F30%2FPE%20mr%20configuration%20and%20yarn%20resource%20manager%20ha%2F</url>
    <content type="text"><![CDATA[本文介绍如何将MR计算模型集成到hdfs ha中，并且实现yarn的RS的高可用。 集群配置需求 集群配置地点 NN DN JN ZK ZKFC RS n1 1 1 1 n2 1 1 1 1 1 n3 1 1 1 1 n4 1 1 1 MR配置，yarn RS ha配置 mapred-site.xml vim mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml &lt;configuration&gt; &lt;!-- 配置MR --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- resourcemanager高可用ha --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;sxt2yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;n3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;n4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;n3:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;n4:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zk集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 同步到其他机器上 scp ./*.xml n2:`pwd` scp ./*.xml n3:`pwd` scp ./*.xml n4:`pwd` MR启动，查看yarn RS ha 停掉所有 zkServer.sh stop stop-dfs.sh kill -9 pid 启动zk root@n1:~# zkServer.sh start ZooKeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED root@n1:~# zkServer.sh status ZooKeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Mode: follower 启动所有 root@n1:~# start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [n1 n2] n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.out n2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.out n4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.out n2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.out n3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.out Starting journal nodes [n2 n3 n4] n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.out n3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.out n4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.out Starting ZK Failover Controllers on NN hosts [n1 n2] n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.out n2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.out starting yarn daemons starting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n1.out n3: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n3.out n2: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n2.out n4: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n4.out 查看进程 root@n1:~# jps 6162 Jps 5363 QuorumPeerMain 5923 DFSZKFailoverController 5578 NameNode # root@n2:~# jps 5600 NodeManager 5715 Jps 5044 NameNode 5157 DataNode 5448 DFSZKFailoverController 4920 QuorumPeerMain 5294 JournalNode # root@n3:~# jps 3856 Jps 3746 NodeManager 3477 DataNode 3609 JournalNode 3354 QuorumPeerMain # root@n4:~# jps 3441 DataNode 3575 JournalNode 3817 Jps 3710 NodeManager 查看DN(也是nodemanager的配置，RS yarn需要手动启动) root@n1:~# cat /root/app/hadoop/etc/hadoop/slaves n2 n3 n4 启动resourcemanager/yarn/nodemanager（n3, n4） yarn-daemon.sh start resourcemanager 启动结果slaves配置就是DN和nodemanager的配置 root@n3:~# yarn-daemon.sh start resourcemanager starting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n3.out root@n3:~# jps 3905 ResourceManager 3746 NodeManager 3477 DataNode 3609 JournalNode 3354 QuorumPeerMain 3951 Jps root@n3:~# # root@n4:~# yarn-daemon.sh start resourcemanager starting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n4.out root@n4:~# jps 3441 DataNode 3859 ResourceManager 3910 Jps 3575 JournalNode 3710 NodeManager root@n4:~# # root@n2:~# jps 5600 NodeManager 5764 Jps 5044 NameNode 5157 DataNode 5448 DFSZKFailoverController 4920 QuorumPeerMain 5294 JournalNode root@n2:~# # root@n1:~# jps 5363 QuorumPeerMain 5923 DFSZKFailoverController 5578 NameNode 6333 Jps 查看yarn http端口8088 root@n3:~# netstat -nltp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:33139 0.0.0.0:* LISTEN 3477/java tcp 0 0 127.0.1.1:53 0.0.0.0:* LISTEN 1210/dnsmasq tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1181/sshd tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN 1048/cupsd tcp 0 0 0.0.0.0:50010 0.0.0.0:* LISTEN 3477/java tcp 0 0 0.0.0.0:50075 0.0.0.0:* LISTEN 3477/java tcp 0 0 0.0.0.0:8480 0.0.0.0:* LISTEN 3609/java tcp 0 0 0.0.0.0:50020 0.0.0.0:* LISTEN 3477/java tcp 0 0 0.0.0.0:8485 0.0.0.0:* LISTEN 3609/java tcp6 0 0 192.168.44.102:3888 :::* LISTEN 3354/java tcp6 0 0 :::22 :::* LISTEN 1181/sshd tcp6 0 0 ::1:631 :::* LISTEN 1048/cupsd tcp6 0 0 192.168.44.102:8088 :::* LISTEN 3905/java tcp6 0 0 :::41529 :::* LISTEN 3354/java tcp6 0 0 :::13562 :::* LISTEN 3746/java tcp6 0 0 192.168.44.102:8030 :::* LISTEN 3905/java tcp6 0 0 192.168.44.102:8031 :::* LISTEN 3905/java tcp6 0 0 192.168.44.102:8032 :::* LISTEN 3905/java tcp6 0 0 192.168.44.102:8033 :::* LISTEN 3905/java tcp6 0 0 :::2181 :::* LISTEN 3354/java tcp6 0 0 :::8040 :::* LISTEN 3746/java tcp6 0 0 :::8042 :::* LISTEN 3746/java tcp6 0 0 :::43405 :::* LISTEN 3746/java 查看wenUI http://n1:50070/dfshealth.html#tab-overview http://n2:50070/dfshealth.html#tab-overview 观察下n1和n2谁是active和standby 观察下datanode是不是全n1,n2,n3 观察下文件系统是不是active的NN可用，standby的NN不可用 http://n3:8088/cluster http://n4:8088/cluster(会自动跳转到上面的链接，n4 RS 处于standby状态) n3和n4上启动RS，自动关联到nodemanager，管理DN，查看节点ActiveNodes是不是和DN数目一样3个 测试yarn RS ha 测试ha的状态yarn-daemon.sh stop resourcemanager（n3） jps 挂掉n3 RS，看看n4:8088的状态active 再次启动n3，这时候n3处于standby状态 生产环境的高可用Hadoop集群的搭建总结 高可用解决了1.x的单节点不可靠的问题 高可用ha一方面指的是hdfs的namenode的高可用，解决NN的单节点故障问题 高可用ha另一方面指的是yarn的resource manager的高可用，解决yarn单节点不可靠的问题 2.x为什么引入了yarn呢？yarn可用保障计算的高可靠 实验的一般流程： Hadoop单机模式：直接跑jar文件 伪分布式模式：配置成分布式模式，只有一台vm 全分布式模式：使用1.x架构，具有secondary NN 全分布式hdfs NN ha模式：使用了QJM实现了hdfs的高可用架构 全分布式hdfs NN ha + yarn RS ha模式：使用了集群实现了yarn对DN的管理 对比可以发现1.x和2.x的共同点都是移动计算不移动数据 对比可以发现1.x和2.x的差异性在于1.x由client直接操作DN，但是2.x加入了yarn层管理DN 2.x有两个瓶颈NN和RS，因此需要ha处理 其中4中的QJM，包含了集群： 设计目标NN ha 从下到上：DN–&gt;NN active/NN standby–&gt;JN(edits管理)–&gt;zk中zkfc–&gt;zk 其中5包含了集群： 设计目标是yarn中的resource manager yarn的管理包含了：resource manager（RS）–&gt;node manager DN–&gt;NN active/NN standby–&gt;JN(edits管理)–&gt;zk中zkfc–&gt;zk–&gt;RS]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PE hadoop 2.x hdfs availablity implement]]></title>
    <url>%2F2018%2F04%2F30%2FPE%20hadoop%202.x%20hdfs%20availablity%20implement%2F</url>
    <content type="text"><![CDATA[本文介绍如何在hdfs上实现namenode集群的高可用。 集群配置需求 集群配置地点 NN DN JN ZK ZKFC n1 1 1 1 n2 1 1 1 1 1 n3 1 1 1 n4 1 1 配置HDFS hdfs-site.xml vim /root/app/hadoop/etc/hadoop/hdfs-site.xml &lt;configuration&gt; &lt;!-- 配置NN空间 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;sxt&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.sxt&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.sxt.nn1&lt;/name&gt; &lt;value&gt;n1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.sxt.nn2&lt;/name&gt; &lt;value&gt;n2:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.sxt.nn1&lt;/name&gt; &lt;value&gt;n1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.sxt.nn2&lt;/name&gt; &lt;value&gt;n2:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置JN能处理的Node，可以理解为DN --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://n2:8485;n3:8485;n4:8485/sxt&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.sxt&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置密钥 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置JN的临时文件夹 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/journal/node/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 单节点故障自动迁移 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; core-site.xml vim /root/app/hadoop/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://sxt&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置Quorum Journal Manager的zk集群，JN管理集群 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置zookeeper 环境变量 cat /etc/profile export ANT_HOME=/root/app/ant export HADOOP_HOME=/root/app/hadoop export JAVA_HOME=/root/app/jdk export JRE_HOME=/root/app/jdk/jre export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$ANT_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin export ZOOKEEPER_HOME=/root/app/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin 注意只有前面三台的zk路径加入环境变量 配置文件 cat /root/app/zookeeper/conf/zoo.cfg # The number of milliseconds of each tick # dataDir=/tmp/zookeeper dataDir=/opt/zookeeper server.1=n1:2888:3888 server.2=n2:2888:3888 server.3=n3:2888:3888 分别配置/opt/zookeeper目录，创建myid文件，加入1, 2, 3 同时启动zk zkServer.sh start 查看状态 zkServer.sh status 启动输出 root@n1:~# zkServer.sh start ZooKeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED root@n1:~# zkServer.sh status ZooKeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Mode: follower n1拷贝zookeeper到n2, n3, n4 scp -r ./dir n1:`pwd` 启动ZK 启动并查看状态（1, 2, 3同时启动）zkServer.sh start zkServer.sh status 启动JN 在n2. n3, n4启动JN hadoop-daemon.sh start journalnode 输出 root@n2:~# hadoop-daemon.sh start journalnode starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.out root@n2:~# jps 1751 QuorumPeerMain 1895 JournalNode 1945 Jps 启动NN，高可用HA操作 在一台NN格式化（NN:n1） hdfs namenode -format 在没有格式化的另外一台hadoop执行standby操作（NN:n2） hdfs namenode -bootstrapStandby 报错提示n1没有启动namenode，先启动namenode hadoop-daemon.sh start namenode 在此执行standby成功（格式化+启动n1，standby另外n2） 18/04/29 11:14:41 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT] 18/04/29 11:14:41 INFO namenode.NameNode: createNameNode [-bootstrapStandby] #===================================================== About to bootstrap Standby ID nn2 from: Nameservice ID: sxt Other Namenode ID: nn1 Other NN&#39;s HTTP address: http://n1:50070 Other NN&#39;s IPC address: n1/192.168.44.100:8020 Namespace ID: 1765158274 Block pool ID: BP-1441163464-192.168.44.100-1525025632122 Cluster ID: CID-2e601647-294c-4e70-8e72-7a82bea94fa9 Layout version: -63 isUpgradeFinalized: true #===================================================== 18/04/29 11:14:42 INFO common.Storage: Storage directory /opt/hadoop/dfs/name has been successfully formatted. 18/04/29 11:14:43 INFO namenode.TransferFsImage: Opening connection to http://n1:50070/imagetransfer?getimage=1&amp;txid=0&amp;storageInfo=-63:1765158274:0:CID-2e601647-294c-4e70-8e72-7a82bea94fa9 18/04/29 11:14:43 INFO namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds 18/04/29 11:14:43 INFO namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s 18/04/29 11:14:43 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 321 bytes. 18/04/29 11:14:43 INFO util.ExitUtil: Exiting with status 0 18/04/29 11:14:43 INFO namenode.NameNode: SHUTDOWN_MSG: SHUTDOWN_MSG: Shutting down NameNode at n2/192.168.44.101 查看HA效果 在一个NN上格式化zookeeper（n1） hdfs zkfc -formatZK 在单节点NN启动n1 start-dfs.sh 输出 Starting namenodes on [n1 n2] n1: namenode running as process 2411. Stop it first. n2: namenode running as process 2239. Stop it first. n2: datanode running as process 2413. Stop it first. n4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.out n3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.out Starting journal nodes [n2 n3 n4] n3: journalnode running as process 2207. Stop it first. n2: journalnode running as process 1895. Stop it first. n4: journalnode running as process 1732. Stop it first. Starting ZK Failover Controllers on NN hosts [n1 n2] n2: zkfc running as process 2804. Stop it first. n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.out root@n1:~# 进程查看 root@n1:~# jps 3091 DFSZKFailoverController 3285 Jps 2411 NameNode 2188 QuorumPeerMain # root@n2:~# jps 3184 Jps 2804 DFSZKFailoverController 1751 QuorumPeerMain 1895 JournalNode 2413 DataNode 2239 NameNode # root@n3:~# jps 2512 Jps 2121 QuorumPeerMain 2362 DataNode 2207 JournalNode # root@n4:~# jps 1732 JournalNode 2039 Jps 1887 DataNode # NN DN JN ZK ZKFC n1 1 1 1 n2 1 1 1 1 1 n3 1 1 1 n4 1 1 查看webUI http://n2:50070/dfshealth.html#tab-overview Overview &#39;n2:8020&#39; (active) # http://n1:50070/dfshealth.html#tab-overview Overview &#39;n1:8020&#39; (standby) # Datanode Information 三个 # http://n1:50070/explorer.html#/ Operation category READ is not supported in state standby # http://n2:50070/explorer.html#/ Browse Directory 可见 故障测试 hadoop-daemon.sh stop namenode（n2） 查看网页，文件系统，DN hadoop-daemon.sh start namenode（n2） 再次查看交换了状态 Overview &#39;n1:8020&#39; (active) Overview &#39;n2:8020&#39; (standby) 重新启动与停止 启停操作stop-dfs.sh root@n1:~# stop-dfs.sh Stopping namenodes on [n1 n2] n1: stopping namenode n2: stopping namenode n3: stopping datanode n4: stopping datanode n2: stopping datanode Stopping journal nodes [n2 n3 n4] n2: stopping journalnode n3: stopping journalnode n4: stopping journalnode Stopping ZK Failover Controllers on NN hosts [n1 n2] n1: stopping zkfc n2: stopping zkfc # 下次启动jps查看ZK是不是启动 先启动ZK，在启动DFS # root@n1:~# zkServer.sh start（根据列表中的三台都同时启动zk） root@n1:~# zkServer.sh status ZooKeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Mode: follower # root@n1:~# start-dfs.sh Starting namenodes on [n1 n2] n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.out n2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.out n4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.out n3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.out n2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.out Starting journal nodes [n2 n3 n4] n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.out n3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.out n4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.out Starting ZK Failover Controllers on NN hosts [n1 n2] n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.out n2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.out]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>ha</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase的基本构成与实践]]></title>
    <url>%2F2018%2F04%2F24%2Fhbase%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[本文介绍hbase的基本构成与实践。 hbase的基本构成 表空间 namespace 两个默认的表空间 hbase： 系统默认表空间 default： 不指定自动加入的表空间 root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data Found 2 items drwxr-xr-x - root supergroup 0 2018-04-23 20:18 /hbase/data/default drwxr-xr-x - root supergroup 0 2018-04-23 20:18 /hbase/data/hbase 表 table 表的存在形式：表以文件夹的形式存在于hdfs中 表的基本组成是：RowKey, Column Family, Column, Value(Cell):Byte array 表的物理属性：以RowKey进行字典排序，行的方向存在多个Region，Region是存储和负载均衡的最小单元，不同的Region分布到不同的RegionServer上 #============================= root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase Found 2 items drwxr-xr-x - root supergroup 0 2018-04-23 20:18 /hbase/data/hbase/meta drwxr-xr-x - root supergroup 0 2018-04-23 20:18 /hbase/data/hbase/namespace #----------------------------- root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/meta Found 3 items drwxr-xr-x - root supergroup 0 2018-04-23 20:18 /hbase/data/hbase/meta/.tabledesc drwxr-xr-x - root supergroup 0 2018-04-23 20:18 /hbase/data/hbase/meta/.tmp drwxr-xr-x - root supergroup 0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740 #----------------------------- root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/meta/1588230740 Found 4 items -rw-r--r-- 1 root supergroup 32 2018-04-23 20:18 /hbase/data/hbase/meta/1588230740/.regioninfo drwxr-xr-x - root supergroup 0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/.tmp drwxr-xr-x - root supergroup 0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/info drwxr-xr-x - root supergroup 0 2018-04-24 01:37 /hbase/data/hbase/meta/1588230740/recovered.edits #----------------------------- root@ubuntu:~/app/hbase/bin# ./hbase shell 2018-04-24 06:29:19,776 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands. Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0&gt; #----------------------------- hbase(main):001:0&gt; create &#39;maizi_hbase&#39;,&#39;f&#39; 0 row(s) in 2.5670 seconds =&gt; Hbase::Table - maizi_hbase hbase(main):002:0&gt; #============================= 访问 http://192.168.231.150:50070/explorer.html#/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458 得到hbse的路径 #----------------------------- root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458 Found 3 items -rw-r--r-- 1 root supergroup 46 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/.regioninfo drwxr-xr-x - root supergroup 0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/f drwxr-xr-x - root supergroup 0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/recovered.edits #----------------------------- 上述操作中，6a028f21704f8fc6bf298598f6b8a458为Region的编号 #============================= 访问 http://192.168.231.150:16010/table.jsp?name=maizi_hbase 得到hbase的管理界面，可以看出路径结构 列族 column family 很多列的集合 hbase中的每个列都属于一个column family 每个column family存在于hdfs的单独文件中 列名以column family为前缀， info:name, info:age #----------------------------- /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/f 数据库/数据/表空间/表/region/列族 #----------------------------- 创建表的时候必须定义列族，因为在hdfs上必须要创建文件夹 #----------------------------- 何如设计RowKey是经典问题？ 列 存放数据的地方 RowKey 可以理解为主键，最大长度为64k，RowKey保存为字节数组 是非关系型数据库中key-value类型的数据的key 自动字典排序 散列原则，分布到不同的Region中，RegionServer的负载均衡问题]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>configuration</tag>
        <tag>原理</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[how to understand habse components ?]]></title>
    <url>%2F2018%2F04%2F24%2Fhow%20to%20understand%20habse%20component%20%2F</url>
    <content type="text"><![CDATA[This article describes the basic components of hbase, including HMaster, HRegionServer, Region. principle of hbase features of HMaster (technical director) add, delete, and modify tables Region load balancing HMaster manages the distribution of data features of RegionServer (department manager) RegionServer is the service component of Hbase RegionServer maintains Regions assigned by HMaster RegionServer can divide big Regions features of Region (developers) Region is a partition handling the tasks assigned by RegionServer]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>configuration</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper working mechanism and installation]]></title>
    <url>%2F2018%2F04%2F24%2FzooKeeper%20working%20mechanism%20and%20installation%2F</url>
    <content type="text"><![CDATA[This article describes the basic working principle and installation process of zookeeper. zookeeper working mechanism zookeeper is a high-performance application coordination server that is mainly used to maintain a file system-like namespace. zookeeper itself contains 2n+1 servers, and their roles are divided into leader and follower. zookeeper maintains multiple server services and maintains data consistency to ensure that clients connecting to any server can get consistent data services. zookeeper’s nodes have 4 life cycles.PERSISTENT (persistent node) PERSISTENT_SEQUENTIAL (Sequential automatic numbering of persistent nodes, this node automatically adds 1 based on the number of existing nodes) EPHEMERAL (temporary node, client session timeout such nodes will be automatically deleted) EPHEMERAL_SEQUENTIAL (temporary automatic numbering node) install and configure zookeeper download &gt; extract &gt; configure &gt; start/stop &gt; test configureroot@ubuntu:~/app/zookeeper# ll total 1596 drwxr-xr-x 10 1001 1001 4096 Mar 23 2017 ./ drwxr-xr-x 13 root root 4096 Apr 23 23:50 ../ drwxr-xr-x 2 1001 1001 4096 Mar 23 2017 bin/ -rw-rw-r-- 1 1001 1001 84725 Mar 23 2017 build.xml drwxr-xr-x 2 1001 1001 4096 Mar 23 2017 conf/ drwxr-xr-x 10 1001 1001 4096 Mar 23 2017 contrib/ drwxr-xr-x 2 1001 1001 4096 Mar 23 2017 dist-maven/ drwxr-xr-x 6 1001 1001 4096 Mar 23 2017 docs/ -rw-rw-r-- 1 1001 1001 1709 Mar 23 2017 ivysettings.xml -rw-rw-r-- 1 1001 1001 5691 Mar 23 2017 ivy.xml drwxr-xr-x 4 1001 1001 4096 Mar 23 2017 lib/ -rw-rw-r-- 1 1001 1001 11938 Mar 23 2017 LICENSE.txt -rw-rw-r-- 1 1001 1001 3132 Mar 23 2017 NOTICE.txt -rw-rw-r-- 1 1001 1001 1770 Mar 23 2017 README_packaging.txt -rw-rw-r-- 1 1001 1001 1585 Mar 23 2017 README.txt drwxr-xr-x 5 1001 1001 4096 Mar 23 2017 recipes/ drwxr-xr-x 8 1001 1001 4096 Mar 23 2017 src/ -rw-rw-r-- 1 1001 1001 1456729 Mar 23 2017 zookeeper-3.4.10.jar -rw-rw-r-- 1 1001 1001 819 Mar 23 2017 zookeeper-3.4.10.jar.asc -rw-rw-r-- 1 1001 1001 33 Mar 23 2017 zookeeper-3.4.10.jar.md5 -rw-rw-r-- 1 1001 1001 41 Mar 23 2017 zookeeper-3.4.10.jar.sha1 #============================== -rw-rw-r-- 1 1001 1001 535 Mar 23 2017 configuration.xsl -rw-rw-r-- 1 1001 1001 2161 Mar 23 2017 log4j.properties -rw-rw-r-- 1 1001 1001 922 Mar 23 2017 zoo_sample.cfg root@ubuntu:~/app/zookeeper/conf# cp zoo_sample.cfg zoo.cfg root@ubuntu:~/app/zookeeper/conf# ll total 24 drwxr-xr-x 2 1001 1001 4096 Apr 24 00:17 ./ drwxr-xr-x 10 1001 1001 4096 Mar 23 2017 ../ -rw-rw-r-- 1 1001 1001 535 Mar 23 2017 configuration.xsl -rw-rw-r-- 1 1001 1001 2161 Mar 23 2017 log4j.properties -rw-r--r-- 1 root root 922 Apr 24 00:17 zoo.cfg -rw-rw-r-- 1 1001 1001 922 Mar 23 2017 zoo_sample.cfg #------------------------------ vim zoo.cfg mkdir -p /root/app/zookeeper/zookdata # dataDir=/tmp/zookeeper dataDir=/root/app/zookeeper/zookdata # append the following: server.1=192.168.231.150:2888:3888 #------------------------------ root@ubuntu:~/app/zookeeper/zookdata# pwd /root/app/zookeeper/zookdata root@ubuntu:~/app/zookeeper/zookdata# touch myid &amp;&amp; echo 1 &gt; myid root@ubuntu:~/app/zookeeper/zookdata# cat myid 1 #============================== scp the zookeeper to other server reset myid file in zookeeper on other server #============================== start/stop zookeeper start zookeeper root@ubuntu:~/app/zookeeper/bin# ll total 52 drwxr-xr-x 2 1001 1001 4096 Apr 24 00:35 ./ drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../ -rwxr-xr-x 1 1001 1001 232 Mar 23 2017 README.txt* -rwxr-xr-x 1 1001 1001 1937 Mar 23 2017 zkCleanup.sh* -rwxr-xr-x 1 1001 1001 1056 Mar 23 2017 zkCli.cmd* -rwxr-xr-x 1 1001 1001 1534 Mar 23 2017 zkCli.sh* -rwxr-xr-x 1 1001 1001 1628 Mar 23 2017 zkEnv.cmd* -rwxr-xr-x 1 1001 1001 2696 Mar 23 2017 zkEnv.sh* -rwxr-xr-x 1 1001 1001 1089 Mar 23 2017 zkServer.cmd* -rwxr-xr-x 1 1001 1001 6773 Mar 23 2017 zkServer.sh* -rw-r--r-- 1 root root 5056 Apr 24 00:35 zookeeper.out #------------------------------ Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Usage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd} #------------------------------ root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh start zookeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED #------------------------------ root@ubuntu:~/app/zookeeper/bin# jps 38113 Jps 34545 HQuorumPeer 34757 HRegionServer 12550 NodeManager 12408 ResourceManager 12024 DataNode 34618 HMaster 12235 SecondaryNameNode 11852 NameNode #------------------------------ root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh status zookeeper JMX enabled by default Using config: /root/app/zookeeper/bin/../conf/zoo.cfg Mode: standalone use zookeeper root@ubuntu:~/app/zookeeper/bin# ./zkCli.sh Connecting to localhost:2181 ... 2018-04-24 00:40:32,972 [myid:] - INFO [main:Environment@100] - Client ... 2018-04-24 00:40:32,984 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib 2018-04-24 00:40:32,984 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp ... WatchedEvent state:SyncConnected type:None path:null #------------------------------ [zk: localhost:2181(CONNECTED) 0] help zookeeper -server host:port cmd args stat path [watch] set path data [version] ls path [watch] delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] sync path listquota path rmr path get path [watch] create [-s] [-e] path data acl addauth scheme auth quit getAcl path close connect host:port #------------------------------ [zk: localhost:2181(CONNECTED) 1] create /data_test &#39;data_test&#39; Created /data_test [zk: localhost:2181(CONNECTED) 2] ls / [data_test, zookeeper, hbase] [zk: localhost:2181(CONNECTED) 3] get /data_test data_test cZxid = 0x75 ctime = Tue Apr 24 00:42:11 PDT 2018 mZxid = 0x75 mtime = Tue Apr 24 00:42:11 PDT 2018 pZxid = 0x75 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 9 numChildren = 0 #------------------------------ [zk: localhost:2181(CONNECTED) 4] create /data_test/dir_2 &#39;123_value&#39; Created /data_test/dir_2 [zk: localhost:2181(CONNECTED) 5] get /data_test data_test cZxid = 0x75 ctime = Tue Apr 24 00:42:11 PDT 2018 mZxid = 0x75 mtime = Tue Apr 24 00:42:11 PDT 2018 pZxid = 0x76 cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 9 numChildren = 1 #------------------------------ [zk: localhost:2181(CONNECTED) 6] quit Quitting... 2018-04-24 00:47:19,034 [myid:] - INFO [main:zookeeper@684] - Session: 0x162f5c0c60f0007 closed 2018-04-24 00:47:19,037 [myid:] - INFO [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x162f5c0c60f0007 view the log where is the log?root@ubuntu:~/app/zookeeper/bin# ll total 52 drwxr-xr-x 2 1001 1001 4096 Apr 24 00:35 ./ drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../ -rwxr-xr-x 1 1001 1001 232 Mar 23 2017 README.txt* -rwxr-xr-x 1 1001 1001 1937 Mar 23 2017 zkCleanup.sh* ... -rw-r--r-- 1 root root 5056 Apr 24 00:35 zookeeper.out]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>configuration</tag>
        <tag>hbase</tag>
        <tag>zookdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pseudo-distributed hbase configuration]]></title>
    <url>%2F2018%2F04%2F24%2Fpseudo-distributed%20hbase%20configuration%2F</url>
    <content type="text"><![CDATA[This article describes how to build a pseudo-distributed hbase on a virtual machine. preconditions jdk environmentroot@ubuntu:~# echo $JAVA_HOME /root/app/jdk1.8.0_171 a pseudo-distributed hadoop root@ubuntu:~/app/hadoop/sbin# jps 12550 NodeManager 34152 Jps 12408 ResourceManager 12024 DataNode 12235 SecondaryNameNode 11852 NameNode test http://localhost:50070 http://192.168.231.150:8099 http://192.168.231.150:8042 configure hbase download hbase from apache mirrors extract files from hbase-2.0.0-beta-2-bin.tar.gz configure xml fileshbase-env.sh hbase-site.xml regionservers #================================ root@ubuntu:~/app/hbase/conf# pwd /root/app/hbase/conf #================================ root@ubuntu:~/app/hbase/conf# ll total 48 drwxr-xr-x 2 root root 4096 Apr 23 20:15 ./ drwxr-xr-x 8 root root 4096 Apr 23 20:17 ../ -rw-r--r-- 1 root root 1811 Dec 26 2015 hadoop-metrics2-hbase.properties -rw-r--r-- 1 root root 4537 Jan 28 2016 hbase-env.cmd -rw-r--r-- 1 root root 7537 Apr 23 20:12 hbase-env.sh -rw-r--r-- 1 root root 2257 Dec 26 2015 hbase-policy.xml -rw-r--r-- 1 root root 1355 Apr 23 20:09 hbase-site.xml -rw-r--r-- 1 root root 4603 May 28 2017 log4j.properties -rw-r--r-- 1 root root 16 Apr 23 20:15 regionservers #================================ vim hbase-env.sh # The java implementation to use. Java 1.7+ required. # export JAVA_HOME=/usr/java/jdk1.6.0/ export JAVA_HOME=/root/app/jdk1.8.0_171 #-------------------------------- # Tell HBase whether it should manage it&#39;s own instance of Zookeeper or not. # export HBASE_MANAGES_ZK=true export HBASE_MANAGES_ZK=true #================================ vim regionservers root@ubuntu:~/app/hbase/conf# cat regionservers 192.168.231.150 #================================ vim hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://192.168.231.150:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;192.168.231.150&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; run/stop hbase run hbaseroot@ubuntu:~/app/hbase/bin# ./start-hbase.sh localhost: starting zookeeper, logging to /root/app/hbase/bin/../logs/hbase-root-zookeeper-ubuntu.out starting master, logging to /root/app/hbase/bin/../logs/hbase-root-master-ubuntu.out Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0 192.168.231.150: starting regionserver, logging to /root/app/hbase/bin/../logs/hbase-root-regionserver-ubuntu.out 192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0 root@ubuntu:~/app/hbase/bin# jps 34545 HQuorumPeer 34757 HRegionServer 12550 NodeManager 12408 ResourceManager 12024 DataNode 34618 HMaster 12235 SecondaryNameNode 11852 NameNode 35053 Jps test hbasehttp://192.168.231.150:16010]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>configuration</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[how to download 1080p videos from youtube]]></title>
    <url>%2F2018%2F04%2F23%2Fhow%20to%20download%201080p%20videos%20from%20youtube%2F</url>
    <content type="text"><![CDATA[the article describes how to download 1080p videos form youtube with your foreigen vps/ecs. general method chrome browser copy the link of the video from youtube paste the link to the input box of https://en.savefrom.net/ download videos though chrome advance method login in your foreign vps such as: ubuntu from digitalocean use the following cmd to download the videoapt-get install youtube-dl -y youtube-dl -f 22 your_video_link ultimate method use the following cmd to analysis all videos and audios about your_video_link youtube-dl -F your_video_link use the following cmd to download videos and audios with the specified code youtube-dl -f 10 your_video_link install the tools of video and audio apt-get install ffmpeg -y merge the video and audio ffmpeg -i /tmp/a.wav -i /tmp/a.avi /tmp/out.avi how to download playlist from youtube cmdyoutube-dl -citk –format mp4 –yes-playlist VIDEO_PLAYLIST_LINK youtube-dl -citk –format mp4 –yes-playlist https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfua youtube-dl -cit &quot;https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfua&quot;]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>vps</tag>
        <tag>youtube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[usage of scp]]></title>
    <url>%2F2018%2F04%2F23%2Fusage%20of%20scp%2F</url>
    <content type="text"><![CDATA[this article describes how download/upload files from server using scp command line. usage of scp download file from server scp root@servername:/path/filename /tmp/local_destination scp root@192.168.0.101:/home/kimi/test.txt /home/kimi/test.txt upload file to server scp /path/local_filename root@servername:/path scp /var/www/test.php root@192.168.0.101:/var/www/ download directory to client scp -r root@servername:remote_dir/ /tmp/local_dir scp -r root@192.168.0.101:/home/kimi/test /tmp/local_dir upload directory to server scp -r /tmp/local_dir root@servername:remote_dir scp -P 22 -r test root@192.168.0.101:/var/www/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>linux</tag>
        <tag>cmd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[write wordcount program on eclipse and run on hadoop in win10]]></title>
    <url>%2F2018%2F04%2F23%2Fwrite%20wordcount%20program%20on%20eclipse%20and%20run%20on%20hadoop%20in%20win10%2F</url>
    <content type="text"><![CDATA[This article describes how to write the wordcount program on eclipse and run it on local hadoop. prerequisites server win10 hadoop 2.7.6 client win10 eclipse neon hadoop-eclipse-plugin-2.7.2.jar create project and coding create project new &gt; other &gt; map reduce program &gt; fix the boxes with name, ${HADOOP_HOME} &gt; finish coding package com.hikvision.bigdata.hadoop.hadoop_wordcount; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; import org.apache.log4j.BasicConfigurator; /** * wordcount */ public class WordCount { public static void main(String[] args) throws Exception { BasicConfigurator.configure(); System.out.println(&quot;Hello World!&quot;); Configuration conf = new Configuration(); @SuppressWarnings(&quot;deprecation&quot;) Job job = new Job(conf, &quot;wordcount&quot;); job.setJarByClass(WordCount.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); } public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) { word.set(tokenizer.nextToken()); context.write(word, one); } } } public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } context.write(key, new IntWritable(sum)); } } } configure program configure hadoop step 1: core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ########################################### step 2: hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; ########################################### step 3: mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;localhost:10020&lt;/value&gt; &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; ########################################### step 4: yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;localhost:8099&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; format namenode cd bin hdfs namenode -format # format only once start hadoop and upload data to hdfs cd ${HADOOP_HOME} input cmd to open cmd line cd sbin start-all.cmd # # hadoop fs -mkdir /data hadoop fs -put D:\a.txt /data/a.txt configure input and output path for program in eclipse1 project name &gt; right clieck &gt; run as &gt; run configuration &gt; java application &gt; new 2 fix the boxes with the program name, main class, run name, and arguments 3 the arguments as follows: hdfs://localhost:9000/data/a.txt hdfs://localhost:9000/data/output run program precondition delete the output floder on hdfs data is ready input/output path is configured in eclipse hadoop is running run programproject name &gt; right clieck &gt; run as &gt; run on hadoop &gt; select the main class &gt; enjoy the ouput]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>configuration</tag>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[configure hadoop on ubuntu and connect to eclipse on ubuntu or win10]]></title>
    <url>%2F2018%2F04%2F22%2Fconfigure%20hadoop%20on%20ubuntu%20and%20connect%20to%20eclipse%20on%20ubuntu%20or%20win10%2F</url>
    <content type="text"><![CDATA[This article describes how to configure hadoop2.7.6 on ubuntu and connect to hadoop2.7.6 using eclipse on ubuntu and win10 respectively. prerequisites server win10 vmware workstation pro 12 Ubuntu 16.04.4 LTS hadoop 2.7.6 download ubuntu clinet eclipse neon download hadoop-eclipse-plugin-2.7.2.jar download win10 client eclipse neon download hadoop-eclipse-plugin-2.7.2.jar download pretreatment for ubuntu virtual machine enable the user-passwd input box on the login screensudo passwd root su root cd /usr/share/lightdm/lightdm.conf.d/ vim 50-unity-greeter.conf # add user-session=ubuntu greeter-show-manual-login=true all-guest=false # reboot reboot # login in ubuntu with root and get a error report vim /root/.profile # locate to mesg n || true # change to tty -s &amp;&amp; mesg n || true configure jdk for ubuntu jdk 1.8 downloadexport JAVA_HOME=/root/app/jdk1.8.0_171 export JRE_HOME=/root/app/jdk1.8.0_171/jre export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin source /etc/profile java -version configure hadoop2.7.6 append JAVA_HOME to stat script step 1: vim /root/app/hadoop/etc/hadoop/hadoop-env.sh # export JAVA_HOME=${JAVA_HOME} export JAVA_HOME=/root/app/jdk1.8.0_171 ########################################### step 2: vim /root/app/hadoop/etc/hadoop/yarn-env.sh # some Java parameters # export JAVA_HOME=/home/y/libexec/jdk1.6.0/ export JAVA_HOME=/root/app/jdk1.8.0_171 configure core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml step 1: vim /root/app/hadoop/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ########################################### step 2: vim /root/app/hadoop/etc/hadoop/hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; ########################################### step 3: vim /root/app/hadoop/etc/hadoop/mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;localhost:10020&lt;/value&gt; &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; ########################################### step 4: vim /root/app/hadoop/etc/hadoop/yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;localhost:8099&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; create floder for hadoop work directory cd /root/app/hadoop mkdir hdfs -p hdfs/data hdfs/name mkdir tmp format hdfs and start/stop hadoop format hdfs# keep all hadoop process stopped ./sbin/stop-all.sh # remove tmp directory rm -rdf tmp/ # format hdfs only once bin/hdfs namenode -format # see if the format is successful tree hdfs/ start/stop hadoop# start hadoop ./sbin/start-all.sh root@ubuntu:~/app/hadoop# jps 63809 Jps 63474 NodeManager 62950 DataNode 63335 ResourceManager 62775 NameNode 63163 SecondaryNameNode # stop hadoop ./sbin/stop-all.sh connect to hadoop with eclipse install plugin in eclipse on ubuntu1 copy to hadoop-eclipse-plugin-2.7.2.jar to ${eclipse_home}/dropins; 2 open eclipse; 3 open menu &gt; windows &gt; show view &gt; other &gt; mapreduce tools &gt; map/reduce locations; 4 map/reduce locations &gt; right click &gt; edit hadoop location; location name: XXX map/reduce master: host: localhost port: 50020 dfs master: host: localhost port: 9000 5 open dfs locations, you will find the file in hdfs. install plugin in eclipse on win101 on win10, your eclipse serves as a clinet, you can connect your server with ip, so you should firstly replace *localhost* with your server ip in all etc files, such as core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml; 2 repeat the step 1,2,3,4 above; 3 map/reduce locations &gt; right click &gt; edit hadoop location &gt; advace parameters, replace *hadoop.tmp.dir* with your own address in /hdfs-site.xml; 4 enjoy the local developing and the remote debuging.]]></content>
      <categories>
        <category>configuration</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
        <tag>eclipse</tag>
        <tag>configuration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo构建博客搜索框加载中的解决方案]]></title>
    <url>%2F2018%2F04%2F19%2Fhexo%E6%9E%84%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%90%9C%E7%B4%A2%E6%A1%86%E5%8A%A0%E8%BD%BD%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[本文介绍nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。 问题与现象 nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。 原因 开发的markdown中出现了非utf-8的字符。 访问可以查找错误出现的位置：https://leebin.top/search.xml 解决方案 逐个排查每个markdown文件，直到找到非utf-8字符，删除，重新部署，点击搜索框测试。 访问：https://leebin.top/search.xml 发现可以解析成源文件。]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop中wordcount程序开发]]></title>
    <url>%2F2018%2F04%2F19%2FHadoop%E4%B8%ADwordcount%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[本文介绍如何利用java和hadoop组件开发wordcount程序。 开发与测试环境 windows eclipse maven，常见的组件如下： Apache Hadoop Common 3.1 Apache Hadoop Client Aggregator 3.1 Hadoop Core 1.2 Apache Hadoop HDFS 3.1 Apache Hadoop MapReduce Core 3.1 ubuntu中hadoop单机模式，搭建过程参考: 如何hadoop单机版 添加依赖后maven报错 报错 Buiding Hadoop with Eclipse / Maven - Missing artifact jdk.tools:jdk.tools:jar:1.6 解决 # cmd C:\Users\BinLee&gt;java -version java version &quot;1.8.0_144&quot; Java(TM) SE Runtime Environment (build 1.8.0_144-b01) Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) # 添加下面的依赖到maven的pom.xml &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8.0_144&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; wordcount程序开发 pom.xml &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.jordiburgos&lt;/groupId&gt; &lt;artifactId&gt;wordcount&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;wordcount&lt;/name&gt; &lt;url&gt;http://jordiburgos.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${java.home}/../lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;distro-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;descriptors&gt; &lt;descriptor&gt;assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; wordcount.java package com.jordiburgos; import java.io.IOException; import java.util.*; import org.apache.hadoop.fs.Path; import org.apache.hadoop.conf.*; import org.apache.hadoop.io.*; import org.apache.hadoop.mapreduce.*; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; public class WordCount { public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) { word.set(tokenizer.nextToken()); context.write(word, one); } } } public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } context.write(key, new IntWritable(sum)); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = new Job(conf, &quot;wordcount&quot;); job.setJarByClass(WordCount.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); } } 使用maven打包程序 打包命令 项目右键&gt;run as&gt;maven build 打包后jar的结构 C:. └─wordcount ├─com │ └─jordiburgos └─META-INF └─maven └─com.jordiburgos └─wordcount 在hadoop上运行程序 上传待分析的文本到hdfs # 本地创建input文件夹和a.txt文件 cd /root/app/hadoop-3.1.0 mkdir input vim a.txt # # 创建文件夹 hadoop fs -mkdir hdfs://localhost:9001/tmp # # 上传文件到hdfs hadoop fs -put /root/app/hadoop-3.1.0/input hdfs://127.0.0.1:9001/tmp 运行jar程序 cd /root/app/hadoop-3.1.0 bin/hadoop jar wordcount.jar com.jordiburgos.WordCount hdfs://localhost:9001/tmp/input/ file:///root/app/hadoop-3.1.0/output/ 在linux中查看输出文件 cd /root/app/hadoop-3.1.0/output root@ubuntu:~/app/hadoop-3.1.0/output# ls part-r-00000 _SUCCESS root@ubuntu:~/app/hadoop-3.1.0/output# cat part-r-00000 0000 1 aaaa 1 ddfh 1 ff 1 ggg 1 hj 1 iiiii 1 sss 1]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>maven</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu中hadoop单机模式和伪分布式搭建]]></title>
    <url>%2F2018%2F04%2F18%2Fubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[本文介绍如何搭建hadoop单机版本/独立模式/standalone模式？ ubuntu开启root用户登录的方法 设置密码、添加信息sudo passwd -u root sudo passwd root su root cd /usr/share/lightdm/lightdm.conf.d/ vim 50-unity-greeter.conf # 添加 user-session=ubuntu greeter-show-manual-login=true all-guest=false # 重启 reboot # 使用user和passwd进入root报错 vim /root/.profile # 找到mesg n || true # 改为tty -s &amp;&amp; mesg n || true ubuntu中的java环境变量配置 编辑 sudo vim /etc/profileexport JAVA_HOME=/root/app/jdk1.8.0_171 export JRE_HOME=/root/app/jdk1.8.0_171/jre export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin source /etc/profile 验证java -version 单机版hadoop配置 官方文档 生成ssh密钥 cd ~ ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys ssh localhost 配置java环境 root@ubuntu:~# vim /etc/profile export JAVA_HOME=/root/app/jdk1.8.0_171 export JRE_HOME=/root/app/jdk1.8.0_171/jre export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 配置hadoop环境 vim /etc/profile #HADOOP VARIABLES START export JAVA_HOME=/root/app/jdk1.8.0_171 export HADOOP_HOME=/root/app/hadoop-3.1.0 export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot; #HADOOP VARIABLES END source /etc/profile 单机版测试 root@ubuntu:~# /root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar An example program must be given as the first argument. Valid program names are: aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files. aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files. bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi. dbcount: An example job that count the pageview counts from a database. distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi. grep: A map/reduce program that counts the matches of a regex in the input. join: A job that effects a join over sorted, equally partitioned datasets multifilewc: A job that counts words from several files. pentomino: A map/reduce tile laying program to find solutions to pentomino problems. pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method. randomtextwriter: A map/reduce program that writes 10GB of random textual data per node. randomwriter: A map/reduce program that writes 10GB of random data per node. secondarysort: An example defining a secondary sort to the reduce. sort: A map/reduce program that sorts the data written by the random writer. sudoku: A sudoku solver. teragen: Generate data for the terasort terasort: Run the terasort teravalidate: Checking results of terasort wordcount: A map/reduce program that counts the words in the input files. wordmean: A map/reduce program that counts the average length of the words in the input files. wordmedian: A map/reduce program that counts the median length of the words in the input files. wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files. 实例测试 /root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar grep ./input ./output &#39;dfs[a-z.]+&#39; cat ./output/* # 查看结果 rm -r ./output # 删除结果 # 结果 root@ubuntu:~/app/hadoop-3.1.0# cat ./output/* 1 dfsadmin 伪分布式hadoop配置 格式化hdfs cd /root/app/hadoop-3.1.0 ./bin/hdfs namenode -format 添加变量到 vim /etc/profile export JAVA_HOME=/root/app/jdk1.8.0_171 export JRE_HOME=/root/app/jdk1.8.0_171/jre export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin # # HADOOP VARIABLES START export JAVA_HOME=/root/app/jdk1.8.0_171 export HADOOP_HOME=/root/app/hadoop-3.1.0 # export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin # export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export HADOOP_YARN_HOME=$HADOOP_HOME # export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot; # export HDFS_DATANODE_USER=root export HDFS_NAMENODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root # export YARN_RESOURCEMANAGER_USER=root export HADOOP_SECURE_DN_USER=yarn export YARN_NODEMANAGER_USER=root # HADOOP VARIABLES END 编辑/root/app/hadoop-3.1.0/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 编辑/root/app/hadoop-3.1.0/etc/hadoop/hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50070&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 编辑 hadoop-env.sh vim /root/app/hadoop-3.1.0/etc/hadoop/hadoop-env.sh export JAVA_HOME=/root/app/jdk1.8.0_171 编辑 yarn-env.sh vim /root/app/hadoop-3.1.0/etc/hadoop/yarn-env.sh export JAVA_HOME=/root/app/jdk1.8.0_171 编辑 mapred-env.sh vim /root/app/hadoop-3.1.0/etc/hadoop/mapred-env.sh export JAVA_HOME=/root/app/jdk1.8.0_171 hadoop的使用 启动与停止 cd /root/app/hadoop-3.1.0 ./sbin/start-all.sh ./sbin/stop-all.sh 查看服务 root@ubuntu:~/app/hadoop-3.1.0# jps 23058 NameNode 23491 SecondaryNameNode 23753 ResourceManager 23225 DataNode 24427 Jps 24030 NodeManager Resource Manager http://localhost:8088 Web UI of the NameNode daemon http://localhost:50070 HDFS NameNode web interface http://localhost:8042]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的环境变量配置]]></title>
    <url>%2F2018%2F04%2F17%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本文介绍常见的环境变量配置方法。 windows常见的环境变量配置 CLASSPATH .;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar; JAVA_HOME C:\app3\Java\jdk1.8.0_144 JRE_HOME C:\app3\Java\jre1.8.0_144 MVN_HOMEC:\app3\apache-maven-3.5.3 Path C:\app3\Python35\Scripts\;C:\app3\Python35\;C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%JAVA_HOME%\bin;%JRE_HOME%\bin;C:\app3\Git\cmd;C:\app3\MinGW\bin;C:\app3\nodejs\;C:\app3\MATLAB\R2017b\runtime\win64;C:\app3\MATLAB\R2017b\bin;%MVN_HOME%\bin;]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>windows</tag>
        <tag>环境变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apache maven的配置与使用]]></title>
    <url>%2F2018%2F04%2F17%2Fmaven%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文介绍了apache maven的配置与使用过程，【清理项目】→【编译项目】→【测试项目】→【生成测试报告】→【打包项目】→【部署项目】，maven详细讲解：他山之石 需要先配置java和maven环境变量 CLASSPATH .;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar; JAVA_HOME C:\app3\Java\jdk1.8.0_144 JRE_HOME C:\app3\Java\jre1.8.0_144 MVN_HOME C:\app3\apache-maven-3.5.3 Path C:\app3\Python35\Scripts\;C:\app3\Python35\;C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%JAVA_HOME%\bin;%JRE_HOME%\bin;C:\app3\Git\cmd;C:\app3\MinGW\bin;C:\app3\nodejs\;C:\app3\MATLAB\R2017b\runtime\win64;C:\app3\MATLAB\R2017b\bin;%MVN_HOME%\bin; 更换maven的仓库为自定义的仓库 创建目标位置如，d:\maven\repo 拷贝C:\app3\apache-maven-3.5.3\conf\settings.xml文件到d:\maven 修改两处的settings.xml文件 定位到localRepository&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; # 修改为： &lt;localRepository&gt;d:\maven\repo&lt;/localRepository&gt; maven手动创建项目 他山之石 创建项目 # cmd cd /d d:\test mvn archetype:generate # log Choose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): 1169:（和eclipse上的maven插件有关系，直接回车） Choose org.apache.maven.archetypes:maven-archetype-quickstart version: 1: 1.0-alpha-1 2: 1.0-alpha-2 3: 1.0-alpha-3 4: 1.0-alpha-4 5: 1.0 6: 1.1 7: 1.3 Choose a number: 7:(直接回车) Define value for property &#39;groupId&#39;: com.hikvision.ai_data.data（从大往小填写自己公司的名字） Define value for property &#39;artifactId&#39;: test_mvn（项目的名字） Define value for property &#39;version&#39; 1.0-SNAPSHOT: :（默认就行） Define value for property &#39;package&#39; com.hikvision.ai_data.data: : test_mvn_pkg（将class打包的jar文件的名称） Confirm properties configuration: groupId: com.hikvision.ai_data.data artifactId: test_mvn version: 1.0-SNAPSHOT package: test_mvn_pkg Y: :(直接回车) [INFO] ---------------------------------------------------------------------------- [INFO] Using following parameters for creating project from Archetype: maven-archetype-quickstart:1.3 [INFO] ---------------------------------------------------------------------------- [INFO] Parameter: groupId, Value: com.hikvision.ai_data.data [INFO] Parameter: artifactId, Value: test_mvn [INFO] Parameter: version, Value: 1.0-SNAPSHOT [INFO] Parameter: package, Value: test_mvn_pkg [INFO] Parameter: packageInPathFormat, Value: test_mvn_pkg [INFO] Parameter: package, Value: test_mvn_pkg [INFO] Parameter: version, Value: 1.0-SNAPSHOT [INFO] Parameter: groupId, Value: com.hikvision.ai_data.data [INFO] Parameter: artifactId, Value: test_mvn [INFO] Project created from Archetype in dir: D:\003---WorkSpace\06---testmaven\test_mvn [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 13:57 min [INFO] Finished at: 2018-04-17T14:54:37+08:00 [INFO] ------------------------------------------------------------------------ 创建项目后查看文件 D:\003---WorkSpace\06---testmaven&gt;tree 卷 工厂 的文件夹 PATH 列表 卷序列号为 0000006C BAA7:827C D:. └─test_mvn └─src ├─main │ └─java │ └─test_mvn_pkg └─test └─java └─test_mvn_pkg 编译项目 # cmd cd test_mvn mvn clean compile # log D:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean compile [INFO] Scanning for projects... [INFO] [INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;----------------- [INFO] Building test_mvn 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn --- [INFO] [INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn --- [INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources [INFO] [INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn --- [INFO] Changes detected - recompiling the module! [INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1.570 s [INFO] Finished at: 2018-04-17T14:58:59+08:00 [INFO] ------------------------------------------------------------------------ 单元测试 # cmd mvn clean test # log D:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean test [INFO] Scanning for projects... [INFO] [INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;----------------- [INFO] Building test_mvn 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn --- [INFO] Deleting D:\003---WorkSpace\06---testmaven\test_mvn\target [INFO] [INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn --- [INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources [INFO] [INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn --- [INFO] Changes detected - recompiling the module! [INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes [INFO] [INFO] --- maven-resources-plugin:3.0.2:testResources (default-testResources) @ test_mvn --- [INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\test\resources [INFO] [INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ test_mvn --- [INFO] Changes detected - recompiling the module! [INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\test-classes [INFO] [INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ test_mvn --- [INFO] [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running test_mvn_pkg.AppTest [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.057 s - in test_mvn_pkg.AppTest [INFO] [INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3.227 s [INFO] Finished at: 2018-04-17T15:00:33+08:00 [INFO] ------------------------------------------------------------------------ 打包项目 # cmd mvn clean package # log D:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean package [INFO] Scanning for projects... [INFO] [INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;----------------- [INFO] Building test_mvn 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn --- [INFO] Deleting D:\003---WorkSpace\06---testmaven\test_mvn\target [INFO] [INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn --- [INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources [INFO] [INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn --- [INFO] Changes detected - recompiling the module! [INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes [INFO] [INFO] --- maven-resources-plugin:3.0.2:testResources (default-testResources) @ test_mvn --- [INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\test\resources [INFO] [INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ test_mvn --- [INFO] Changes detected - recompiling the module! [INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\test-classes [INFO] [INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ test_mvn --- [INFO] [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running test_mvn_pkg.AppTest [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.052 s - in test_mvn_pkg.AppTest [INFO] [INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] [INFO] [INFO] --- maven-jar-plugin:3.0.2:jar (default-jar) @ test_mvn --- [INFO] Building jar: D:\003---WorkSpace\06---testmaven\test_mvn\target\test_mvn-1.0-SNAPSHOT.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3.599 s [INFO] Finished at: 2018-04-17T15:02:35+08:00 [INFO] ------------------------------------------------------------------------ 运行项目 # cmd # 1.无参数，类在target下面test_mvn\target\classes\test_mvn_pkg\App.class mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; # 即 mvn exec:java -Dexec.mainClass=&quot;test_mvn_pkg.App&quot; # # 2.有参数 mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.args=&quot;arg0 arg1 arg2&quot; # # 3.指定对classpath的运行时依赖 mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.classpathScope=runtime # # log D:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn exec:java -Dexec.mainClass=&quot;test_mvn_pkg.App&quot; [INFO] Scanning for projects... [INFO] [INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;----------------- [INFO] Building test_mvn 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- exec-maven-plugin:1.6.0:java (default-cli) @ test_mvn --- Hello World! [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1.010 s [INFO] Finished at: 2018-04-17T15:09:49+08:00 [INFO] ------------------------------------------------------------------------ 项目部署 # 前提是jboss web server已经成功启动 # cmd mvn clean jboss-as:deploy eclipse上的maven项目 打开eclipse进行java配置，然后关闭 windows &gt; preferences &gt; java &gt; installed jres &gt; add jdk floder and jre floder select jdk floder &gt; apply eclipse上的maven插件M2Eclipse help menu &gt; install new software &gt; input the url as follow http://download.eclipse.org/technology/m2e/releases/ # 备注插件官网 http://www.eclipse.org/m2e/ # 该插件可以解决mvn install报错问题 eclipse中的maven配置 windows &gt; preferences &gt; maven &gt; installations &gt; maven &gt; $(maven_home) &gt; apply windows &gt; preferences &gt; maven &gt; users seting &gt; user setting &gt; C:\app3\apache-maven-3.5.3\conf\settings.xml windows &gt; preferences &gt; maven &gt; users seting &gt; local repository &gt; C:\Users\BinLee\.m2\repository 创建maven项目 创建 New-&gt;Other…-&gt;Maven-&gt;Maven Project use default workspace location archetypes maven-archetype-quickstart new maven project com.hikvision.big_data.data test_eclipse_maven 0.0.1-SNAPSHOT com.hikvision.big_data.data.test_eclipse_maven 其中pom.xml &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.hikvision.big_data.data&lt;/groupId&gt; &lt;artifactId&gt;test_eclipse_maven&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;test_eclipse_maven&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 在网站上找到自己需要的依赖 http://mvnrepository.com/ # 比如：我需要找到time相关的操作，直接mavenrepository中搜索time， 得到的Joda Time # 再用google搜索Joda Time，查看其用法 # mavenrepository中的Joda Time依赖添加到pom.xml &lt;!-- https://mvnrepository.com/artifact/org.webjars.npm/d3-array --&gt; &lt;!-- https://mvnrepository.com/artifact/joda-time/joda-time --&gt; &lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;version&gt;2.9.9&lt;/version&gt; &lt;/dependency&gt; # 保存自动下载 # 使用everything搜索Joda Time发现已经在C:\Users\BinLee\.m2\repository\joda-time\joda-time\2.9.9\joda-time-2.9.9.jar下面 使用依赖 # 在窗口上project explorer&gt;maven dependencies查看需要的依赖类 # 在需要地方直接插入 # code package com.hikvision.big_data.data.test_eclipse_maven; import org.joda.time.DateTime; import org.joda.time.Days; import org.joda.time.LocalDateTime; /** * Hello world! * */ public class App { public static void main(String[] args) { System.out.println(&quot;Hello World!&quot;); DateTime now = DateTime.now(); System.out.println(now); Days maxValue = Days.MAX_VALUE; System.out.println(maxValue); System.out.println(LocalDateTime.now()); } } # output Hello World! 2018-04-17T16:15:02.211+08:00 P2147483647D 2018-04-17T16:15:02.289 关于maven源码打包 命令行方式，他山之石 cd {项目目录下} mvn source:jar mvn source:test-jar eclipse中pom.xml结尾加入插件，然后执行maven install ... &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 利用idea运行maven install报错的问题解决 参考 他山之石 右边maven projects &gt; lifecycle &gt; install 不要点击plugins &gt; install会报错]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>maven</tag>
        <tag>配置</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse中的git基本配置]]></title>
    <url>%2F2018%2F04%2F16%2Feclipse%E4%B8%AD%E7%9A%84git%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本文介绍了如何使用git和eclipse进行代码的版本控制。 命令行模式操作 服务端注册github或者giteee账号 客户端下载git软件 使用命令生成本地的密钥 将秘钥添加到服务端的git中 服务端新建git仓库，客户端克隆到本地 客户端添加文件到仓库中，使用各种命令对该仓库进行版本控制 上述的属于git的基本操作详细步骤参考 如何利用ubuntu实现私有git服务端-附ssh操作？ eclipse中git上传代码 服务端已经添加了客户端的ssh密钥 服务端已经新建了仓库 客户端eclipse新建项目 在路径eclipse&gt;windows&gt;preference&gt;team&gt;git&gt;configuration下查看user和passwd的配置 在路径package explorer&gt;项目右键&gt;share project&gt;repository&gt;create，新建本地的仓库名字要和服务端的名字一致，如：d:\test.git，完成了新建仓库 在路径package explorer&gt;项目右键&gt;team&gt;add to index，完成文件的add 在路径package explorer&gt;项目右键&gt;team&gt;commit或者Ctrl+#，提交 接上一步，先填写commit message 接上一步，填写服务器地址remote name: origin url: git@github.com:xjdlb/testgit.git # git 地址 hostname: github.com # 域名 repository path: xjdlb/testgit.git 一路next就好了 他山之石 eclipse中git下载代码 在路径package explorer&gt;空白右键&gt;import&gt;Git&gt;Projects from Git，next 接上步，选择URI，包含了远程和本地 主要的分支 新建本地的仓库，如：d:\test.git 继续coding 返回上面上传代码操作 eclipse push 出现了 rejected-non-fast-forward错误 他山之石 打开windows&gt;show view&gt;other&gt;git repositories git repositories&gt;remote&gt;origin&gt;绿色分支&gt;右键&gt;configure fetch&gt;save and fetch 此时可以看见remote tracking&gt;origin/mater&gt;右键&gt;merge 问题解决，可以上传了 add&gt;commit&gt;push]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>eclipse</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用kcptun加速ss服务]]></title>
    <url>%2F2018%2F04%2F15%2F%E4%BD%BF%E7%94%A8kcptun%E5%8A%A0%E9%80%9Fss%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用kcptun加速ss服务。 软件准备 安装组件apt-get update apt-get upgrade apt-get install build-essential python-pip m2crypto supervisor 安装sspip install shadowsocks 安装加密用软件 libsodiumwget https://github.com/jedisct1/libsodium/releases/download/1.0.11/libsodium-1.0.11.tar.gz tar zxvf libsodium-1.0.11.tar.gz cd libsodium-1.0.11 ./configure make &amp;&amp; make check make install echo /usr/local/lib &gt; /etc/ld.so.conf.d/usr_local_lib.conf ldconfig [可选] 配置supervisor, vi /etc/supervisor/conf.d/shadowsocks.conf [program:shadowsocks] command=ssserver -c /etc/shadowsocks.json autorestart=true user=root [可选] 使用supervisor supervisorctl reload supervisorctl status ss配置 ss服务器配置ss_config.json{ &quot;server&quot;: &quot;127.0.0.1&quot;, &quot;port_password&quot;: { &quot;10001&quot;: &quot;helloworld&quot;, &quot;10002&quot;: &quot;helloworld&quot;, &quot;10003&quot;: &quot;helloworld&quot; }, &quot;local_port&quot;: 1080, &quot;timeout&quot;: 600, &quot;method&quot;: &quot;chacha20&quot;, &quot;auth&quot;: true } 启动和停止脚本ssserver -c /root/shadowsocks/ss_config.json -d start ssserver -c /root/shadowsocks/ss_config.json -d stop kcptun配置 kcptun官网 https://github.com/xtaci/kcptun/releases 其中 kcptun-linux-amd64-20180316.tar.gz 为Linux版本 其中 kcptun-windows-amd64-20180316.tar.gz 为Windows版本 安装 kcptunmkdir /root/kcptun cd /root/kcptun ln -sf /bin/bash /bin/sh wget https://github.com/xtaci/kcptun/releases/download/v20161118/kcptun-linux-amd64-20161118.tar.gz tar -zxf kcptun-linux-amd64-*.tar.gz 配置三个脚本start.sh, stop.sh, server-config.json 启动脚本vi /root/kcptun/start.sh #!/bin/bash cd /root/kcptun/ ./server_linux_amd64 -c /root/kcptun/server-config.json &gt; kcptun.log 2&gt;&amp;1 &amp; echo &quot;Kcptun started.&quot; 停止脚本 vi /root/kcptun/stop.sh #!/bin/bash echo &quot;Stopping Kcptun...&quot; PID=`ps -ef | grep server_linux_amd64 | grep -v grep | awk &#39;{print $2}&#39;` if [ &quot;&quot; != &quot;$PID&quot; ]; then echo &quot;killing $PID&quot; kill -9 $PID fi echo &quot;Kcptun stoped.&quot; kcptun配置文件 vi /root/kcptun/server-config.json { &quot;listen&quot;: &quot;:443&quot;, &quot;target&quot;: &quot;127.0.0.1:10001&quot;, &quot;key&quot;: &quot;helloworld&quot;, &quot;crypt&quot;: &quot;salsa20&quot;, &quot;mode&quot;: &quot;fast2&quot;, &quot;mtu&quot;: 1350, &quot;sndwnd&quot;: 1024, &quot;rcvwnd&quot;: 1024, &quot;datashard&quot;: 5, &quot;parityshard&quot;: 5, &quot;dscp&quot;: 46, &quot;nocomp&quot;: true, &quot;acknodelay&quot;: false, &quot;nodelay&quot;: 0, &quot;interval&quot;: 40, &quot;resend&quot;: 0, &quot;nc&quot;: 0, &quot;sockbuf&quot;: 4194304, &quot;keepalive&quot;: 10 } 启动或停止kcptunsh /root/kcptun/start.sh sh /root/kcptun/stop.sh 客户端windows环境中的kcptun配置 kcptun官网 https://github.com/xtaci/kcptun/releases client_windows_amd64.exe 放在全部英文目录下 创建下面的三个文件：run.vbs, client-config.json, stop.sh 在当前文件夹下，创建 run.vbsDim RunKcptun Set fso = CreateObject(&quot;Scripting.FileSystemObject&quot;) Set WshShell = WScript.CreateObject(&quot;WScript.Shell&quot;) currentPath = fso.GetFile(Wscript.ScriptFullName).ParentFolder.Path &amp; &quot;\&quot; configFile = currentPath &amp; &quot;client-config.json&quot; logFile = currentPath &amp; &quot;kcptun.log&quot; exeConfig = currentPath &amp; &quot;client_windows_amd64.exe -c &quot; &amp; configFile cmdLine = &quot;cmd /c &quot; &amp; exeConfig &amp; &quot; &gt; &quot; &amp; logFile &amp; &quot; 2&gt;&amp;1&quot; WshShell.Run cmdLine, 0, False &#39;WScript.Sleep 1000 &#39;Wscript.echo cmdLine Set WshShell = Nothing Set fso = Nothing WScript.quit 在当前文件夹下，创建client-config.json{ &quot;localaddr&quot;: &quot;:12345&quot;, &quot;remoteaddr&quot;: &quot;165.227.213.57:443&quot;, &quot;key&quot;: &quot;helloworld&quot;, &quot;crypt&quot;: &quot;salsa20&quot;, &quot;mode&quot;: &quot;fast2&quot;, &quot;conn&quot;: 1, &quot;autoexpire&quot;: 60, &quot;mtu&quot;: 1350, &quot;sndwnd&quot;: 128, &quot;rcvwnd&quot;: 1024, &quot;datashard&quot;: 5, &quot;parityshard&quot;: 5, &quot;dscp&quot;: 46, &quot;nocomp&quot;: true, &quot;acknodelay&quot;: false, &quot;nodelay&quot;: 0, &quot;interval&quot;: 40, &quot;resend&quot;: 0, &quot;nc&quot;: 0, &quot;sockbuf&quot;: 4194304, &quot;keepalive&quot;: 10 } 在当前文件夹下，创建stop.shtaskkill /f /im client_windows_amd64.exe 客户端windows环境中的ss配置 使用本地的配置127.0.0.1 12345 helloworld(服务端ss的密码，不是kcptun的密码) chacha20]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>kcptun</tag>
        <tag>ss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo如何开启语法高亮？]]></title>
    <url>%2F2018%2F04%2F03%2Fhexo%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文介绍hexo有关语法高亮的配置方案。 配置过程 配置主站点下的配置文件highlight: enable: true line_number: true auto_detect: true tab_replace: 代码后面添加名称，如```java code ```]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LC_001_TwoSum_HashMap]]></title>
    <url>%2F2018%2F04%2F03%2FLC_001_TwoSum_HashMap%2F</url>
    <content type="text"><![CDATA[leetcode第001题，主要用到了hashmap数据结构。 题解package LC; import java.util.Arrays; import java.util.HashMap; /** * https://leetcode.com/problems/two-sum/description/ * Given an array of integers, * return indices of the two numbers such that they add up to a specific target. * You may assume that each input would have exactly one solution, * and you may not use the same element twice. * Example: * Given nums = [2, 7, 11, 15], target = 9, * Because nums[0] + nums[1] = 2 + 7 = 9, * return [0, 1]. */ public class LC_001_TwoSum_HashMap { public static void main(String[] args) { int[] a = {1, 2, 3, 4, 5, 7}; int t = 10; System.out.println(Arrays.toString(twoSum(a, t))); } private static int[] twoSum(int[] nums, int target) { HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) { int diff = target - nums[i]; if (map.containsKey(diff)) return new int[]{map.get(diff), i}; map.put(nums[i], i); } throw new IllegalArgumentException(&quot;-1&quot;); } }]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>leetcode</tag>
        <tag>java</tag>
        <tag>basic algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何去掉valine的Powered By信息？]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%A6%82%E4%BD%95%E5%8E%BB%E6%8E%89valine%E7%9A%84Powered%20By%E4%BF%A1%E6%81%AF%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文介绍如何去掉valine页面上的Powered By信息。 步骤 找到配置文件blog/themes/next/layout/_third-party/comments/valine.swig 配置如下{% if theme.valine.enable and theme.valine.appid and theme.valine.appkey %} var GUEST = ['nick','mail','link']; var guest = '{{ theme.valine.guest_info }}'; guest = guest.split(',').filter(item=>{ return GUEST.indexOf(item)>-1; }); new Valine({ el: '#comments' , verify: {{ theme.valine.verify }}, notify: {{ theme.valine.notify }}, appId: '{{ theme.valine.appid }}', appKey: '{{ theme.valine.appkey }}', placeholder: '{{ theme.valine.placeholder }}', avatar:'{{ theme.valine.avatar }}', guest_info:guest, pageSize:'{{ theme.valine.pageSize }}' || 10, }); //新增以下代码即可，可以移除.info下所有子节点。 var infoEle = document.querySelector('#comments .info'); if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){ infoEle.childNodes.forEach(function(item) { item.parentNode.removeChild(item); }); } {% endif %}]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>valine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在next配置站内的搜索引擎？]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%A6%82%E4%BD%95%E5%9C%A8next%E9%85%8D%E7%BD%AE%E7%AB%99%E5%86%85%E7%9A%84%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文介绍如何在next配置站内的搜索引擎。 配置过程 安装hexo-generator-searchdb插件，以管理员身份打开cmd进入项目目录下，运行npm install hexo-generator-searchdb --save 在站点的-config.yml文件中增加search: path: search.xml field: post format: html limit: 10000 配置theme/next/-config.yml文件# Algolia Search algolia_search: enable: false hits: per_page: 10 labels: input_placeholder: Search for Posts hits_empty: &quot;We didn&#39;t find any results for the search: ${query}&quot; hits_stats: &quot;${hits} results found in ${time} ms&quot; # # Local search # Dependencies: https://github.com/flashlab/hexo-generator-search local_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux shell入门练习]]></title>
    <url>%2F2018%2F04%2F03%2Flinux%20shell%E5%85%A5%E9%97%A8%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[本文介绍linux shell的入门程序，后期会陆续更新。 基本程序实例 计算文件夹下的文件数量 #!/bin/bash echo &quot;this is a shell print file&#39;s number in the local dir.&quot; ls &gt; filename.log y=1 for i in $( cat filename.log ) do echo &quot;the file number is $y&quot; y=$(( $y + 1 )) done rm -rf filename.log 简单求和程序 #!/bin/bash # author: leebin s=0 for(( i=1; i&lt;=100; i=i+1 )) do s=$(( $s+$i )) done echo &quot;the sum of 1+2+3+...+100 is $s&quot; 使用数组 #!/bin/bash for x in morning noon afternoon evening do echo &quot;This time is $x&quot; done 使用函数 #!/bin/bash function func1(){ echo AAA } func1 echo this is the end of the loop echo Now this is the end of the script 判断 #!/bin/bash if [ -d /etc/mysql ] then echo &quot;the path is right!!&quot; else echo &quot;the path is not right&quot; fi 判断硬盘是否已经满了 #!/bin/bash # Author: LeeBin rate=$( df | grep &quot;sda&quot; | awk &#39;{print $5}&#39;| cut -d &quot;%&quot; -f 1 ) if [ $rate -ge 80 ] then echo &quot;Warning! /dev/sda1 is full!!&quot; else echo &quot;/dev/sda1 is not full!!&quot; fi until循环 #!/bin/bash # Author:LeeBin i=1 s=0 until [ $i -gt 100 ] do s=$(( $s+$i )) i=$(( $i+1 )) done echo &quot;the sum is $s&quot; while循环 #!/bin/bash function func1(){ echo this is an example of a function } count=1 while [ $count -le 5 ] do func1 count=$[ $count+1 ] done echo end of loop func1 echo end of script while循环求和 #!/bin/bash # Author:LeeBin i=1 s=0 # while [ $i -le 100 ] do s=$(( $s+$i )) i=$(( $i+1 )) done echo &quot;the sum is $s&quot; 备份脚本 #!/bin/sh # auto mail for system info # time /bin/date +%F &gt;&gt; ~/app/shell/sysinfo echo &gt;&gt; ~/app/shell/sysinfo # disk info echo &quot;disk info:&quot; &gt;&gt; ~/app/shell/sysinfo /bin/df -h &gt;&gt; ~/app/shell/sysinfo echo &gt;&gt; ~/app/shell/sysinfo echo &quot;online users&quot; &gt;&gt; ~/app/shell/sysinfo /usr/bin/who | /bin/grep -v root &gt;&gt; ~/app/shell/sysinfo echo &gt;&gt; ~/app/shell/sysinfo echo &quot;memory info:&quot; &gt;&gt; ~/app/shell/sysinfo /usr/bin/free -m &gt;&gt; ~/app/shell/sysinfo echo &gt;&gt; ~/app/shell/sysinfo case语句 #!/bin/bash # author: leebin read -p &quot;Please choose yes/no: &quot; -t 30 cho # case $cho in &quot;yes&quot;) echo &quot;Your choose is yes!!&quot; ;; &quot;no&quot;) echo &quot;Your choose is no!!&quot; ;; *) echo &quot;Your choose is error!!&quot; ;; esac # 批量解压缩 #!/bin/bash # author: leebin cd /lamp ls *.tar.gz &gt; ls.log # for i in $( cat ls.log ) do tar -zxvf $i &amp;&gt; /dev/null done # rm -rf /lamp/ls.log]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将hexo的git pages项目部署vps？]]></title>
    <url>%2F2018%2F03%2F29%2F%E5%A6%82%E4%BD%95%E5%B0%86hexo%E7%9A%84git%20pages%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2vps%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文介绍如何在vps上搭建自己的blog。 环境 digitalocean上ubuntu的vps一台 window10+nodejs+hexo软件环境，参考hexo搭建博客 思路 方案一 vps上使用本地模式搭建hexo博客，使用Nginx将域名指向 http://localhost:4000 方案二 在客户端写blog，git推送到服务端，服务端用Nginx解析网页文件 过程 下文分为众多的详细步骤 安装git和nginx 安装软件git和nginxapt-get update apt-get install git apt-get install nginx 配置git用户和仓库 git用户权限设定（可以不需要）chmod 740 /etc/sudoers vim /etc/sudoers #在root ALL=(ALL:ALL) ALL下面新增一行 git ALL=(ALL:ALL) ALL chmod 440 /etc/sudoers 配置git用户和仓库, 参考在vps上构建私有git服务器 配置git hooks 在hexo.git/hooks/目录下修改post-update.sample为post-update，并覆盖加入#!/bin/bash GIT_REPO=/home/git/hexo.git TMP_GIT_CLONE=/tmp/hexo PUBLIC_WWW=/var/www/hexo rm -rf ${TMP_GIT_CLONE} git clone $GIT_REPO $TMP_GIT_CLONE rm -rf ${PUBLIC_WWW}/* cp -rf ${TMP_GIT_CLONE}/* ${PUBLIC_WWW} 保证post-update有执行权限chmod +x post-receive nginx配置 新建站点文件夹mkdir -p /var/www/blog chmod -R 775 /var/www/blog chown -R git /var/www/blog chgrp -R git /var/www/blog 配nginx的站点文件2处#配置1 vim /etc/nginx/conf.d/hexo.conf server { listen 80 ; listen [::]:80; root /var/www/blog; server_name clearsky.me www.clearsky.me; #server_ip access_log /var/log/nginx/hexo_access.log; error_log /var/log/nginx/hexo_error.log; error_page 404 = /404.html; location ~* ^.+\.(ico|gif|jpg|jpeg|png)$ { root /var/www/blog; access_log off; expires 1d; } location ~* ^.+\.(css|js|txt|xml|swf|wav)$ { root /var/www/blog; access_log off; expires 10m; } location / { root /var/www/blog; if (-f $request_filename) { rewrite ^/(.*)$ /$1 break; } } location /nginx_status { stub_status on; access_log off; } } #配置2 vim /etc/nginx/sites-available/default root /var/www/html; 重启nginx服务器service nginx restart #或者 /etc/init.d/nginx stop /etc/init.d/nginx start 后续 修改本地的blog源文件，配置推送git服务器，推送到vps服务器上 参考git pages多服务器部署]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>git</tag>
        <tag>vps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git pages的迁移和多服务器部署]]></title>
    <url>%2F2018%2F03%2F29%2Fgit%20pages%E7%9A%84%E8%BF%81%E7%A7%BB%E5%92%8C%E5%A4%9A%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[本文介绍如何将github上的项目迁移到gitee上，如何实现源文件多服务器的部署。 github pages迁移到gitee服务器上 在gitee上新建一个和gitee用户名一样的git仓库，并且在pages标签开启pages服务 克隆github pages仓库到本地，安装必要的插件，保证github pages能够与github服务器正常上传部署 更改github仓库/.git/config文件[core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [remote &quot;origin&quot;] url = git@github.com:xjdlb/xjdlb.github.io.git fetch = +refs/heads/*:refs/remotes/origin/* [branch &quot;master&quot;] remote = origin merge = refs/heads/master 更改url为gitee仓库的url 更改github仓库/-config.yml文件deploy: type: git repo: gitee: git@gitee.com:bin_lee/bin_lee.git branch: master 更改repo为gitee仓库的url 然后使用下面的脚本提交、推送、发布到gitee仓库，迁移就成功了echo &quot;hello&quot; yy=$(date +%y) mm=$(date +%m) dd=$(date +%d) HH=$(date +%H) MM=$(date +%M) SS=$(date +%S) xW=$(date +%U) we=$(date +%a) xD=$(date +%j) git status git add . git commit -m &quot; $yy/$mm/$dd-$HH:$MM:$SS 把github服务器上的pages迁移到gitee上 &quot; echo &quot;==================================&quot; git push git@gitee.com:bin_lee/bin_lee.git hexo git log --oneline | head echo &quot;==================================&quot; hexo clean &amp;&amp; hexo g -d 迁移完成，实现多服务器部署 更改github仓库/.git/config文件，改为主要的服务器地址[core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [remote &quot;origin&quot;] url = git@github.com:xjdlb/xjdlb.github.io.git fetch = +refs/heads/*:refs/remotes/origin/* [branch &quot;master&quot;] remote = origin merge = refs/heads/master 更改url为gitee仓库的url 更改github仓库/-config.yml文件deploy: type: git repo: github: git@github.com:xjdlb/xjdlb.github.io.git gitee: git@gitee.com:bin_lee/bin_lee.git branch: master 更改repo为gitee仓库的url 然后使用下面的脚本提交、推送、发布到gitee仓库和github仓库，多服务器部署就成功了echo &quot;hello&quot; yy=$(date +%y) mm=$(date +%m) dd=$(date +%d) HH=$(date +%H) MM=$(date +%M) SS=$(date +%S) xW=$(date +%U) we=$(date +%a) xD=$(date +%j) git status git add . git commit -m &quot; $yy/$mm/$dd-$HH:$MM:$SS 同时部署到两个服务器上测试 &quot; git push git@github.com:xjdlb/xjdlb.github.io.git hexo git log --oneline | head echo &quot;==================================&quot; git push git@gitee.com:bin_lee/bin_lee.git hexo git log --oneline | head echo &quot;==================================&quot; hexo clean &amp;&amp; hexo g -d]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将hexo源文件整合到部署的git中？]]></title>
    <url>%2F2018%2F03%2F28%2F%E5%A6%82%E4%BD%95%E5%B0%86hexo%E6%BA%90%E6%96%87%E4%BB%B6%E6%95%B4%E5%90%88%E5%88%B0%E9%83%A8%E7%BD%B2%E7%9A%84git%E4%B8%AD%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文介绍如何将hexo源文件整合到部署的git中，实现不携带源文件也能写博客，其中，发布和部署实现了自动化脚本操作。 先决条件 已经使用了hexo部署了自己的blog 源文件没有丢失 克隆 在github上克隆部署后的文件到本地客户端 使用git bash here进入git仓库 新建hexo分支并转换到hexo分支git checkout -b hexo 拷贝 git仓库转换到hexo分支 将源文件blog文件夹下的所有文件拷贝到上述git仓库中 创建自动化脚本 在git仓库的根目录下创建脚本 脚本1 create_new_page.shecho &quot;hello&quot; yy=$(date +%Y) mm=$(date +%m) dd=$(date +%d) HH=$(date +%H) MM=$(date +%M) SS=$(date +%S) filename=&quot;11111&quot; filepostfix=&quot;.md&quot; cd source/_posts touch $filename$filepostfix echo &gt; $filename$filepostfix echo &quot;---&quot; &gt;&gt; $filename$filepostfix echo &quot;title: $filename&quot; &gt;&gt; $filename$filepostfix echo &quot;date: $yy-$mm-$dd $HH:$MM:$SS&quot; &gt;&gt; $filename$filepostfix echo &quot;tags: [列表,2222,3333,4444]&quot; &gt;&gt; $filename$filepostfix echo &quot;categories: 5555&quot; &gt;&gt; $filename$filepostfix echo &quot;toc: true&quot; &gt;&gt; $filename$filepostfix echo &quot;mathjax: true&quot; &gt;&gt; $filename$filepostfix echo &quot;---&quot; &gt;&gt; $filename$filepostfix echo &quot;&quot; &gt;&gt; $filename$filepostfix echo &quot;&lt;!-- more --&gt;&quot; &gt;&gt; $filename$filepostfix cd ../.. 文本1 commit.txtecho &quot;hello&quot; yy=$(date +%y) mm=$(date +%m) dd=$(date +%d) HH=$(date +%H) MM=$(date +%M) SS=$(date +%S) xW=$(date +%U) we=$(date +%a) xD=$(date +%j) git status git add . git commit -m &quot; $yy/$mm/$dd-$HH:$MM:$SS 新增了列表标签 &quot; git push origin hexo git log --oneline | head echo &quot;==================================&quot; hexo clean &amp;&amp; hexo g -d 脚本2 upload_and_deploy.shehco &quot;push and deploy...&quot; sh commit.txt 本次修改完成直接在commit.txt中修改commit，然后运行upload_and_deploy.sh，即可上传代码到hexo分支，发布blog到master分支 换电脑，安装环境，继续写作 先决条件：电脑+网络+nodejs+hexo 克隆仓库到本地 在仓库中建立hexo配置脚本 init_hexo_after_clone.shgit checkout hexo npm install hexo npm install npm install hexo-deployer-git]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>git</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo部署在github和gitee上的坑]]></title>
    <url>%2F2018%2F03%2F28%2Fhexo%E9%83%A8%E7%BD%B2%E5%9C%A8github%E5%92%8Cgitee%E4%B8%8A%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[本文介绍了hexo博客github和gitee上部署时候遇到的坑。 遇到的坑列举如下 gitee不支持个性化的域名绑定，所以不要试图申请阿里云的域名，将域名指向gitee pages。 github pages支持个性化域名的绑定，需要在blog/source目录下新建CNAME文件，并写入自己域名。]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>gitee</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo的主题相关的配置]]></title>
    <url>%2F2018%2F03%2F27%2Fhexo%E7%9A%84%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本文将逐渐介绍blog和themes相关的配置方法. 导航上的首页、标签、分类、关于等配置 保留blog下的配置文件中的首页、标签、分类、关于的目录正确 在theme下配置文件打开menu相关的导航 博文前面文件为：title: 使用github pages和hexo搭建自己的博客 date: 2018-03-27 13:56:08 tags: [githubpages,hexo,配置] categories: 配置 toc: true mathjax: true 或者title: 如何利用ubuntu云服务器实现私有git服务端-附ssh常见操作？ date: 2018-03-27 18:37:32 tags: - git - 配置 categories: 配置 toc: true mathjax: true]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>hexo</tag>
        <tag>githubpages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用github pages和hexo搭建自己的博客]]></title>
    <url>%2F2018%2F03%2F27%2F%E4%BD%BF%E7%94%A8github%20pages%E5%92%8Chexo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84blog%2F</url>
    <content type="text"><![CDATA[本文描述了如何使用github pages和hexo搭建自己的博客。 安装node.js node.js下载地址 下载node.js，并安装 安装git并配置ssh密钥 在客户端下载git下载地址 安装git 在客户端右键打开git bash here 设置user.name和user.emailgit config --global user.name &quot;你的GitHub用户名&quot; git config --global user.email &quot;你的GitHub注册邮箱&quot; 生成ssh密钥ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; 显示密钥cat ~/.ssh/id_rsa.pub 添加密钥到github服务器中密钥管理添加地址 安装hexo 安装hexo、安装扩展插件# 安装hexo npm install hexo-cli g # 初始化博客文件夹 hexo init blog # 切换到该路径 cd blog # 安装hexo的扩展插件 npm install # 安装其它插件 npm install hexo-server --save npm install hexo-admin --save npm install hexo-generator-archive --save npm install hexo-generator-feed --save npm install hexo-generator-search --save npm install hexo-generator-tag --save npm install hexo-deployer-git --save npm install hexo-generator-sitemap --save 本地开发blog与本地测试 添加自己的markdown到 blog/source/posts目录下 生成静态页面并开启服务器# 生成静态页面 hexo generate # 开启本地服务器 hexo s # 或者 hexo s -p 指定的port 打开浏览器，地址栏中输入：http://localhost:4000/ 服务端新建自己的博客仓库 在 https://github.com/new 中新建自己的仓库 其中Repository name要和Owner是一致的 客户端将hexo博客部署到github上 修改配置文件blog/config.yml，修改deploy项的内容# Deployment 注释 ## Docs: https://hexo.io/docs/deployment.html deploy: # 类型 type: git # 仓库 repo: git@github.com:xjdlb/xjdlb.github.io.git # 分支 branch: master 注意：type: git中的冒号后面由空格 注意：将xjdlb换成自己的用户名 客户端将自己的blog部署hexo 将自己的项目部署到github pages中# 清空静态页面 hexo clean # 生成静态页面 hexo generate # 部署 hexo deploy 打开网页，输入 http://github_username.github.io 打开github上托管的博客 如我的博客地址是：http://xjdlb.github.io hexo命令缩写与组合 含义hexo g：hexo generate hexo c：hexo clean hexo s：hexo server hexo d：hexo deploy 组合# 清除、生成、启动 hexo clean &amp;&amp; hexo g -s # 清除、生成、部署 hexo clean &amp;&amp; hexo g -d 主题相关配置 在hexo themes中下载相关的主题 下载方法在blog目录中克隆git clone https://github.com/iissnan/hexo-theme-next themes/next 在blog/config.yml中配置主题theme: next 新建blog文件 hexo new “Hexo教程” 添加标题及其分类信息title: Hello World date: 2016-01-15 20:19:32 tags: [SayHi] categories: SayHi toc: true mathjax: true 或者 在blog目录下可以写成脚本yy=$(date +%Y) mm=$(date +%m) dd=$(date +%d) HH=$(date +%H) MM=$(date +%M) SS=$(date +%S) filename=&quot;11111&quot; filepostfix=&quot;.md&quot; cd source/_posts touch $filename$filepostfix echo &gt; $filename$filepostfix echo &quot;---&quot; &gt;&gt; $filename$filepostfix echo &quot;title: $filename&quot; &gt;&gt; $filename$filepostfix echo &quot;date: $yy-$mm-$dd $HH:$MM:$SS&quot; &gt;&gt; $filename$filepostfix echo &quot;tags: [2222,3333,4444]&quot; &gt;&gt; $filename$filepostfix echo &quot;categories: 5555&quot; &gt;&gt; $filename$filepostfix echo &quot;toc: true&quot; &gt;&gt; $filename$filepostfix echo &quot;mathjax: true&quot; &gt;&gt; $filename$filepostfix echo &quot;---&quot; &gt;&gt; $filename$filepostfix cd ../.. 将github pages绑定自己的域名 在阿里云控制台找到域名管理 在阿里云上购买自己的域名注册地址 在xjdlb/xjdlb.github.io/settings中Custom domain处添加自己的域名，不要http://和www ping https://xjdlb.github.io/ 查看github pages的ip 添加解析 记录类型 主机记录 解析线路 记录值 TTL值 A @ 默认 151.101.41.147 600 A www 默认 151.101.41.147 600 使用自己的域名测试 CNAME问题问题：每次hexo deploy之后，https://www.leebin.top 都会出现404错误一般解决：Github pages–&gt;Settings–&gt;Custom domain最优解决：在将CNAME文件放在source目录下，CNAME文件内容为：leebin.top 环境变更 换电脑，安装环境git config --global user.name &quot;你的GitHub用户名&quot; git config --global user.email &quot;你的GitHub注册邮箱&quot; node -v npm -v git --version npm intsall hexo -g 或 npm install hexo-cli g hexo -v npm install npm install hexo-deployer-git --save # 下面是全部组件，源git仓库不需要全部用上 npm install hexo-server --save npm install hexo-admin --save npm install hexo-generator-archive --save npm install hexo-generator-feed --save npm install hexo-generator-search --save npm install hexo-generator-tag --save npm install hexo-deployer-git --save npm install hexo-generator-sitemap --save]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何利用ubuntu实现私有git服务端-附ssh操作？]]></title>
    <url>%2F2018%2F03%2F27%2F%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ubuntu%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89git%E6%9C%8D%E5%8A%A1%E7%AB%AF-%E9%99%84ssh%E6%93%8D%E4%BD%9C%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文介绍如何利用云服务器实现私有git服务端，包含了git新建仓库、本地与服务器的ssh互连、保留gitlog迁移git的方法、以及创建仓库的自动化脚本。 在服务端下载git 下载安装gitapt-get update apt-get install git -y 配置git用户 添加git用户useradd git passwd git 通过ssh客户端和服务器互连 客户端生成ssh密钥git config --global user.name &quot;你的GitHub用户名&quot; git config --global user.email &quot;你的GitHub注册邮箱&quot; ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; cat ~/.ssh/id_rsa.pub 或者 上述操作可以集成为无交互的脚本在本地直接执行即可y=$(date +%y) m=$(date +%m) d=$(date +%d) H=$(date +%H) M=$(date +%M) S=$(date +%S) path=$(pwd) cd ~ git config --global user.name &quot;bin_lee&quot; git config --global user.email &quot;xjd.binlee@qq.com&quot; #cd ~/.ssh tar -zcvf ssh_binlee_backup_$y-$m-$d-$H-$M-$S.tar.gz .ssh rm -rfd ~/.ssh # ssh-keygen -t rsa -C &quot;xjd.binlee@qq.com&quot; ssh-keygen -t rsa -P &quot;&quot; -C &quot;xjd.binlee@qq.com&quot; -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub cd $path 服务端安装ssh并实现xshell连接 #安装 sudo apt-get install openssh-server -y sudo ps -e |grep ssh sudo service ssh start sudo passwd root sudo vi /etc/ssh/sshd_config PermitRootLogin prohibit-password PermitRootLogin yes sudo service ssh restart #在服务器的指定用户目录下 mkdir -p /root/.ssh touch authorized_keys 将上述生成的密钥文件添加到服务端 echo &quot;密钥&quot; &gt;&gt; /root/.ssh/authorized_keys 客户端测试连通性ssh -T git@gitee.com 或者 ssh -T git@server_ip 新建git仓库并使用 新建git仓库mkdir -p /srv/git/repos/xxx.git cd /srv/git/repos 初始化git仓库git init --bare /srv/git/repos/xxx.git 设置git仓库的访问权限cd /srv/git/repos chmod -R 775 xxx.git chown -R git xxx.git chgrp -R git xxx.git 克隆git仓库并测试git clone git@server_ip:/srv/git/repos/xxx.git 大招 将上述操作合并为git脚本 合并如下：apt-get update echo &quot;----------------------------------------&quot; echo &quot;&gt;&gt;&gt; update finished...&quot; echo &quot;----------------------------------------&quot; apt-get install git -y echo &quot;----------------------------------------&quot; echo &quot;&gt;&gt;&gt; install finished...&quot; echo &quot;----------------------------------------&quot; #useradd git #passwd git #or #openssl passwd -stdin useradd -p &quot;8iENHwQTXrdZM&quot; git #change passwd touch chpass.txt echo &quot;git:hest&quot; &gt;&gt; chpass.txt chpasswd &lt; chpass.txt rm -rf chpass.txt echo &quot;----------------------------------------&quot; echo &quot;&gt;&gt;&gt; useradd and reset passwd finished...&quot; echo &quot;----------------------------------------&quot; key=&quot;ssh-rsa AAA.......&quot; mkdir -p /home/git/.ssh touch /home/git/.ssh/authorized_keys #vim /home/git/.ssh/authorized_keys echo &quot;${key}&quot; &gt;&gt; /home/git/.ssh/authorized_keys cat /home/git/.ssh/authorized_keys echo &quot;----------------------------------------&quot; echo &quot;&gt;&gt;&gt; add authorized_keys finished...&quot; echo &quot;----------------------------------------&quot; respos_path=&quot;/srv/git/respos/&quot; project_name=&quot;test.git&quot; project_path=${respos_path}${project_name} mkdir -p ${project_path} git init --bare ${project_path} chmod -R 775 ${project_path} chown -R git ${project_path} chgrp -R git ${project_path} echo &quot;----------------------------------------&quot; echo &quot;init git respos finished...&quot; my_ip=$(/sbin/ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk &#39;{print $2}&#39;|tr -d &quot;addr:&quot;) echo &quot;git clone git@${my_ip}:${project_path}&quot; echo &quot;----------------------------------------&quot; 如果出错销毁服务端git 删除用户和仓库userdel -r git rm -rdf /srv/git/ 如果服务器出现问题，保留gitlog迁移git的方法 使用镜像克隆保留gitlog#在源服务器上裸克隆 git clone --bare git://github.com/username/project.git cd project.git #镜像上传到新的服务器上 git push --mirror git@gitcafe.com/username/newproject.git cd .. rm -rf project.git #克隆新服务器下的工程到客户端 git clone git@gitcafe.com/username/newproject.git #设置新的上传url为新服务器的地址 git remote set-url origin remote_git_address]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何利用VPS搭建自己的ss服务器？]]></title>
    <url>%2F2018%2F03%2F27%2F%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8VPS%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84ss%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本文描述了如何在ubuntu服务器上快速搭建自己的shadowcoks代理服务器。 以详细步骤安装配置启动过程1.安装ssapt-get update sudo apt-get install python-pip -y sudo pip install shadowsocks 2.配置mkdir /etc/shadowsocks touch /etc/shadowsocks/ss_config.json vim /etc/shadowsocks/ss_config.json { &quot;server&quot;: &quot;165.227.213.57&quot;, &quot;port_password&quot;: { &quot;10001&quot;: &quot;112345678a!&quot;, &quot;10002&quot;: &quot;112345678a!&quot;, &quot;10003&quot;: &quot;112345678a!&quot; }, &quot;local_port&quot;: 1080, &quot;timeout&quot;: 600, &quot;method&quot;: &quot;aes-256-cfb&quot; } 3.启动cd ~ touch start.sh chmod 775 start.sh vim start.sh ssserver -c /etc/shadowsocks/ss_config.json -d start ssserver -c /etc/shadowsocks/ss_config_multiple.json -d start netstat -ntlp | grep python touch stop.sh chmod 775 stop.sh vim stop.sh ssserver -c /etc/shadowsocks/ss_config.json -d stop ssserver -c /etc/shadowsocks/ss_config_multiple.json -d stop netstat -ntlp | grep python 用脚本实现一键安装1.创建配置启动脚本创建x脚本 touch x &amp;&amp; chmod 775 x &amp;&amp; vim x 直接复制到x脚本里面 cd ~ &amp;&amp; touch ss_cfg.json ip=&quot;162.243.161.150&quot; echo &quot;{&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;server\&quot;: \&quot;${ip}\&quot;,&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;port_password\&quot;: {&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;10001\&quot;: \&quot;helloworld\&quot;,&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;10002\&quot;: \&quot;helloworld\&quot;,&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;10003\&quot;: \&quot;helloworld\&quot;&quot; &gt;&gt; ss_cfg.json echo &quot;},&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;local_port\&quot;: 1080,&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;timeout\&quot;: 600,&quot; &gt;&gt; ss_cfg.json echo &quot;\&quot;method\&quot;: \&quot;aes-256-cfb\&quot;&quot; &gt;&gt; ss_cfg.json echo &quot;}&quot; &gt;&gt; ss_cfg.json cd ~ touch sta.sh &amp;&amp; chmod 775 sta.sh echo &quot;ssserver -c ~/ss_cfg.json -d start&quot; &gt;&gt; ~/sta.sh echo &quot;netstat -ntlp | grep python&quot; &gt;&gt; ~/sta.sh touch sto.sh &amp;&amp; chmod 775 sto.sh echo &quot;ssserver -c ~/ss_cfg.json -d stop&quot; &gt;&gt; ~/sto.sh echo &quot;netstat -ntlp | grep python&quot; &gt;&gt; ~/sto.sh echo &quot;-------------report-------------------&quot; echo &quot;the fie list as follows:&quot; ls echo &quot;-------------start ss-----------------&quot; ./sta.sh echo &quot;-------------your ss config-----------&quot; echo &quot;ip=${ip}&quot; echo &quot;port=10001, password=helloworld&quot; echo &quot;port=10002, password=helloworld&quot; echo &quot;port=10003, password=helloworld&quot; echo &quot;local_port=1080&quot; echo &quot;timeout=600&quot; echo &quot;method=aes-256-cfb&quot; echo &quot;-------------end---------------------&quot; 2.启动服务运行x脚本 ./x 启动服务 ./sta.sh 关闭服务 ./sto.sh 删除文件 rm -rf x ss_cfg.json sta.sh sto.sh &amp;&amp; touch x &amp;&amp; chmod 775 x &amp;&amp; vim x]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>配置</tag>
        <tag>ss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F03%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>SayHi</category>
      </categories>
      <tags>
        <tag>列表</tag>
        <tag>SayHi</tag>
      </tags>
  </entry>
</search>
