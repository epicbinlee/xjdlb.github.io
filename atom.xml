<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>初心</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-05-06T09:08:34.537Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LeeBin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HBase基础开发</title>
    <link href="http://yoursite.com/2018/05/05/HBase%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91/"/>
    <id>http://yoursite.com/2018/05/05/HBase基础开发/</id>
    <published>2018-05-05T08:45:31.000Z</published>
    <updated>2018-05-06T09:08:34.537Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了如何利用eclipse进行HBase相关的开发。</p><a id="more"></a><h2 id="模拟场景"><a href="#模拟场景" class="headerlink" title="模拟场景"></a>模拟场景</h2><ul><li>运营商通话记录<br>查询通话详单<br>本机号码 主叫/被叫 通话时长 时间 对方号码 号码属性 归属地</li></ul><h2 id="第1次启动分布式hadoop和hbase"><a href="#第1次启动分布式hadoop和hbase" class="headerlink" title="第1次启动分布式hadoop和hbase"></a>第1次启动分布式hadoop和hbase</h2><ul><li>删除所有的临时文件夹(重要)</li><li>启动zk集群，三台，最好在xshell下面一起启动，并且查看状态status</li><li>edits交给JN,手动启动三个JN</li><li>格式化一台NN，并启动这一台(只能格式化一次，不然重新来)</li><li>另外一台NN执行同步standby</li><li>格式化zk</li><li>启动dfs(可以直接全部启动)</li><li>最后启动RS<pre><code>rm -rdf /opt/hadoop/* /opt/journal/* /opt/zookeeper/v* /opt/zookeeper/z*zkServer.sh start(xshell同时启动)vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode)hadoop-daemon.sh start journalnode(根据hdfs配置文件，n2 n3 n4)hdfs namenode -format(选一个NN格式化，一次机会，失败重头再来)hadoop-daemon.sh start namenode(格式化的NN上)hdfs namenode -bootstrapStandby(n2,没有格式化的NN上)hdfs zkfc -formatZK(n1)start-all.sh(n1)yarn-daemon.sh start resourcemanager(指定的机器，n3 n4)start-hbase.sh</code></pre></li></ul><h2 id="第2次及以后启动分布式hadoop和hbase"><a href="#第2次及以后启动分布式hadoop和hbase" class="headerlink" title="第2次及以后启动分布式hadoop和hbase"></a>第2次及以后启动分布式hadoop和hbase</h2><ul><li>启动过程<pre><code>zkServer.sh start(xshell同时启动)vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode)start-all.sh(n1)yarn-daemon.sh start resourcemanager(指定的机器，n3 n4)start-hbase.sh</code></pre></li></ul><h2 id="eclipse连接hadoop"><a href="#eclipse连接hadoop" class="headerlink" title="eclipse连接hadoop"></a>eclipse连接hadoop</h2><ul><li>连接过程具体<a href="https://leebin.top/2018/04/30/%E6%9C%AC%E5%9C%B0eclipse%E9%93%BE%E6%8E%A5%E8%BF%9C%E7%A8%8Bhadoop%E7%BC%96%E5%86%99hdfs%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81/" target="_blank" rel="noopener">参考</a></li></ul><h2 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h2><ul><li>新建项目</li><li>到入hbase安装包下面的lib文件夹到项目目录下</li><li><p>写代码创建表</p><pre><code>package com.hikvision.hbase;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.log4j.BasicConfigurator;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HBase_test {  HBaseAdmin hAdmin;  String TABLE_NAME = &quot;phone&quot;;  // 列族 一般是1到2个  String COLUMN_FAMILY = &quot;cf1&quot;;  @SuppressWarnings(&quot;deprecation&quot;)  @Before  public void begin() throws Exception {      BasicConfigurator.configure();      Configuration conf = new Configuration();      conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;n1,n2,n3&quot;);      conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);      conf.set(&quot;hbase.master&quot;, &quot;n5:16020&quot;);      hAdmin = new HBaseAdmin(conf);  }  @After  public void end() {      if (hAdmin != null) {          try {              hAdmin.close();          } catch (IOException e) {              e.printStackTrace();          }      }  }  @Test  public void createTable() throws Exception {      // 0.容错,先判断是不是存在      if (hAdmin.tableExists(TABLE_NAME)) {          hAdmin.disableTable(TABLE_NAME);          hAdmin.deleteTable(TABLE_NAME);      }      // 1.表描述      HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(TABLE_NAME));      // 2.列族描述      HColumnDescriptor family = new HColumnDescriptor(COLUMN_FAMILY);      // 2.1读缓存      family.setBlockCacheEnabled(true);      family.setInMemory(true);      // 2.2设定最大版本数默认为1      family.setMaxVersions(1);      // 为表指定列族      desc.addFamily(family);      hAdmin.createTable(desc);  }}</code></pre></li><li><p>查看hbase shell</p><pre><code>root@n5:~# hbase shellSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/root/app/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/root/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017hbase(main):001:0&gt; listTABLE                                                                                                                                                                                                            phone                                                                                                                                                                                                            t3                                                                                                                                                                                                               tab-1                                                                                                                                                                                                            3 row(s) in 0.2560 seconds=&gt; [&quot;phone&quot;, &quot;t3&quot;, &quot;tab-1&quot;]hbase(main):002:0&gt; describe &quot;phone&quot;Table phone is ENABLED                                                                                                                                                                                           phone                                                                                                                                                                                                            COLUMN FAMILIES DESCRIPTION                                                                                                                                                                                      {NAME =&gt; &#39;cf1&#39;, BLOOMFILTER =&gt; &#39;ROW&#39;, VERSIONS =&gt; &#39;1&#39;, IN_MEMORY =&gt; &#39;true&#39;, KEEP_DELETED_CELLS =&gt; &#39;FALSE&#39;, DATA_BLOCK_ENCODING =&gt; &#39;NONE&#39;, TTL =&gt; &#39;FOREVER&#39;, COMPRESSION =&gt; &#39;NONE&#39;, MIN_VERSIONS =&gt; &#39;0&#39;, BLOCKCACHE =&gt; &#39;true&#39;, BLOCKSIZE =&gt; &#39;65536&#39;, REPLICATION_SCOPE =&gt; &#39;0&#39;}                                                                                                                                                     1 row(s) in 0.1350 seconds</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍了如何利用eclipse进行HBase相关的开发。&lt;/p&gt;
    
    </summary>
    
      <category term="开发" scheme="http://yoursite.com/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="HBase" scheme="http://yoursite.com/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HBase原理搭建高可用与基本使用</title>
    <link href="http://yoursite.com/2018/05/03/HBase%E5%8E%9F%E7%90%86%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B8%8E%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/05/03/HBase原理搭建高可用与基本使用/</id>
    <published>2018-05-03T13:34:47.000Z</published>
    <updated>2018-05-05T08:10:13.871Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何安装并使用HBase。</p><a id="more"></a><h2 id="伪分布式的HBase环境搭建Pseudo-Distributed-Local-Install-参考"><a href="#伪分布式的HBase环境搭建Pseudo-Distributed-Local-Install-参考" class="headerlink" title="伪分布式的HBase环境搭建Pseudo-Distributed Local Install 参考"></a>伪分布式的HBase环境搭建Pseudo-Distributed Local Install <a href="https://hbase.apache.org/book.html#standalone_dist" target="_blank" rel="noopener">参考</a></h2><ul><li><p>配置</p><pre><code>&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="完全分布式安装"><a href="#完全分布式安装" class="headerlink" title="完全分布式安装"></a>完全分布式安装</h2></li><li><p>架构</p></li></ul><table><thead><tr><th>VM</th><th>NN</th><th>DN</th><th>JN</th><th>ZK</th><th>ZKFC</th><th>RS</th><th>HM</th><th>HRS</th></tr></thead><tbody><tr><td>n1</td><td>y</td><td>n</td><td>n</td><td>y</td><td>y</td><td>n</td><td>y(b)</td><td>n</td></tr><tr><td>n2</td><td>y</td><td>y</td><td>y</td><td>y</td><td>y</td><td>n</td><td>n</td><td>y</td></tr><tr><td>n3</td><td>n</td><td>y</td><td>y</td><td>y</td><td>n</td><td>y</td><td>n</td><td>y</td></tr><tr><td>n4</td><td>n</td><td>y</td><td>y</td><td>n</td><td>n</td><td>y</td><td>n</td><td>y</td></tr><tr><td>n5</td><td>n</td><td>y</td><td>n</td><td>n</td><td>n</td><td>n</td><td>y(m)</td><td>n</td></tr><tr><td>n6</td><td>n</td><td>y</td><td>n</td><td>n</td><td>n</td><td>n</td><td>n</td><td>n</td></tr></tbody></table><ul><li><p>hbase-env.sh</p><pre><code>export JAVA_HOME=/root/app/jdkexport HBASE_MANAGES_ZK=false (不启动自带的zookeeper)</code></pre></li><li><p>hbase-site.xml</p><pre><code>&lt;configuration&gt;&lt;property&gt;  &lt;name&gt;hbase.rootdir&lt;/name&gt;  &lt;value&gt;hdfs://sxt:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;  &lt;value&gt;n1,n2,n3&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>conf/regionservers</p><pre><code>#根据架构把n5设定为HMaster，下面的为HReginServern2n3n4</code></pre></li><li><p>conf/backup-masters file(master的HA)</p><pre><code>vim backup-mastersn1#根据架构将n1设定为HMaster从节点</code></pre></li><li><p>HDFS Client Configuration(hbase访问hdfs)</p><pre><code>copy hdfs-site.xml ${HBASE_HOME}/confroot@n1:~/app/hbase/conf# pwd/root/app/hbase/confcp /root/app/hadoop/etc/hadoop/hdfs-site.xml ./</code></pre></li><li><p>启动</p><pre><code>#-----------------------------------在n5 HMaster主节点启动root@n5:~/app/hbase/conf# jps1058 NodeManager962 DataNode2706 HMaster3083 Jps#-----------------------------------在n2,n3,n4查看HRegionServerroot@n4:~# jps1041 DataNode949 JournalNode2618 Jps2396 HRegionServer1198 NodeManager1263 ResourceManager</code></pre></li><li><p>webUI查看</p><pre><code>主节点 http://n5:16010/master-status从节点 http://n1:16010/master-statusroot@n5:~/app/hbase/conf# netstat -ntlp | grep &quot;java&quot;tcp        0      0 0.0.0.0:50010           0.0.0.0:*               LISTEN      962/java        tcp        0      0 0.0.0.0:50075           0.0.0.0:*               LISTEN      962/java        tcp        0      0 0.0.0.0:50020           0.0.0.0:*               LISTEN      962/java        tcp        0      0 127.0.0.1:42933         0.0.0.0:*               LISTEN      962/java        tcp6       0      0 :::13562                :::*                    LISTEN      1058/java       tcp6       0      0 192.168.44.105:16000    :::*                    LISTEN      2706/java       tcp6       0      0 :::8040                 :::*                    LISTEN      1058/java       tcp6       0      0 :::16010                :::*                    LISTEN      2706/java       tcp6       0      0 :::8042                 :::*                    LISTEN      1058/java       tcp6       0      0 :::34187                :::*                    LISTEN      1058/java </code></pre></li></ul><h2 id="常见的命令"><a href="#常见的命令" class="headerlink" title="常见的命令"></a>常见的命令</h2><ul><li><p>增删改查</p><pre><code>hbase shellhelp#-----------------------------------status table_help version whoamiddl（disable drop enable exist is_disableed is_enabled list show_filters）namespace（alter_namespace ...）dml（append put get delete scan）tools（balancer flush:menorystore2storefile）replicationsnapshots#-----------------------------------createcreate &#39;person&#39;,&#39;cf1&#39;,&#39;cf2&#39;#-----------------------------------lsitdesc &#39;person&#39;TTL =&gt; &#39;forever&#39;#-----------------------------------putput &#39;person&#39;,&#39;r1&#39;,&#39;c1&#39;,&#39;value&#39;put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;,&#39;xiaoming&#39;put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:sex&#39;,&#39;boy&#39;#-----------------------------------scanscan &#39;person&#39;#-----------------------------------getget &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;#-----------------------------------插入数据put &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;,&#39;xiaoming2&#39;get &#39;person&#39;,&#39;0001&#39;,&#39;cf1:name&#39;#-----------------------------------#查看命名空间list_namespacedefaulthbase#-----------------------------------#先禁用再删除create &#39;tbl&#39;,&#39;cf&#39;listdisabledisabled &#39;tbl&#39;listdrop &#39;tbl&#39;</code></pre></li><li><p>查看hbase的文件<br>单机模式文件在Linux上<br>伪分布式模式在Linux上<br>分布式模式在hdfs上</p><pre><code>D:\HBASE目录结构└─namespace  ├─主节点HRegionMaster  └─域节点HRegionServer      └─表table          ├─域HRegion          │  ├─存储Store          │  │  ├─内存存储memoryStore          │  │  ├─内存存储memoryStore1          │  │  ├─磁盘储存storeFile          │  │  └─磁盘储存storeFile1          │  └─存储Store1          │      ├─内存存储memoryStore          │      └─内存存储memoryStore1          └─域HRegion1              ├─存储Store              │  ├─内存存储memoryStore              │  └─内存存储memoryStore1              └─存储Store1                  ├─内存存储memoryStore                  └─内存存储memoryStore1</code></pre></li><li><p>webUI查看 n1:60010</p></li></ul><h2 id="HBase-的ha验证"><a href="#HBase-的ha验证" class="headerlink" title="HBase 的ha验证"></a>HBase 的ha验证</h2><ul><li><p>创建表</p><pre><code>#--------------------------------------------create &#39;tab-1&#39; ,&#39;cf-1&#39;put &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:name&#39;,&#39;xiaoli&#39;#--------------------------------------------hbase(main):007:0&gt; scan &#39;tab-1&#39;ROW                        COLUMN+CELL                                                                 rk-0001                   column=cf-1:name, timestamp=1525506703143, value=xiaoli                     1 row(s) in 0.2510 seconds#--------------------------------------------hbase(main):008:0&gt; get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:name&#39;COLUMN                     CELL                                                                        cf-1:name                 timestamp=1525506703143, value=xiaoli                                       1 row(s) in 0.0480 seconds#--------------------------------------------put &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1:age&#39;,&#39;18&#39;#--------------------------------------------hbase(main):010:0&gt; scan &#39;tab-1&#39;ROW                        COLUMN+CELL                                                                 rk-0001                   column=cf-1:age, timestamp=1525507159178, value=18                          rk-0001                   column=cf-1:name, timestamp=1525506703143, value=xiaoli                     1 row(s) in 0.0320 seconds#--------------------------------------------get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1&#39;#--------------------------------------------hbase(main):011:0&gt; get &#39;tab-1&#39;,&#39;rk-0001&#39;,&#39;cf-1&#39;COLUMN                     CELL                                                                        cf-1:age                  timestamp=1525507159178, value=18                                           cf-1:name                 timestamp=1525506703143, value=xiaoli                                       2 row(s) in 0.0200 seconds</code></pre></li><li><p>验证</p><pre><code>#--------------------------------------------root@n5:~# jps1058 NodeManager962 DataNode5444 Jps3625 HMaster#--------------------------------------------kill -9 3625查看n5:16010查看n1:16010查看hbase shellscan &#39;tab-1&#39;#--------------------------------------------重新启动n5上的master#--------------------------------------------</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何安装并使用HBase。&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="HBase" scheme="http://yoursite.com/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MR实例温度</title>
    <link href="http://yoursite.com/2018/05/01/MR%E5%AE%9E%E4%BE%8B%E6%B8%A9%E5%BA%A6/"/>
    <id>http://yoursite.com/2018/05/01/MR实例温度/</id>
    <published>2018-05-01T14:52:09.000Z</published>
    <updated>2018-05-02T07:41:29.961Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何分析具体的MR实例。</p><a id="more"></a><h2 id="数据与需求"><a href="#数据与需求" class="headerlink" title="数据与需求"></a>数据与需求</h2><ul><li><p>数据</p><pre><code>  1949-10-01 14:21:02 34c  1949-10-02 14:01:02 36c  1950-01-01 11:21:02 32c  1950-10-01 12:21:02 37c  1951-12-01 12:21:02 23c  1950-10-02 12:21:02 41c  1950-10-03 12:21:02 27c  1951-07-01 12:21:02 45c  1951-07-02 12:21:02 46c  1951-07-03 12:21:03 47c</code></pre></li><li><p>需求</p><pre><code>  输出：得出每个年月下，温度最高的前两天  年月：升序  温度：降序</code></pre></li><li><p>技术点分析</p><pre><code>  sort需要进行二次排序，需要从写sort方法  reduce个数设定为3个，可能有%3操作，进行负载均衡  全部的MR模型中包含了map,reduce,part,sort,group,combine</code></pre></li></ul><h2 id="写代码"><a href="#写代码" class="headerlink" title="写代码"></a>写代码</h2><ul><li>代码涉及到了MapReduce的整个生命周期</li><li><p>每个周期的代码如下</p><blockquote><p>TQJob  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.FileSystem;  import org.apache.hadoop.fs.Path;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.mapreduce.Job;  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  public class TQJob {      public static void main(String[] args) throws Exception {          // 默认加载根目录src目录下的配置文件          Configuration conf = new Configuration();          // 运行方式1：手动指定提交服务器的地址          // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);          // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);          // 运行方式2：使用配置文件指定，屏蔽上面的手动指定,报错的需要指定跨平台，指定本地jar的位置          conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);          conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\tq.jar&quot;);          // 运行方式3：手动上传到服务器上，屏蔽上的面的conf指定          // System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);          // 开始job相关的设定          Job job = Job.getInstance(conf);          // 设定MR主类          job.setJarByClass(TQJob.class);          // 设定mapper          job.setMapperClass(TQMapper.class);          job.setOutputKeyClass(Weather.class);          job.setMapOutputValueClass(IntWritable.class);          // 设定reducer          job.setReducerClass(TQReducer.class);          job.setPartitionerClass(TQPartition.class);          job.setSortComparatorClass(TQSort.class);          job.setGroupingComparatorClass(TQGroup.class);          job.setNumReduceTasks(3);          // 要分析的文件          FileInputFormat.addInputPath(job, new Path(&quot;/weather/input/tq.txt&quot;));          // 输出路径          Path outPath = new Path(&quot;/weather/output&quot;);          FileSystem fs = FileSystem.get(conf);          if (fs.exists(outPath)) {              fs.delete(outPath, true);          }          FileOutputFormat.setOutputPath(job, outPath);          // 提交job作业          boolean flag = job.waitForCompletion(true);          if (flag) {              System.out.println(&quot;job commit successfully...&quot;);          }      }  }</code></pre><blockquote><p>TQMapper  </p></blockquote><pre><code>  package com.hikvision.tq;  import java.io.IOException;  import java.text.ParseException;  import java.text.SimpleDateFormat;  import java.util.Calendar;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.LongWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Mapper;  import org.apache.hadoop.util.StringUtils;  public class TQMapper extends Mapper&lt;LongWritable, Text, Weather, IntWritable&gt; {      @Override      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String[] strs = StringUtils.split(value.toString(), &#39;\t&#39;);          SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);          Calendar cal = Calendar.getInstance();          try {              cal.setTime(sdf.parse(strs[0]));              Weather weather = new Weather();              weather.setYear(cal.get(Calendar.YEAR));              weather.setMonth(cal.get(Calendar.MONTH) + 1);              weather.setDay(cal.get(Calendar.DAY_OF_MONTH));              int temperature = Integer.parseInt(strs[1].substring(0, strs[1].lastIndexOf(&#39;c&#39;)));              weather.setTemperature(temperature);              context.write(weather, new IntWritable(temperature));          } catch (ParseException e) {              e.printStackTrace();          }      }  }</code></pre><blockquote><p>TQPartition  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;  public class TQPartition extends HashPartitioner&lt;Weather, IntWritable&gt; {      @Override      public int getPartition(Weather key, IntWritable value, int numReduceTasks) {          // 重写partition的过程，需要满足业务的规则          // 越简单越好，会影响计算速度          return (key.getYear() - 1949) % numReduceTasks;          // return super.getPartition(key, value, numReduceTasks);      }  }</code></pre><blockquote><p>TQSort  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.io.WritableComparable;  import org.apache.hadoop.io.WritableComparator;  public class TQSort extends WritableComparator {      // 重写构造方法      public TQSort() {          super(Weather.class, true);      }      @SuppressWarnings(&quot;rawtypes&quot;)      @Override      public int compare(WritableComparable a, WritableComparable b) {          Weather w1 = (Weather) a;          Weather w2 = (Weather) b;          int c1 = Integer.compare(w1.getYear(), w2.getYear());          if (c1 == 0) {              int c2 = Integer.compare(w1.getMonth(), w2.getMonth());              if (c2 == 0) {                  return -Integer.compare(w1.getTemperature(), w2.getTemperature());              }              return c2;          }          return c1;      }  }</code></pre><blockquote><p>TQGroup  </p></blockquote><pre><code>  package com.hikvision.tq;  import org.apache.hadoop.io.WritableComparable;  import org.apache.hadoop.io.WritableComparator;  public class TQGroup extends WritableComparator {      public TQGroup() {          super(Weather.class, true);      }      @SuppressWarnings(&quot;rawtypes&quot;)      @Override      public int compare(WritableComparable a, WritableComparable b) {          Weather w1 = (Weather) a;          Weather w2 = (Weather) b;          int c1 = Integer.compare(w1.getYear(), w2.getYear());          if (c1 == 0) {              return Integer.compare(w1.getMonth(), w2.getMonth());          }          return c1;      }  }</code></pre><blockquote><p>TQReducer  </p></blockquote><pre><code>  package com.hikvision.tq;  import java.io.IOException;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.NullWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Reducer;  public class TQReducer extends Reducer&lt;Weather, IntWritable, Text, NullWritable&gt; {      @Override      protected void reduce(Weather weather, Iterable&lt;IntWritable&gt; iterable, Context context) throws IOException, InterruptedException {          int flag = 0;          for (IntWritable i : iterable) {              flag++;              if (flag &gt; 2) {                  break;              }              String msg = weather.getYear() + &quot;-&quot; + weather.getMonth() + &quot;-&quot; + weather.getDay() + &quot;-&quot; + i.get();              context.write(new Text(msg), NullWritable.get());          }      }  }</code></pre></li></ul><h2 id="提交job操作"><a href="#提交job操作" class="headerlink" title="提交job操作"></a>提交job操作</h2><ul><li>新建文件夹，上传文件到hdfs</li><li>已经有了配置文件所以，使用方式2进行提交，需要手动打包，直接运行<pre><code>  // 运行方式2：使用配置文件指定，屏蔽上面的手动指定,报错的需要指定跨平台，指定本地jar的位置  conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\tq.jar&quot;);</code></pre></li></ul><h2 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h2><ul><li><p>控制台日志</p><pre><code>  2018-05-02 15:22:31,473 WARN  mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.  2018-05-02 15:22:34,937 INFO  input.FileInputFormat (FileInputFormat.java:listStatus(283)) - Total input paths to process : 1  2018-05-02 15:22:35,103 INFO  mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:1  2018-05-02 15:22:35,119 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - mapred.jar is deprecated. Instead, use mapreduce.job.jar  2018-05-02 15:22:35,237 INFO  mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1525168992297_0007  2018-05-02 15:22:35,674 INFO  impl.YarnClientImpl (YarnClientImpl.java:submitApplication(273)) - Submitted application application_1525168992297_0007  2018-05-02 15:22:35,825 INFO  mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://n3:8088/proxy/application_1525168992297_0007/  2018-05-02 15:22:35,826 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1525168992297_0007  2018-05-02 15:22:48,438 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1525168992297_0007 running in uber mode : false  2018-05-02 15:22:48,439 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%  2018-05-02 15:22:58,537 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%  2018-05-02 15:23:14,820 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 33%  2018-05-02 15:23:15,837 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 100%  2018-05-02 15:23:16,853 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_1525168992297_0007 completed successfully  2018-05-02 15:23:17,092 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 50      File System Counters          FILE: Number of bytes read=238          FILE: Number of bytes written=501967          FILE: Number of read operations=0          FILE: Number of large read operations=0          FILE: Number of write operations=0          HDFS: Number of bytes read=347          HDFS: Number of bytes written=101          HDFS: Number of read operations=12          HDFS: Number of large read operations=0          HDFS: Number of write operations=6      Job Counters           Killed reduce tasks=1          Launched map tasks=1          Launched reduce tasks=3          Data-local map tasks=1          Total time spent by all maps in occupied slots (ms)=7783          Total time spent by all reduces in occupied slots (ms)=43298          Total time spent by all map tasks (ms)=7783          Total time spent by all reduce tasks (ms)=43298          Total vcore-milliseconds taken by all map tasks=7783          Total vcore-milliseconds taken by all reduce tasks=43298          Total megabyte-milliseconds taken by all map tasks=7969792          Total megabyte-milliseconds taken by all reduce tasks=44337152      Map-Reduce Framework          Map input records=10          Map output records=10          Map output bytes=200          Map output materialized bytes=238          Input split bytes=96          Combine input records=0          Combine output records=0          Reduce input groups=5          Reduce shuffle bytes=238          Reduce input records=10          Reduce output records=8          Spilled Records=20          Shuffled Maps =3          Failed Shuffles=0          Merged Map outputs=3          GC time elapsed (ms)=498          CPU time spent (ms)=8400          Physical memory (bytes) snapshot=570830848          Virtual memory (bytes) snapshot=7779991552          Total committed heap usage (bytes)=175120384      Shuffle Errors          BAD_ID=0          CONNECTION=0          IO_ERROR=0          WRONG_LENGTH=0          WRONG_MAP=0          WRONG_REDUCE=0      File Input Format Counters           Bytes Read=251      File Output Format Counters           Bytes Written=101  job commit successfully...</code></pre></li><li><p>webUI和输出的目录</p><blockquote><p>输出目录下分为三个文件,和设定的三个reducer有关  </p><pre><code>  1949-10-2-36  1949-10-1-34  #  1950-1-1-32  1950-10-2-41  1950-10-1-37  #  1951-7-3-47  1951-7-2-46  1951-12-1-23</code></pre><p>查看webUI <a href="http://n3:8088/cluster" target="_blank" rel="noopener">http://n3:8088/cluster</a> 显示已经执行成功</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何分析具体的MR实例。&lt;/p&gt;
    
    </summary>
    
      <category term="开发" scheme="http://yoursite.com/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="mapreduce" scheme="http://yoursite.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>工欲善其事必先利其器(持续更新中)</title>
    <link href="http://yoursite.com/2018/05/01/%E5%B7%A5%E6%AC%B2%E5%96%84%E5%85%B6%E4%BA%8B%E5%BF%85%E5%85%88%E5%88%A9%E5%85%B6%E5%99%A8/"/>
    <id>http://yoursite.com/2018/05/01/工欲善其事必先利其器/</id>
    <published>2018-05-01T03:33:22.000Z</published>
    <updated>2018-05-03T14:55:04.063Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍所有的优化配置过程，设计到操作系统，开发工具，常用的小工具等。</p><a id="more"></a><h2 id="win10-系统优化"><a href="#win10-系统优化" class="headerlink" title="win10 系统优化"></a>win10 系统优化</h2><ul><li><p>安装win10:</p><blockquote><p>搜索关键词：msdn我告诉你，下载原生的win10系统<br>下载魔方，用魔方组件将ISO文件写入到U盘<br>重启，开机即可安装win10<br>用户名可以设置为root，很多开发场景中需要windows的用户名为root  </p></blockquote></li><li><p>安装驱动：</p><blockquote><p>360官网 &gt; 驱动大师<br>安装驱动大师 &gt; 安装驱动</p></blockquote></li><li><p>安装office 2016</p><blockquote><p>搜索关键词：msdn我告诉你，下载office2016<br>安装  </p></blockquote></li><li><p>激活win10 office2016</p><blockquote><p>小马激活OEM10.exe激活win10<br>KMSpico_setup.exe激活office 2016  </p></blockquote></li><li><p>关闭/打开windows BISO启动入口</p><blockquote><p>控制面板\硬件和声音\电源选项\系统设置<br>更改不可用设置 &gt; 启用快速启动  </p></blockquote></li><li><p>关闭掉windows update</p><blockquote><p>在中文输入法下 &gt; 按win键 &gt; 输入服务<br>windows update &gt; 停止 &gt; 禁止启动  </p></blockquote></li><li><p>关闭掉windows defender</p><blockquote><p>设置 &gt; 更新和安全 &gt; Windows Defender &gt; 关闭实时保护  </p></blockquote></li><li><p>关闭windows 防火墙</p><blockquote><p>计算机右键属性 &gt; windows防火墙 &gt; 启用和关闭windows防火墙  </p></blockquote></li><li><p>备份系统</p><blockquote><p><a href="http://www.uqidong.com/" target="_blank" rel="noopener">http://www.uqidong.com/</a> 下载U启动工具，制作U盘启动<br>查找电脑主板 &gt; 查找进入bios的方法 &gt; 重启按键 &gt; 从U盘启动<br>选择备份系统 &gt; 备份C盘  </p></blockquote></li></ul><h2 id="windows-常用的工具优化"><a href="#windows-常用的工具优化" class="headerlink" title="windows 常用的工具优化"></a>windows 常用的工具优化</h2><ul><li><p>everything+wox 实现mac的sportlight搜索框功能</p></li><li><p>autohotkey实现workflow功能</p><blockquote><p>ahk常用的快捷键  </p></blockquote></li></ul><table><thead><tr><th>ahk</th><th>win</th></tr></thead><tbody><tr><td>^</td><td>Ctrl键</td></tr><tr><td>+</td><td>Shift键</td></tr><tr><td>!</td><td>Alt键</td></tr><tr><td>井</td><td>Win键</td></tr><tr><td>Up</td><td>上箭头键</td></tr><tr><td>Down</td><td>下箭头键</td></tr><tr><td>Left</td><td>左箭头键</td></tr><tr><td>Right</td><td>右箭头键</td></tr><tr><td>PgUp</td><td>PageUp键</td></tr><tr><td>PgDn</td><td>PageDn键</td></tr><tr><td>F1-F12</td><td>功能键</td></tr><tr><td>a-z</td><td>a-z键</td></tr><tr><td>LButton</td><td>鼠标左键</td></tr><tr><td>RButton</td><td>鼠标右键</td></tr><tr><td>MButton</td><td>鼠标中键</td></tr><tr><td>WheelUp</td><td>鼠标滑轮向上</td></tr><tr><td>WheelDown</td><td>鼠标滑轮向下</td></tr><tr><td>Del</td><td>Del删除</td></tr><tr><td>Enter</td><td>Enter回车</td></tr><tr><td>Tab</td><td>Table制表符</td></tr><tr><td>Space</td><td>Space空格</td></tr></tbody></table><h2 id="eclipse优化"><a href="#eclipse优化" class="headerlink" title="eclipse优化"></a>eclipse优化</h2><ul><li>eclipse快捷键 <a href="https://www.cnblogs.com/zhangqie/p/6432477.html" target="_blank" rel="noopener">参考</a></li></ul><table><thead><tr><th>key</th><th>meaning</th></tr></thead><tbody><tr><td>ctrl+o</td><td>打开类</td></tr><tr><td>alt+shift+s v</td><td>重写方法</td></tr><tr><td>alt+shift+s</td><td>source菜单</td></tr><tr><td>alt+shift+l</td><td>补全变量名</td></tr><tr><td>ctrl+2+l</td><td>补全变量名</td></tr></tbody></table><ul><li><p>依赖包的导入方式</p><blockquote><p>手动创建lib，然后导入<br>添加用户依赖  </p></blockquote></li><li><p>设置代码显示最大宽度</p><blockquote><p>Window &gt; Preferences &gt; Java &gt; Code Style &gt; Formatter &gt; new Profile<br>Line Wrapping &gt; Maximum line width  </p></blockquote></li><li><p>eclipse插件</p><ol><li><p>maven插件M2Eclipse</p><blockquote><p>找到maven链接 <a href="http://www.eclipse.org/m2e/" target="_blank" rel="noopener">http://www.eclipse.org/m2e/</a><br>软件 &gt; 安装其他软件  </p></blockquote></li><li><p>hadoop插件</p><blockquote><p>连接远程hadoop集群，需要本地的windows用户为root用户  </p></blockquote></li></ol></li></ul><h2 id="ubuntu设置"><a href="#ubuntu设置" class="headerlink" title="ubuntu设置"></a>ubuntu设置</h2><ul><li>启动用户名和密码的登录界面<pre><code>  sudo passwd -u root  sudo passwd root  su root  cd /usr/share/lightdm/lightdm.conf.d/  vim 50-unity-greeter.conf  # 添加  user-session=ubuntu  greeter-show-manual-login=true  all-guest=false  # 重启  reboot  # 使用user和passwd进入root报错  vim /root/.profile  # 找到mesg n || true  # 改为tty -s &amp;&amp; mesg n || true</code></pre></li><li><p>更换国内的源(fq除外)</p><pre><code>  sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup  vi /etc/apt/sources.list  #Ubuntu 官方源   deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse  deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse  deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse  #网易163  deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse  deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse  deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse  sudo apt-get update  sudo apt-get dist-upgrade</code></pre></li><li><p>desktop桌面版本vm下的ubuntu更改为固定的IP</p><blockquote><p>查看vm的IP地址 &gt; 网段和DNS<br>编辑ubutnu的网卡配置 &gt; 设置在合理的IP范围<br>重启网卡  </p></blockquote></li><li><p>server服务器版本vm下的ubuntu更改为固定的IP</p><pre><code>  vmware &gt; 编辑 &gt; 虚拟网络编辑器 &gt; 点击vmnet8 &gt; NET设置 &gt; 查看由vmnet8设定的子网先关参数 &gt; 也可以自己更改子网的IP和网段  配置固定IP三个步骤  首先ifconfig查看自己ubuntu工作网卡名称，替换下面的eth0  #----------------------------------------  /etc/network/interfaces  原来为：  auto lo  iface lo inet loopback  auto eth0  iface eth0 inet dhcp  #----------------------------------------  改为：  auto lo  iface lo inet loopback  auto eth0  iface eth0 inet static      #定义为静态IP  address 192.168.2.29        #所要设置的IP地址  netmask 255.255.255.0       #子网掩码  gateway 192.168.2.1         #网关（路由地址）  #----------------------------------------  手动设动网关和DNS    /etc/resolv.conf  nameserver 192.168.2.1      #网关（同上）  #nameserver 202.106.0.20    #DNS服务器地址（参考其他电脑，VM上的ubuntu可以不用设定）  #----------------------------------------  永久性更改网关和DNS    /etc/resolvconf/resolv.conf.d/base  nameserver 192.168.2.1      #网关  #nameserver 202.106.0.20    #DNS  #----------------------------------------  /etc/init.d/networking restart  reboot</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍所有的优化配置过程，设计到操作系统，开发工具，常用的小工具等。&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="工具" scheme="http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>hadoop开发代码三种提交方式</title>
    <link href="http://yoursite.com/2018/05/01/hadoop%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/"/>
    <id>http://yoursite.com/2018/05/01/hadoop开发代码三种提交方式/</id>
    <published>2018-05-01T02:37:20.000Z</published>
    <updated>2018-05-01T13:16:37.981Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何在本地编写wordcount程序，在远程的hadoop集群上运行测试代码，重点介绍了hadoop本地开发后的三种提交到服务器的方式，分别涉及到了项目开发，项目测试，项目交付时期的不同提交方式。</p><a id="more"></a><h2 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h2><ul><li>新建java项目</li><li><p>添加依赖</p><blockquote><p>maven：下载设置maven，新建maven项目，在maven repository找到坐标<br>手动添加依赖：用户依赖，直接把hadoop/share中lib包逐个引用过来<br>手动添加依赖：lib依赖，将所有的jar放在一个目录下  </p></blockquote></li><li><p>关联源代码：ctrl+单击打开api，管理源码，打开解压后的源码文件夹</p><blockquote><p>JDK基本的源码在JDK安装路径中src.zip<br>hadoop源码在官网下载解压后关联文件夹到eclipse</p></blockquote></li><li><p>代码分为三个部分：主程序设置job任务，mapper，reducer</p></li></ul><ol><li>main<pre><code>package com.hikvision.wc;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WCJob { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {     // 默认加载根目录src目录下的配置文件     Configuration conf = new Configuration();     conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);     conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);     Job job = Job.getInstance(conf);     // 设定MR主类     job.setJarByClass(WCJob.class);     // 设定mapper     job.setMapperClass(WCMapper.class);     job.setOutputKeyClass(Text.class);     job.setMapOutputValueClass(IntWritable.class);     // 设定reducer     job.setReducerClass(WCReducer.class);     // 要分析的文件     FileInputFormat.addInputPath(job, new Path(&quot;/wc/input/data.txt&quot;));     // 输出路径     Path outPath = new Path(&quot;/wc/output&quot;);     FileSystem fs = FileSystem.get(conf);     if (fs.exists(outPath)) {         fs.delete(outPath, true);     }     FileOutputFormat.setOutputPath(job, outPath);     // 提交job作业     boolean flag = job.waitForCompletion(true);     if (flag) {         System.out.println(&quot;job commit successfully...&quot;);     } }}</code></pre></li><li>mapper<pre><code>package com.hikvision.wc;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.util.StringUtils;public class WCMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {     String str = value.toString();     String[] strs = StringUtils.split(str, &#39; &#39;);     for (String s : strs) {         context.write(new Text(s), new IntWritable(1));     } }}</code></pre></li><li>reducer<pre><code>package com.hikvision.wc;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { @Override protected void reduce(Text text, Iterable&lt;IntWritable&gt; iterable, Context context) throws IOException, InterruptedException {     int sum = 0;     for (IntWritable i : iterable) {         sum += i.get();     }     context.write(text, new IntWritable(sum)); }}</code></pre></li></ol><h2 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h2><ul><li>测试数据放在src下</li><li>三种运行方式<blockquote><p>本地测试环境（企业开发过程中测试）<br>提交到服务器（测试代码的执行能力）<br>自动打包<br>手动打包  </p></blockquote></li></ul><h2 id="提交1：不打包本地提交到远程服务器直接执行（功能开发过程）"><a href="#提交1：不打包本地提交到远程服务器直接执行（功能开发过程）" class="headerlink" title="提交1：不打包本地提交到远程服务器直接执行（功能开发过程）"></a>提交1：不打包本地提交到远程服务器直接执行（功能开发过程）</h2><ul><li>没有配置文件+必要的conf设置</li><li><p>本地eclipse提交代码</p><blockquote><p>解压hadoop，配置环境变量<br>安装windows中的hadoop依赖工具win-utils的%HADOOP_HOME%\bin<br>拷贝hadoop源码到自己的工程中，检查包是不是报错<br>配置本地hosts文件<br>启动zkServer.sh start<br>启动start-all.sh(dfs yarn)<br>启动resource manager(RS)<br>测试50070，找到active的dfs节点<br>测试8088，找到active的resourcemanager节点<br>配置本地的hadoop连接<br>创建输入文件夹，上传数据文件<br>配置conf提交地址<br>在main中填写输入输出路径<br>直接以java application运行代码  </p></blockquote></li><li><p>实验结果</p><pre><code>2018-05-01 18:11:01,551 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - session.id is deprecated. Instead, use dfs.metrics.session-id2018-05-01 18:11:01,559 INFO  jvm.JvmMetrics (JvmMetrics.java:init(76)) - Initializing JVM Metrics with processName=JobTracker, sessionId=2018-05-01 18:11:02,328 WARN  mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.2018-05-01 18:11:02,401 WARN  mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(171)) - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).2018-05-01 18:11:02,466 INFO  input.FileInputFormat (FileInputFormat.java:listStatus(283)) - Total input paths to process : 12018-05-01 18:11:02,601 INFO  mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:12018-05-01 18:11:02,776 INFO  mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_local2042689909_00012018-05-01 18:11:03,239 INFO  mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://localhost:8080/2018-05-01 18:11:03,242 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_local2042689909_00012018-05-01 18:11:03,251 INFO  mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(471)) - OutputCommitter set in config null2018-05-01 18:11:03,260 INFO  output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 12018-05-01 18:11:03,265 INFO  mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(489)) - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter2018-05-01 18:11:03,438 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for map tasks2018-05-01 18:11:03,445 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(224)) - Starting task: attempt_local2042689909_0001_m_000000_02018-05-01 18:11:03,493 INFO  output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 12018-05-01 18:11:03,502 INFO  util.ProcfsBasedProcessTree (ProcfsBasedProcessTree.java:isAvailable(192)) - ProcfsBasedProcessTree currently is supported only on Linux.2018-05-01 18:11:03,567 INFO  mapred.Task (Task.java:initialize(614)) -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5b5d72a52018-05-01 18:11:03,574 INFO  mapred.MapTask (MapTask.java:runNewMapper(756)) - Processing split: hdfs://n1:8020/wc/input/data.txt:0+442018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:setEquator(1205)) - (EQUATOR) 0 kvi 26214396(104857584)2018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(998)) - mapreduce.task.io.sort.mb: 1002018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(999)) - soft limit at 838860802018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(1000)) - bufstart = 0; bufvoid = 1048576002018-05-01 18:11:03,666 INFO  mapred.MapTask (MapTask.java:init(1001)) - kvstart = 26214396; length = 65536002018-05-01 18:11:03,671 INFO  mapred.MapTask (MapTask.java:createSortingCollector(403)) - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer2018-05-01 18:11:04,185 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 2018-05-01 18:11:04,189 INFO  mapred.MapTask (MapTask.java:flush(1460)) - Starting flush of map output2018-05-01 18:11:04,190 INFO  mapred.MapTask (MapTask.java:flush(1482)) - Spilling map output2018-05-01 18:11:04,190 INFO  mapred.MapTask (MapTask.java:flush(1483)) - bufstart = 0; bufend = 70; bufvoid = 1048576002018-05-01 18:11:04,191 INFO  mapred.MapTask (MapTask.java:flush(1485)) - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/65536002018-05-01 18:11:04,212 INFO  mapred.MapTask (MapTask.java:sortAndSpill(1667)) - Finished spill 02018-05-01 18:11:04,217 INFO  mapred.Task (Task.java:done(1046)) - Task:attempt_local2042689909_0001_m_000000_0 is done. And is in the process of committing2018-05-01 18:11:04,234 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - map2018-05-01 18:11:04,234 INFO  mapred.Task (Task.java:sendDone(1184)) - Task &#39;attempt_local2042689909_0001_m_000000_0&#39; done.2018-05-01 18:11:04,242 INFO  mapred.Task (Task.java:done(1080)) - Final Counters for attempt_local2042689909_0001_m_000000_0: Counters: 22  File System Counters      FILE: Number of bytes read=150      FILE: Number of bytes written=335288      FILE: Number of read operations=0      FILE: Number of large read operations=0      FILE: Number of write operations=0      HDFS: Number of bytes read=44      HDFS: Number of bytes written=0      HDFS: Number of read operations=6      HDFS: Number of large read operations=0      HDFS: Number of write operations=1  Map-Reduce Framework      Map input records=8      Map output records=8      Map output bytes=70      Map output materialized bytes=92      Input split bytes=97      Combine input records=0      Spilled Records=8      Failed Shuffles=0      Merged Map outputs=0      GC time elapsed (ms)=0      Total committed heap usage (bytes)=233832448  File Input Format Counters       Bytes Read=442018-05-01 18:11:04,243 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(249)) - Finishing task: attempt_local2042689909_0001_m_000000_02018-05-01 18:11:04,243 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.2018-05-01 18:11:04,246 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for reduce tasks2018-05-01 18:11:04,249 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(302)) - Starting task: attempt_local2042689909_0001_r_000000_02018-05-01 18:11:04,249 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_local2042689909_0001 running in uber mode : false2018-05-01 18:11:04,250 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%2018-05-01 18:11:04,259 INFO  output.FileOutputCommitter (FileOutputCommitter.java:&lt;init&gt;(108)) - File Output Committer Algorithm version is 12018-05-01 18:11:04,260 INFO  util.ProcfsBasedProcessTree (ProcfsBasedProcessTree.java:isAvailable(192)) - ProcfsBasedProcessTree currently is supported only on Linux.2018-05-01 18:11:04,317 INFO  mapred.Task (Task.java:initialize(614)) -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@368baef32018-05-01 18:11:04,328 INFO  mapred.ReduceTask (ReduceTask.java:run(362)) - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@42ca90ce2018-05-01 18:11:04,347 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:&lt;init&gt;(205)) - MergerManager: memoryLimit=1987680640, maxSingleShuffleLimit=496920160, mergeThreshold=1311869312, ioSortFactor=10, memToMemMergeOutputsThreshold=102018-05-01 18:11:04,349 INFO  reduce.EventFetcher (EventFetcher.java:run(61)) - attempt_local2042689909_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events2018-05-01 18:11:04,428 INFO  reduce.LocalFetcher (LocalFetcher.java:copyMapOutput(144)) - localfetcher#1 about to shuffle output of map attempt_local2042689909_0001_m_000000_0 decomp: 88 len: 92 to MEMORY2018-05-01 18:11:04,446 INFO  reduce.InMemoryMapOutput (InMemoryMapOutput.java:shuffle(100)) - Read 88 bytes from map-output for attempt_local2042689909_0001_m_000000_02018-05-01 18:11:04,455 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:closeInMemoryFile(319)) - closeInMemoryFile -&gt; map-output of size: 88, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;882018-05-01 18:11:04,461 INFO  reduce.EventFetcher (EventFetcher.java:run(76)) - EventFetcher is interrupted.. Returning2018-05-01 18:11:04,462 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied.2018-05-01 18:11:04,462 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(691)) - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs2018-05-01 18:11:04,482 INFO  mapred.Merger (Merger.java:merge(606)) - Merging 1 sorted segments2018-05-01 18:11:04,482 INFO  mapred.Merger (Merger.java:merge(705)) - Down to the last merge-pass, with 1 segments left of total size: 82 bytes2018-05-01 18:11:04,483 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(758)) - Merged 1 segments, 88 bytes to disk to satisfy reduce memory limit2018-05-01 18:11:04,484 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(788)) - Merging 1 files, 92 bytes from disk2018-05-01 18:11:04,484 INFO  reduce.MergeManagerImpl (MergeManagerImpl.java:finalMerge(803)) - Merging 0 segments, 0 bytes from memory into reduce2018-05-01 18:11:04,485 INFO  mapred.Merger (Merger.java:merge(606)) - Merging 1 sorted segments2018-05-01 18:11:04,486 INFO  mapred.Merger (Merger.java:merge(705)) - Down to the last merge-pass, with 1 segments left of total size: 82 bytes2018-05-01 18:11:04,486 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied.2018-05-01 18:11:04,546 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1243)) - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords2018-05-01 18:11:04,924 INFO  mapred.Task (Task.java:done(1046)) - Task:attempt_local2042689909_0001_r_000000_0 is done. And is in the process of committing2018-05-01 18:11:04,935 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 1 / 1 copied.2018-05-01 18:11:04,935 INFO  mapred.Task (Task.java:commit(1225)) - Task attempt_local2042689909_0001_r_000000_0 is allowed to commit now2018-05-01 18:11:05,044 INFO  output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task &#39;attempt_local2042689909_0001_r_000000_0&#39; to hdfs://n1:8020/wc/output/_temporary/0/task_local2042689909_0001_r_0000002018-05-01 18:11:05,051 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - reduce &gt; reduce2018-05-01 18:11:05,052 INFO  mapred.Task (Task.java:sendDone(1184)) - Task &#39;attempt_local2042689909_0001_r_000000_0&#39; done.2018-05-01 18:11:05,060 INFO  mapred.Task (Task.java:done(1080)) - Final Counters for attempt_local2042689909_0001_r_000000_0: Counters: 29  File System Counters      FILE: Number of bytes read=366      FILE: Number of bytes written=335380      FILE: Number of read operations=0      FILE: Number of large read operations=0      FILE: Number of write operations=0      HDFS: Number of bytes read=44      HDFS: Number of bytes written=32      HDFS: Number of read operations=9      HDFS: Number of large read operations=0      HDFS: Number of write operations=3  Map-Reduce Framework      Combine input records=0      Combine output records=0      Reduce input groups=5      Reduce shuffle bytes=92      Reduce input records=8      Reduce output records=5      Spilled Records=8      Shuffled Maps =1      Failed Shuffles=0      Merged Map outputs=1      GC time elapsed (ms)=28      Total committed heap usage (bytes)=267386880  Shuffle Errors      BAD_ID=0      CONNECTION=0      IO_ERROR=0      WRONG_LENGTH=0      WRONG_MAP=0      WRONG_REDUCE=0  File Output Format Counters       Bytes Written=322018-05-01 18:11:05,060 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(325)) - Finishing task: attempt_local2042689909_0001_r_000000_02018-05-01 18:11:05,061 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - reduce task executor complete.2018-05-01 18:11:05,253 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 100%2018-05-01 18:11:06,254 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_local2042689909_0001 completed successfully2018-05-01 18:11:06,267 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 35  File System Counters      FILE: Number of bytes read=516      FILE: Number of bytes written=670668      FILE: Number of read operations=0      FILE: Number of large read operations=0      FILE: Number of write operations=0      HDFS: Number of bytes read=88      HDFS: Number of bytes written=32      HDFS: Number of read operations=15      HDFS: Number of large read operations=0      HDFS: Number of write operations=4  Map-Reduce Framework      Map input records=8      Map output records=8      Map output bytes=70      Map output materialized bytes=92      Input split bytes=97      Combine input records=0      Combine output records=0      Reduce input groups=5      Reduce shuffle bytes=92      Reduce input records=8      Reduce output records=5      Spilled Records=16      Shuffled Maps =1      Failed Shuffles=0      Merged Map outputs=1      GC time elapsed (ms)=28      Total committed heap usage (bytes)=501219328  Shuffle Errors      BAD_ID=0      CONNECTION=0      IO_ERROR=0      WRONG_LENGTH=0      WRONG_MAP=0      WRONG_REDUCE=0  File Input Format Counters       Bytes Read=44  File Output Format Counters       Bytes Written=32job commit successfully...</code></pre></li><li><p>查看结果</p><pre><code>aaa    2apple    3ooo    1ttt    1yyy    1</code></pre></li></ul><h2 id="提交2：打包到本地，直接运行（项目性能测试）"><a href="#提交2：打包到本地，直接运行（项目性能测试）" class="headerlink" title="提交2：打包到本地，直接运行（项目性能测试）"></a>提交2：打包到本地，直接运行（项目性能测试）</h2><ul><li>配置文件+必要的conf设置</li><li>使用配置文件<blockquote><p>拷贝四个配置文件到源代码下面  </p><pre><code>  core-site.xml  hdfs-site.xml  mapred-site.xml  yarn-site.xml</code></pre><p>在主代码中指定打包的位置，并注释掉代码配置  </p><pre><code>  // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);  // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);  conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;);</code></pre><p>手动打jar：项目右键 &gt; export &gt; &gt; java jar file &gt; 指定路径<br>选择main calss<br>运行下java程序<br>刷新 <a href="http://n3:8088/cluster" target="_blank" rel="noopener">http://n3:8088/cluster</a> 是不是有新的任务<br>运行任务时，常见的报错如下<br><a href="https://stackoverflow.com/questions/24075669/mapreduce-job-fail-when-submitted-from-windows-machine" target="_blank" rel="noopener">解决链接1:conf设置夸平台提交</a><br><a href="http://zy19982004.iteye.com/blog/2031172" target="_blank" rel="noopener">解决链接2:修改源码</a>  </p><pre><code>  问题：  org.apache.hadoop.util.Shell$ExitCodeException: /bin/bash: line 0: fg: no job control  解决1：  这是由于跨平台造成的，经过Google搜索口在StackOverflow上发现答案  conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  解决2：  需要手动修改hadoop源码，目前还没有弄明白</code></pre><p>处理后，现在的main如下  </p><pre><code>  // 默认加载根目录src目录下的配置文件  Configuration conf = new Configuration();  // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);  // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);  conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;);</code></pre><p>以java application的方式运行程序<br>在控制台和active节点 <a href="http://n3:8088/cluster" target="_blank" rel="noopener">http://n3:8088/cluster</a> 查看运行的日志，WebUI会显示提交的任务<br>在eclipse-hadoop文件系统查看是不是有数据输出  </p></blockquote></li></ul><h2 id="提交3：打包到本地，手动上传（项目上线时候）"><a href="#提交3：打包到本地，手动上传（项目上线时候）" class="headerlink" title="提交3：打包到本地，手动上传（项目上线时候）"></a>提交3：打包到本地，手动上传（项目上线时候）</h2><ul><li>配置文件+删除不必要的conf设置</li><li>屏蔽不必要的cond指定<pre><code>  // 默认加载根目录src目录下的配置文件  Configuration conf = new Configuration();  // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n1:8020&quot;);  // conf.set(&quot;yarn.resourcemanager.hotsname&quot;, &quot;n3&quot;);  // conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);  // conf.set(&quot;mapred.jar&quot;, &quot;D:\\mr\\wc.jar&quot;);</code></pre></li><li>手动打包，指定main class</li><li>通过命令运行程序<pre><code>  hadoop jar jar路径 程序的入口(主程序的入口)  hadoop jar wc-1.jar com.hikvision.wc.WCJob</code></pre></li><li>查看输出和webUI信息</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何在本地编写wordcount程序，在远程的hadoop集群上运行测试代码，重点介绍了hadoop本地开发后的三种提交到服务器的方式，分别涉及到了项目开发，项目测试，项目交付时期的不同提交方式。&lt;/p&gt;
    
    </summary>
    
      <category term="程序" scheme="http://yoursite.com/categories/%E7%A8%8B%E5%BA%8F/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="程序" scheme="http://yoursite.com/tags/%E7%A8%8B%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>PE hadoop高可用集群搭建总结</title>
    <link href="http://yoursite.com/2018/04/30/PE%20hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/04/30/PE hadoop高可用集群搭建总结/</id>
    <published>2018-04-30T10:26:25.000Z</published>
    <updated>2018-05-04T08:49:44.929Z</updated>
    
    <content type="html"><![CDATA[<p>本文简单记录生产环境中的hadoop高可用集群搭建总结。</p><a id="more"></a><h2 id="hdfs-ha理论总结"><a href="#hdfs-ha理论总结" class="headerlink" title="hdfs ha理论总结"></a>hdfs ha理论总结</h2><ul><li>NN内存受到限制：federation</li><li>NN单点故障：edits和fsimage原理，在没有格式化的NN上standby操作</li><li>edits记录日志文件，保障一致性，交给第三方JNs来管理，奇数个，不用NFS不可靠</li><li>zookeeper，奇数个，它的ZKFC心跳监控NN；管理和切换zkfc</li></ul><h2 id="yarn-ha搭建过程总结"><a href="#yarn-ha搭建过程总结" class="headerlink" title="yarn ha搭建过程总结"></a>yarn ha搭建过程总结</h2><ul><li>删除所有的临时文件夹(重要)</li><li>启动zk集群，三台，最好在xshell下面一起启动，并且查看状态status</li><li>edits交给JN,手动启动三个JN</li><li>格式化一台NN，并启动这一台(只能格式化一次，不然重新来)</li><li>另外一台NN执行同步standby</li><li>格式化zk</li><li>启动dfs(可以直接全部启动)</li><li>最后启动RS<pre><code>rm -rdf /opt/hadoop/* /opt/journal/* /opt/zookeeper/v* /opt/zookeeper/z*zkServer.sh start(xshell同时启动)vim /root/app/hadoop/etc/hadoop/slaves(配置好datanode)hadoop-daemon.sh start journalnode(根据hdfs配置文件，n2 n3 n4)hdfs namenode -format(选一个NN格式化，一次机会，失败重头再来)hadoop-daemon.sh start namenode(格式化的NN上)hdfs namenode -bootstrapStandby(n2,没有格式化的NN上)hdfs zkfc -formatZK(n1)start-all.sh(n1)yarn-daemon.sh start resourcemanager(指定的机器，n3 n4)</code></pre></li></ul><h2 id="mr-yarn理论"><a href="#mr-yarn理论" class="headerlink" title="mr yarn理论"></a>mr yarn理论</h2><ul><li>四个阶段：split，map,shuffle,reduce, 最小的MR程序包含了前面两个</li><li>split块</li><li>map task按行读&lt;K,V&gt;排序</li><li>分区partition，memory buffer，sort &amp; combine（手动实现，不能代替reduce，可以提升效率）</li><li>reduce&lt;K,V&gt;迭代器取数据</li></ul><h2 id="整合mr到hadoop，实现RS-ha"><a href="#整合mr到hadoop，实现RS-ha" class="headerlink" title="整合mr到hadoop，实现RS ha"></a>整合mr到hadoop，实现RS ha</h2><ul><li>先停止hdfs</li><li>配置mr，配置yarn高可用</li><li>resource manager需要单独启动</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文简单记录生产环境中的hadoop高可用集群搭建总结。&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>本地eclipse链接远程hadoop编写hdfs测试代码</title>
    <link href="http://yoursite.com/2018/04/30/%E6%9C%AC%E5%9C%B0eclipse%E9%93%BE%E6%8E%A5%E8%BF%9C%E7%A8%8Bhadoop%E7%BC%96%E5%86%99hdfs%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2018/04/30/本地eclipse链接远程hadoop编写hdfs测试代码/</id>
    <published>2018-04-30T07:45:35.000Z</published>
    <updated>2018-05-01T13:16:37.996Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何使用eclipse插件连接远程的hadoop，并且编写hdfs测试代码。</p><a id="more"></a><h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><ul><li>local: win10 + eclipse neon + hadoop-eclipse-2.7.3.jar</li><li>remote: hadoop 2.7.6 + hdfs namenode ha 架构 + yarn resource manager ha 架构</li></ul><h2 id="本地eclipse连接远程的hadoop集群"><a href="#本地eclipse连接远程的hadoop集群" class="headerlink" title="本地eclipse连接远程的hadoop集群"></a>本地eclipse连接远程的hadoop集群</h2><ul><li>后面所有开发的前提是windows用户名必须为root，win+x+G &gt; 本地用户和组 &gt; 用户 &gt; 修改系统的用户名为root</li><li>上面的步骤，我没有成功，后来重新安装系统设置为root用户</li><li>安装hadoop-eclipse插件，copy jar到插件目录，启动，选择windows &gt; show view &gt; other &gt; hadoop/mr</li><li>需要在windows本地安装配置和服务器相同版本的hadoop，并且下载关键字为&lt;hadoo2.7.3的hadoop.dll和winutils.exe&gt;的组件放在本地hadoop/bin下</li><li>在MR locations窗口下配置连接远程hadoop服务器，其中dfs master的端口为 50070 webUI能够访问的且显示为active的端口</li><li>比如：我的n2:50070 webUI上面显示8020端口 active, 所以hadoop配置为8020</li><li>暂时没有用到MR，所以默认就好</li><li>完成上述步骤即可连接远程服务器成功</li><li>可以创建文件进行测试：eclipse窗口创建，webUI显示，或者终端上使用hdfs shell查看文件列表</li></ul><h2 id="创建项目手动导入依赖包"><a href="#创建项目手动导入依赖包" class="headerlink" title="创建项目手动导入依赖包"></a>创建项目手动导入依赖包</h2><ul><li>创建项目，直接常见一个Java项目；不着急创建hadoop/mapreduce项目</li><li>手动添加依赖；不着急使用maven构建项目</li><li>build path &gt; libraries窗口 &gt; add library &gt; user library &gt; user libraries &gt; new</li><li>name随意hadoop2.7.6 &gt; and external jars</li><li>导入hadoop安装路径下面的C:\app2\hadoop-2.7.6\share\hadoop下面全部组件的lib下面的所有jars</li><li>最后可以看到窗口有了外部依赖hadoop2.7.6</li><li>最后还需要以上面的方式导入junit测试包</li></ul><h2 id="编写连接hdfs的测试代码"><a href="#编写连接hdfs的测试代码" class="headerlink" title="编写连接hdfs的测试代码"></a>编写连接hdfs的测试代码</h2><ul><li><p>代码文件为/hdfs-test/src/com/hik/hdfs/HdfsDemo.java，下面的代码包含了连接, 创建文件夹，上传下载文件，合并小文件，下载小文件等操作，简单实现网盘的功能</p><pre><code>package com.hik.hdfs;import java.io.File;import java.io.FileNotFoundException;import java.io.IOException;import org.apache.commons.io.FileUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.SequenceFile;import org.apache.hadoop.io.SequenceFile.Writer;import org.apache.hadoop.io.Text;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HdfsDemo {  FileSystem fs;  Configuration configuration;  @Before  public void begin() throws Exception {      // load configuration file from src      configuration = new Configuration();      // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://n2:8020&quot;);      fs = FileSystem.get(configuration);  }  @After  public void end() throws Exception {      fs.close();  }  @Test  public void mkdir() throws Exception {      Path path = new Path(&quot;/tmp_innerConfig&quot;);      boolean newdir = fs.mkdirs(path);      System.out.println(newdir);  }  @Test  public void upload() throws IOException {      Path path = new Path(&quot;/tmp/a2.txt&quot;);      FSDataOutputStream outputStream = fs.create(path);      FileUtils.copyFile(new File(&quot;D:\\test.txt&quot;), outputStream);  }  @Test  public void list() throws FileNotFoundException, IOException {      Path path = new Path(&quot;/tmp/&quot;);      FileStatus[] listStatus = fs.listStatus(path);      for (FileStatus x : listStatus) {          String str = x.getPath() + &quot;--&quot; + x.getLen() + &quot;--&quot; + x.getAccessTime();          System.out.println(str);      }  }  @Test  public void uploadSmalltoBig() throws Exception {      Path path = new Path(&quot;/seq.txt&quot;);      @SuppressWarnings(&quot;deprecation&quot;)      Writer writer = SequenceFile.createWriter(fs, configuration, path, Text.class, Text.class);      File file = new File(&quot;D:\\file&quot;);      for (File f : file.listFiles()) {          Text name = new Text(f.getName());          Text content = new Text(FileUtils.readFileToString(f, &quot;UTF-8&quot;));          writer.append(name, content);      }  }  @Test  public void downloadBig() throws Exception {      Path path = new Path(&quot;/seq.txt&quot;);      @SuppressWarnings({ &quot;resource&quot;, &quot;deprecation&quot; })      SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, configuration);      Text key = new Text();      Text value = new Text();      while (reader.next(key, value)) {          System.out.println(key);          System.out.println(value);      }  }}</code></pre></li></ul><h2 id="运行测试程序"><a href="#运行测试程序" class="headerlink" title="运行测试程序"></a>运行测试程序</h2><ul><li>配置本地windows的hosts文件，直接用everything搜索找到c盘下面的hosts,C:\Windows\System32\drivers\etc\hosts, 加入ip和主机名的映射关系<pre><code>192.168.44.100 n1192.168.44.101 n2192.168.44.102 n3192.168.44.103 n4</code></pre></li><li>运行代码前，需要导入连接先关的配置文件，将hadoop/etc/hadoop下的hdfs-site.xml和core-site.xml拷贝到src文件夹下，直接在eclipse里面粘贴</li><li>上面的步骤也可以在程序的configuration中设置对应的rpc接口地址</li><li>直接在函数名上右键run as &gt; junit test 即可</li><li>如果成功绿色状态，查看hdfs文件系统是不是生成相应的文件</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何使用eclipse插件连接远程的hadoop，并且编写hdfs测试代码。&lt;/p&gt;
    
    </summary>
    
      <category term="代码" scheme="http://yoursite.com/categories/%E4%BB%A3%E7%A0%81/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
      <category term="eclipse" scheme="http://yoursite.com/tags/eclipse/"/>
    
  </entry>
  
  <entry>
    <title>PE mr configuration and yarn resource manager ha</title>
    <link href="http://yoursite.com/2018/04/30/PE%20mr%20configuration%20and%20yarn%20resource%20manager%20ha/"/>
    <id>http://yoursite.com/2018/04/30/PE mr configuration and yarn resource manager ha/</id>
    <published>2018-04-30T06:26:24.000Z</published>
    <updated>2018-05-01T13:16:37.977Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何将MR计算模型集成到hdfs ha中，并且实现yarn的RS的高可用。</p><a id="more"></a><h2 id="集群配置需求"><a href="#集群配置需求" class="headerlink" title="集群配置需求"></a>集群配置需求</h2><ul><li>集群配置地点<pre><code>  NN  DN  JN  ZK  ZKFC    RSn1  1           1   1n2  1   1   1   1   1n3      1   1   1           1n4      1   1               1</code></pre></li></ul><h2 id="MR配置，yarn-RS-ha配置"><a href="#MR配置，yarn-RS-ha配置" class="headerlink" title="MR配置，yarn RS ha配置"></a>MR配置，yarn RS ha配置</h2><ul><li><p>mapred-site.xml</p><pre><code>vim mapred-site.xml &lt;configuration&gt;      &lt;property&gt;              &lt;name&gt;mapreduce.framework.name&lt;/name&gt;              &lt;value&gt;yarn&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>yarn-site.xml</p><pre><code>&lt;configuration&gt;      &lt;!-- 配置MR --&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;      &lt;/property&gt;      &lt;!-- resourcemanager高可用ha --&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;        &lt;value&gt;sxt2yarn&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;        &lt;value&gt;rm1,rm2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;        &lt;value&gt;n3&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;        &lt;value&gt;n4&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;        &lt;value&gt;n3:8088&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;        &lt;value&gt;n4:8088&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置zk集群 --&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;        &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>同步到其他机器上</p><pre><code>scp ./*.xml n2:`pwd`scp ./*.xml n3:`pwd`scp ./*.xml n4:`pwd`</code></pre></li></ul><h2 id="MR启动，查看yarn-RS-ha"><a href="#MR启动，查看yarn-RS-ha" class="headerlink" title="MR启动，查看yarn RS ha"></a>MR启动，查看yarn RS ha</h2><ul><li><p>停掉所有</p><pre><code>zkServer.sh stopstop-dfs.shkill -9 pid</code></pre></li><li><p>启动zk</p><pre><code>root@n1:~# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower</code></pre></li><li><p>启动所有</p><pre><code>root@n1:~# start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.shStarting namenodes on [n1 n2]n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.outn2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.outn4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outStarting journal nodes [n2 n3 n4]n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outn3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.outn4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.outStarting ZK Failover Controllers on NN hosts [n1 n2]n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outn2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.outstarting yarn daemonsstarting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n1.outn3: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n3.outn2: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n2.outn4: starting nodemanager, logging to /root/app/hadoop/logs/yarn-root-nodemanager-n4.out</code></pre></li><li><p>查看进程</p><pre><code>root@n1:~# jps6162 Jps5363 QuorumPeerMain5923 DFSZKFailoverController5578 NameNode#root@n2:~# jps5600 NodeManager5715 Jps5044 NameNode5157 DataNode5448 DFSZKFailoverController4920 QuorumPeerMain5294 JournalNode#root@n3:~# jps3856 Jps3746 NodeManager3477 DataNode3609 JournalNode3354 QuorumPeerMain#root@n4:~# jps3441 DataNode3575 JournalNode3817 Jps3710 NodeManager</code></pre></li><li><p>查看DN(也是nodemanager的配置，RS yarn需要手动启动)</p><pre><code>root@n1:~#  cat /root/app/hadoop/etc/hadoop/slavesn2n3n4</code></pre></li><li><p>启动resourcemanager/yarn/nodemanager（n3, n4）</p><pre><code>yarn-daemon.sh start resourcemanager</code></pre></li><li><p>启动结果slaves配置就是DN和nodemanager的配置</p><pre><code>root@n3:~# yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n3.outroot@n3:~# jps3905 ResourceManager3746 NodeManager3477 DataNode3609 JournalNode3354 QuorumPeerMain3951 Jpsroot@n3:~# #root@n4:~# yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /root/app/hadoop/logs/yarn-root-resourcemanager-n4.outroot@n4:~# jps3441 DataNode3859 ResourceManager3910 Jps3575 JournalNode3710 NodeManagerroot@n4:~# #root@n2:~# jps5600 NodeManager5764 Jps5044 NameNode5157 DataNode5448 DFSZKFailoverController4920 QuorumPeerMain5294 JournalNoderoot@n2:~# #root@n1:~# jps5363 QuorumPeerMain5923 DFSZKFailoverController5578 NameNode6333 Jps</code></pre></li><li><p>查看yarn http端口8088</p><pre><code>root@n3:~# netstat -nltpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:33139         0.0.0.0:*               LISTEN      3477/java       tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      1210/dnsmasq    tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1181/sshd       tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1048/cupsd      tcp        0      0 0.0.0.0:50010           0.0.0.0:*               LISTEN      3477/java       tcp        0      0 0.0.0.0:50075           0.0.0.0:*               LISTEN      3477/java       tcp        0      0 0.0.0.0:8480            0.0.0.0:*               LISTEN      3609/java       tcp        0      0 0.0.0.0:50020           0.0.0.0:*               LISTEN      3477/java       tcp        0      0 0.0.0.0:8485            0.0.0.0:*               LISTEN      3609/java       tcp6       0      0 192.168.44.102:3888     :::*                    LISTEN      3354/java       tcp6       0      0 :::22                   :::*                    LISTEN      1181/sshd       tcp6       0      0 ::1:631                 :::*                    LISTEN      1048/cupsd      tcp6       0      0 192.168.44.102:8088     :::*                    LISTEN      3905/java       tcp6       0      0 :::41529                :::*                    LISTEN      3354/java       tcp6       0      0 :::13562                :::*                    LISTEN      3746/java       tcp6       0      0 192.168.44.102:8030     :::*                    LISTEN      3905/java       tcp6       0      0 192.168.44.102:8031     :::*                    LISTEN      3905/java       tcp6       0      0 192.168.44.102:8032     :::*                    LISTEN      3905/java       tcp6       0      0 192.168.44.102:8033     :::*                    LISTEN      3905/java       tcp6       0      0 :::2181                 :::*                    LISTEN      3354/java       tcp6       0      0 :::8040                 :::*                    LISTEN      3746/java       tcp6       0      0 :::8042                 :::*                    LISTEN      3746/java       tcp6       0      0 :::43405                :::*                    LISTEN      3746/java </code></pre></li><li><p>查看wenUI</p><pre><code>http://n1:50070/dfshealth.html#tab-overviewhttp://n2:50070/dfshealth.html#tab-overview观察下n1和n2谁是active和standby观察下datanode是不是全n1,n2,n3观察下文件系统是不是active的NN可用，standby的NN不可用http://n3:8088/clusterhttp://n4:8088/cluster(会自动跳转到上面的链接，n4 RS 处于standby状态)n3和n4上启动RS，自动关联到nodemanager，管理DN，查看节点ActiveNodes是不是和DN数目一样3个</code></pre></li></ul><h2 id="测试yarn-RS-ha"><a href="#测试yarn-RS-ha" class="headerlink" title="测试yarn RS ha"></a>测试yarn RS ha</h2><ul><li>测试ha的状态<pre><code>yarn-daemon.sh stop resourcemanager（n3）jps挂掉n3 RS，看看n4:8088的状态active再次启动n3，这时候n3处于standby状态</code></pre></li></ul><h2 id="生产环境的高可用Hadoop集群的搭建总结"><a href="#生产环境的高可用Hadoop集群的搭建总结" class="headerlink" title="生产环境的高可用Hadoop集群的搭建总结"></a>生产环境的高可用Hadoop集群的搭建总结</h2><ul><li>高可用解决了1.x的单节点不可靠的问题</li><li>高可用ha一方面指的是hdfs的namenode的高可用，解决NN的单节点故障问题</li><li>高可用ha另一方面指的是yarn的resource manager的高可用，解决yarn单节点不可靠的问题</li><li>2.x为什么引入了yarn呢？yarn可用保障计算的高可靠</li><li>实验的一般流程：</li></ul><ol><li>Hadoop单机模式：直接跑jar文件</li><li>伪分布式模式：配置成分布式模式，只有一台vm</li><li>全分布式模式：使用1.x架构，具有secondary NN</li><li>全分布式hdfs NN ha模式：使用了QJM实现了hdfs的高可用架构</li><li>全分布式hdfs NN ha + yarn RS ha模式：使用了集群实现了yarn对DN的管理</li></ol><ul><li>对比可以发现1.x和2.x的共同点都是移动计算不移动数据</li><li>对比可以发现1.x和2.x的差异性在于1.x由client直接操作DN，但是2.x加入了yarn层管理DN</li><li>2.x有两个瓶颈NN和RS，因此需要ha处理</li><li>其中4中的QJM，包含了集群：</li></ul><ol><li>设计目标NN ha</li><li>从下到上：DN–&gt;NN active/NN standby–&gt;JN(edits管理)–&gt;zk中zkfc–&gt;zk</li></ol><ul><li>其中5包含了集群：</li></ul><ol><li>设计目标是yarn中的resource manager</li><li>yarn的管理包含了：resource manager（RS）–&gt;node manager</li><li>DN–&gt;NN active/NN standby–&gt;JN(edits管理)–&gt;zk中zkfc–&gt;zk–&gt;RS</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何将MR计算模型集成到hdfs ha中，并且实现yarn的RS的高可用。&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="mapreduce" scheme="http://yoursite.com/tags/mapreduce/"/>
    
      <category term="yarn" scheme="http://yoursite.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>PE hadoop 2.x hdfs availablity implement</title>
    <link href="http://yoursite.com/2018/04/30/PE%20hadoop%202.x%20hdfs%20availablity%20implement/"/>
    <id>http://yoursite.com/2018/04/30/PE hadoop 2.x hdfs availablity implement/</id>
    <published>2018-04-30T06:08:31.000Z</published>
    <updated>2018-05-01T13:16:37.976Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何在hdfs上实现namenode集群的高可用。</p><a id="more"></a><h2 id="集群配置需求"><a href="#集群配置需求" class="headerlink" title="集群配置需求"></a>集群配置需求</h2><ul><li>集群配置地点<pre><code>  NN  DN  JN  ZK  ZKFCn1  1           1   1n2  1   1   1   1   1n3      1   1   1   n4      1   1</code></pre></li></ul><h2 id="配置HDFS"><a href="#配置HDFS" class="headerlink" title="配置HDFS"></a>配置HDFS</h2><ul><li><p>hdfs-site.xml</p><pre><code>vim /root/app/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;      &lt;!-- 配置NN空间 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.nameservices&lt;/name&gt;              &lt;value&gt;sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.namenodes.sxt&lt;/name&gt;              &lt;value&gt;nn1,nn2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.rpc-address.sxt.nn1&lt;/name&gt;              &lt;value&gt;n1:8020&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.rpc-address.sxt.nn2&lt;/name&gt;              &lt;value&gt;n2:8020&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.http-address.sxt.nn1&lt;/name&gt;              &lt;value&gt;n1:50070&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.http-address.sxt.nn2&lt;/name&gt;              &lt;value&gt;n2:50070&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置JN能处理的Node，可以理解为DN --&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;              &lt;value&gt;qjournal://n2:8485;n3:8485;n4:8485/sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.client.failover.proxy.provider.sxt&lt;/name&gt;              &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置密钥 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;              &lt;value&gt;sshfence&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;              &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置JN的临时文件夹 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;              &lt;value&gt;/opt/journal/node/data&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 单节点故障自动迁移 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;              &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>core-site.xml</p><pre><code>vim /root/app/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;      &lt;property&gt;              &lt;name&gt;fs.defaultFS&lt;/name&gt;              &lt;value&gt;hdfs://sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;              &lt;value&gt;/opt/hadoop&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置Quorum Journal Manager的zk集群，JN管理集群 --&gt;      &lt;property&gt;              &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;              &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li></ul><h2 id="配置zookeeper"><a href="#配置zookeeper" class="headerlink" title="配置zookeeper"></a>配置zookeeper</h2><ul><li><p>环境变量</p><pre><code>cat /etc/profileexport ANT_HOME=/root/app/antexport HADOOP_HOME=/root/app/hadoopexport JAVA_HOME=/root/app/jdkexport JRE_HOME=/root/app/jdk/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$ANT_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/app/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin注意只有前面三台的zk路径加入环境变量</code></pre></li><li><p>配置文件</p><pre><code>cat /root/app/zookeeper/conf/zoo.cfg# The number of milliseconds of each tick# dataDir=/tmp/zookeeperdataDir=/opt/zookeeperserver.1=n1:2888:3888server.2=n2:2888:3888server.3=n3:2888:3888</code></pre></li><li><p>分别配置/opt/zookeeper目录，创建myid文件，加入1, 2, 3</p></li><li><p>同时启动zk</p><pre><code>zkServer.sh start</code></pre></li><li><p>查看状态</p><pre><code>zkServer.sh status</code></pre></li><li><p>启动输出</p><pre><code>root@n1:~# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower</code></pre></li><li><p>n1拷贝zookeeper到n2, n3, n4</p><pre><code>scp -r ./dir n1:`pwd`</code></pre></li></ul><h2 id="启动ZK"><a href="#启动ZK" class="headerlink" title="启动ZK"></a>启动ZK</h2><ul><li>启动并查看状态（1, 2, 3同时启动）<pre><code>zkServer.sh startzkServer.sh status</code></pre></li></ul><h2 id="启动JN"><a href="#启动JN" class="headerlink" title="启动JN"></a>启动JN</h2><ul><li><p>在n2. n3, n4启动JN</p><pre><code>hadoop-daemon.sh start journalnode</code></pre></li><li><p>输出</p><pre><code>root@n2:~# hadoop-daemon.sh start journalnodestarting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outroot@n2:~# jps1751 QuorumPeerMain1895 JournalNode1945 Jps</code></pre></li></ul><h2 id="启动NN，高可用HA操作"><a href="#启动NN，高可用HA操作" class="headerlink" title="启动NN，高可用HA操作"></a>启动NN，高可用HA操作</h2><ul><li><p>在一台NN格式化（NN:n1）</p><pre><code>hdfs namenode -format</code></pre></li><li><p>在没有格式化的另外一台hadoop执行standby操作（NN:n2）</p><pre><code>hdfs namenode -bootstrapStandby</code></pre></li><li><p>报错提示n1没有启动namenode，先启动namenode</p><pre><code>hadoop-daemon.sh start namenode</code></pre></li><li><p>在此执行standby成功（格式化+启动n1，standby另外n2）</p><pre><code>18/04/29 11:14:41 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/04/29 11:14:41 INFO namenode.NameNode: createNameNode [-bootstrapStandby]#=====================================================About to bootstrap Standby ID nn2 from:         Nameservice ID: sxt      Other Namenode ID: nn1Other NN&#39;s HTTP address: http://n1:50070Other NN&#39;s IPC  address: n1/192.168.44.100:8020           Namespace ID: 1765158274          Block pool ID: BP-1441163464-192.168.44.100-1525025632122             Cluster ID: CID-2e601647-294c-4e70-8e72-7a82bea94fa9         Layout version: -63     isUpgradeFinalized: true#=====================================================18/04/29 11:14:42 INFO common.Storage: Storage directory /opt/hadoop/dfs/name has been successfully formatted.18/04/29 11:14:43 INFO namenode.TransferFsImage: Opening connection to http://n1:50070/imagetransfer?getimage=1&amp;txid=0&amp;storageInfo=-63:1765158274:0:CID-2e601647-294c-4e70-8e72-7a82bea94fa918/04/29 11:14:43 INFO namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds18/04/29 11:14:43 INFO namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s18/04/29 11:14:43 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 321 bytes.18/04/29 11:14:43 INFO util.ExitUtil: Exiting with status 018/04/29 11:14:43 INFO namenode.NameNode: SHUTDOWN_MSG: SHUTDOWN_MSG: Shutting down NameNode at n2/192.168.44.101</code></pre></li></ul><h2 id="查看HA效果"><a href="#查看HA效果" class="headerlink" title="查看HA效果"></a>查看HA效果</h2><ul><li><p>在一个NN上格式化zookeeper（n1）</p><pre><code>hdfs zkfc -formatZK</code></pre></li><li><p>在单节点NN启动n1</p><pre><code>start-dfs.sh</code></pre></li><li><p>输出</p><pre><code>Starting namenodes on [n1 n2]n1: namenode running as process 2411. Stop it first.n2: namenode running as process 2239. Stop it first.n2: datanode running as process 2413. Stop it first.n4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outStarting journal nodes [n2 n3 n4]n3: journalnode running as process 2207. Stop it first.n2: journalnode running as process 1895. Stop it first.n4: journalnode running as process 1732. Stop it first.Starting ZK Failover Controllers on NN hosts [n1 n2]n2: zkfc running as process 2804. Stop it first.n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outroot@n1:~# </code></pre></li><li><p>进程查看</p><pre><code>root@n1:~# jps3091 DFSZKFailoverController3285 Jps2411 NameNode2188 QuorumPeerMain#root@n2:~# jps3184 Jps2804 DFSZKFailoverController1751 QuorumPeerMain1895 JournalNode2413 DataNode2239 NameNode#root@n3:~# jps2512 Jps2121 QuorumPeerMain2362 DataNode2207 JournalNode#root@n4:~# jps1732 JournalNode2039 Jps1887 DataNode#  NN  DN  JN  ZK  ZKFCn1  1           1   1n2  1   1   1   1   1n3      1   1   1   n4      1   1</code></pre></li><li><p>查看webUI</p><pre><code>http://n2:50070/dfshealth.html#tab-overviewOverview &#39;n2:8020&#39; (active)#http://n1:50070/dfshealth.html#tab-overviewOverview &#39;n1:8020&#39; (standby)#Datanode Information三个#http://n1:50070/explorer.html#/Operation category READ is not supported in state standby#http://n2:50070/explorer.html#/Browse Directory 可见</code></pre></li><li><p>故障测试</p><pre><code>hadoop-daemon.sh stop namenode（n2）查看网页，文件系统，DNhadoop-daemon.sh start namenode（n2）再次查看交换了状态Overview &#39;n1:8020&#39; (active)Overview &#39;n2:8020&#39; (standby)</code></pre></li></ul><h2 id="重新启动与停止"><a href="#重新启动与停止" class="headerlink" title="重新启动与停止"></a>重新启动与停止</h2><ul><li>启停操作<pre><code>stop-dfs.shroot@n1:~# stop-dfs.sh Stopping namenodes on [n1 n2]n1: stopping namenoden2: stopping namenoden3: stopping datanoden4: stopping datanoden2: stopping datanodeStopping journal nodes [n2 n3 n4]n2: stopping journalnoden3: stopping journalnoden4: stopping journalnodeStopping ZK Failover Controllers on NN hosts [n1 n2]n1: stopping zkfcn2: stopping zkfc#下次启动jps查看ZK是不是启动先启动ZK，在启动DFS#root@n1:~# zkServer.sh start（根据列表中的三台都同时启动zk）root@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower#root@n1:~# start-dfs.sh Starting namenodes on [n1 n2]n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.outn2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.outn4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outn2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.outStarting journal nodes [n2 n3 n4]n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outn3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.outn4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.outStarting ZK Failover Controllers on NN hosts [n1 n2]n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outn2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.out</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何在hdfs上实现namenode集群的高可用。&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="ha" scheme="http://yoursite.com/tags/ha/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>hbase的基本构成与实践</title>
    <link href="http://yoursite.com/2018/04/24/hbase%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
    <id>http://yoursite.com/2018/04/24/hbase的基本构成与实践/</id>
    <published>2018-04-24T13:14:15.000Z</published>
    <updated>2018-05-01T13:16:37.983Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍hbase的基本构成与实践。</p><a id="more"></a><h2 id="hbase的基本构成"><a href="#hbase的基本构成" class="headerlink" title="hbase的基本构成"></a>hbase的基本构成</h2><ul><li><p>表空间 namespace</p><pre><code>两个默认的表空间hbase： 系统默认表空间default： 不指定自动加入的表空间root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/dataFound 2 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/defaultdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase</code></pre></li><li><p>表 table</p><pre><code>表的存在形式：表以文件夹的形式存在于hdfs中表的基本组成是：RowKey, Column Family, Column, Value(Cell):Byte array表的物理属性：以RowKey进行字典排序，行的方向存在多个Region，Region是存储和负载均衡的最小单元，不同的Region分布到不同的RegionServer上#=============================root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbaseFound 2 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/metadrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/namespace#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/metaFound 3 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/meta/.tabledescdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/meta/.tmpdrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/meta/1588230740Found 4 items-rw-r--r--   1 root supergroup         32 2018-04-23 20:18 /hbase/data/hbase/meta/1588230740/.regioninfodrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/.tmpdrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/infodrwxr-xr-x   - root supergroup          0 2018-04-24 01:37 /hbase/data/hbase/meta/1588230740/recovered.edits#-----------------------------root@ubuntu:~/app/hbase/bin# ./hbase shell2018-04-24 06:29:19,776 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017hbase(main):001:0&gt;#-----------------------------hbase(main):001:0&gt; create &#39;maizi_hbase&#39;,&#39;f&#39;0 row(s) in 2.5670 seconds=&gt; Hbase::Table - maizi_hbasehbase(main):002:0&gt;#=============================访问 http://192.168.231.150:50070/explorer.html#/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458得到hbse的路径#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458Found 3 items-rw-r--r--   1 root supergroup         46 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/.regioninfodrwxr-xr-x   - root supergroup          0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/fdrwxr-xr-x   - root supergroup          0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/recovered.edits#-----------------------------上述操作中，6a028f21704f8fc6bf298598f6b8a458为Region的编号#=============================访问 http://192.168.231.150:16010/table.jsp?name=maizi_hbase 得到hbase的管理界面，可以看出路径结构</code></pre></li><li><p>列族 column family</p><pre><code>很多列的集合hbase中的每个列都属于一个column family每个column family存在于hdfs的单独文件中列名以column family为前缀， info:name, info:age#-----------------------------/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/f数据库/数据/表空间/表/region/列族#-----------------------------创建表的时候必须定义列族，因为在hdfs上必须要创建文件夹#-----------------------------何如设计RowKey是经典问题？</code></pre></li><li><p>列</p><pre><code>存放数据的地方</code></pre></li><li><p>RowKey</p><pre><code>可以理解为主键，最大长度为64k，RowKey保存为字节数组是非关系型数据库中key-value类型的数据的key自动字典排序散列原则，分布到不同的Region中，RegionServer的负载均衡问题</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍hbase的基本构成与实践。&lt;/p&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>how to understand habse components ?</title>
    <link href="http://yoursite.com/2018/04/24/how%20to%20understand%20habse%20component%20/"/>
    <id>http://yoursite.com/2018/04/24/how to understand habse component /</id>
    <published>2018-04-24T08:45:26.000Z</published>
    <updated>2018-05-01T13:16:37.988Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes the basic components of hbase, including HMaster, HRegionServer, Region.</p><a id="more"></a><h2 id="principle-of-hbase"><a href="#principle-of-hbase" class="headerlink" title="principle of hbase"></a>principle of hbase</h2><ul><li><strong>features of HMaster (technical director)</strong></li></ul><ol><li>add, delete, and modify tables</li><li>Region load balancing</li><li>HMaster manages the distribution of data</li></ol><ul><li><strong>features of RegionServer (department manager)</strong></li></ul><ol><li>RegionServer is the service component of Hbase</li><li>RegionServer maintains Regions assigned by HMaster</li><li>RegionServer can divide big Regions</li></ol><ul><li><strong>features of Region (developers)</strong></li></ul><ol><li>Region is a partition</li><li>handling the tasks assigned by RegionServer</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes the basic components of hbase, including HMaster, HRegionServer, Region.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper working mechanism and installation</title>
    <link href="http://yoursite.com/2018/04/24/zooKeeper%20working%20mechanism%20and%20installation/"/>
    <id>http://yoursite.com/2018/04/24/zooKeeper working mechanism and installation/</id>
    <published>2018-04-24T06:55:35.000Z</published>
    <updated>2018-05-01T13:16:37.990Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes the basic working principle and installation process of zookeeper.</p><a id="more"></a><h2 id="zookeeper-working-mechanism"><a href="#zookeeper-working-mechanism" class="headerlink" title="zookeeper working mechanism"></a>zookeeper working mechanism</h2><ul><li>zookeeper is a high-performance application coordination server that is mainly used to maintain a file system-like namespace.</li><li>zookeeper itself contains 2n+1 servers, and their roles are divided into leader and follower.</li><li>zookeeper maintains multiple server services and maintains data consistency to ensure that clients connecting to any server can get consistent data services.</li><li>zookeeper’s nodes have 4 life cycles.<pre><code>PERSISTENT (persistent node)PERSISTENT_SEQUENTIAL (Sequential automatic numbering of persistent nodes, this node automatically adds 1 based on the number of existing nodes)EPHEMERAL (temporary node, client session timeout such nodes will be automatically deleted)EPHEMERAL_SEQUENTIAL (temporary automatic numbering node)</code></pre></li></ul><h2 id="install-and-configure-zookeeper"><a href="#install-and-configure-zookeeper" class="headerlink" title="install and configure zookeeper"></a>install and configure zookeeper</h2><ul><li>download &gt; extract &gt; configure &gt; start/stop &gt; test</li><li>configure<pre><code>root@ubuntu:~/app/zookeeper# lltotal 1596drwxr-xr-x 10 1001 1001    4096 Mar 23  2017 ./drwxr-xr-x 13 root root    4096 Apr 23 23:50 ../drwxr-xr-x  2 1001 1001    4096 Mar 23  2017 bin/-rw-rw-r--  1 1001 1001   84725 Mar 23  2017 build.xmldrwxr-xr-x  2 1001 1001    4096 Mar 23  2017 conf/drwxr-xr-x 10 1001 1001    4096 Mar 23  2017 contrib/drwxr-xr-x  2 1001 1001    4096 Mar 23  2017 dist-maven/drwxr-xr-x  6 1001 1001    4096 Mar 23  2017 docs/-rw-rw-r--  1 1001 1001    1709 Mar 23  2017 ivysettings.xml-rw-rw-r--  1 1001 1001    5691 Mar 23  2017 ivy.xmldrwxr-xr-x  4 1001 1001    4096 Mar 23  2017 lib/-rw-rw-r--  1 1001 1001   11938 Mar 23  2017 LICENSE.txt-rw-rw-r--  1 1001 1001    3132 Mar 23  2017 NOTICE.txt-rw-rw-r--  1 1001 1001    1770 Mar 23  2017 README_packaging.txt-rw-rw-r--  1 1001 1001    1585 Mar 23  2017 README.txtdrwxr-xr-x  5 1001 1001    4096 Mar 23  2017 recipes/drwxr-xr-x  8 1001 1001    4096 Mar 23  2017 src/-rw-rw-r--  1 1001 1001 1456729 Mar 23  2017 zookeeper-3.4.10.jar-rw-rw-r--  1 1001 1001     819 Mar 23  2017 zookeeper-3.4.10.jar.asc-rw-rw-r--  1 1001 1001      33 Mar 23  2017 zookeeper-3.4.10.jar.md5-rw-rw-r--  1 1001 1001      41 Mar 23  2017 zookeeper-3.4.10.jar.sha1#==============================-rw-rw-r--  1 1001 1001  535 Mar 23  2017 configuration.xsl-rw-rw-r--  1 1001 1001 2161 Mar 23  2017 log4j.properties-rw-rw-r--  1 1001 1001  922 Mar 23  2017 zoo_sample.cfgroot@ubuntu:~/app/zookeeper/conf# cp zoo_sample.cfg zoo.cfgroot@ubuntu:~/app/zookeeper/conf# lltotal 24drwxr-xr-x  2 1001 1001 4096 Apr 24 00:17 ./drwxr-xr-x 10 1001 1001 4096 Mar 23  2017 ../-rw-rw-r--  1 1001 1001  535 Mar 23  2017 configuration.xsl-rw-rw-r--  1 1001 1001 2161 Mar 23  2017 log4j.properties-rw-r--r--  1 root root  922 Apr 24 00:17 zoo.cfg-rw-rw-r--  1 1001 1001  922 Mar 23  2017 zoo_sample.cfg#------------------------------vim zoo.cfgmkdir -p /root/app/zookeeper/zookdata# dataDir=/tmp/zookeeperdataDir=/root/app/zookeeper/zookdata# append the following:server.1=192.168.231.150:2888:3888#------------------------------root@ubuntu:~/app/zookeeper/zookdata# pwd/root/app/zookeeper/zookdataroot@ubuntu:~/app/zookeeper/zookdata# touch myid &amp;&amp; echo 1 &gt; myidroot@ubuntu:~/app/zookeeper/zookdata# cat myid1#==============================scp the zookeeper to other serverreset myid file in zookeeper on other server#==============================</code></pre></li></ul><h2 id="start-stop-zookeeper"><a href="#start-stop-zookeeper" class="headerlink" title="start/stop zookeeper"></a>start/stop zookeeper</h2><ul><li><p>start zookeeper</p><pre><code>root@ubuntu:~/app/zookeeper/bin# lltotal 52drwxr-xr-x  2 1001 1001 4096 Apr 24 00:35 ./drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../-rwxr-xr-x  1 1001 1001  232 Mar 23  2017 README.txt*-rwxr-xr-x  1 1001 1001 1937 Mar 23  2017 zkCleanup.sh*-rwxr-xr-x  1 1001 1001 1056 Mar 23  2017 zkCli.cmd*-rwxr-xr-x  1 1001 1001 1534 Mar 23  2017 zkCli.sh*-rwxr-xr-x  1 1001 1001 1628 Mar 23  2017 zkEnv.cmd*-rwxr-xr-x  1 1001 1001 2696 Mar 23  2017 zkEnv.sh*-rwxr-xr-x  1 1001 1001 1089 Mar 23  2017 zkServer.cmd*-rwxr-xr-x  1 1001 1001 6773 Mar 23  2017 zkServer.sh*-rw-r--r--  1 root root 5056 Apr 24 00:35 zookeeper.out#------------------------------Using config: /root/app/zookeeper/bin/../conf/zoo.cfgUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}#------------------------------root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh startzookeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED#------------------------------root@ubuntu:~/app/zookeeper/bin# jps38113 Jps34545 HQuorumPeer34757 HRegionServer12550 NodeManager12408 ResourceManager12024 DataNode34618 HMaster12235 SecondaryNameNode11852 NameNode#------------------------------root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh statuszookeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: standalone</code></pre></li><li><p>use zookeeper</p><pre><code>root@ubuntu:~/app/zookeeper/bin# ./zkCli.shConnecting to localhost:2181...2018-04-24 00:40:32,972 [myid:] - INFO  [main:Environment@100] - Client...2018-04-24 00:40:32,984 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2018-04-24 00:40:32,984 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp...WatchedEvent state:SyncConnected type:None path:null#------------------------------[zk: localhost:2181(CONNECTED) 0] helpzookeeper -server host:port cmd args  stat path [watch]  set path data [version]  ls path [watch]  delquota [-n|-b] path  ls2 path [watch]  setAcl path acl  setquota -n|-b val path  history  redo cmdno  printwatches on|off  delete path [version]  sync path  listquota path  rmr path  get path [watch]  create [-s] [-e] path data acl  addauth scheme auth  quit  getAcl path  close  connect host:port#------------------------------[zk: localhost:2181(CONNECTED) 1] create /data_test &#39;data_test&#39;       Created /data_test[zk: localhost:2181(CONNECTED) 2] ls /[data_test, zookeeper, hbase][zk: localhost:2181(CONNECTED) 3] get /data_testdata_testcZxid = 0x75ctime = Tue Apr 24 00:42:11 PDT 2018mZxid = 0x75mtime = Tue Apr 24 00:42:11 PDT 2018pZxid = 0x75cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 0#------------------------------[zk: localhost:2181(CONNECTED) 4] create /data_test/dir_2 &#39;123_value&#39;Created /data_test/dir_2[zk: localhost:2181(CONNECTED) 5] get /data_testdata_testcZxid = 0x75ctime = Tue Apr 24 00:42:11 PDT 2018mZxid = 0x75mtime = Tue Apr 24 00:42:11 PDT 2018pZxid = 0x76cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 1#------------------------------[zk: localhost:2181(CONNECTED) 6] quitQuitting...2018-04-24 00:47:19,034 [myid:] - INFO  [main:zookeeper@684] - Session: 0x162f5c0c60f0007 closed2018-04-24 00:47:19,037 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x162f5c0c60f0007</code></pre></li></ul><h2 id="view-the-log"><a href="#view-the-log" class="headerlink" title="view the log"></a>view the log</h2><ul><li>where is the log?<pre><code>root@ubuntu:~/app/zookeeper/bin# lltotal 52drwxr-xr-x  2 1001 1001 4096 Apr 24 00:35 ./drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../-rwxr-xr-x  1 1001 1001  232 Mar 23  2017 README.txt*-rwxr-xr-x  1 1001 1001 1937 Mar 23  2017 zkCleanup.sh*...-rw-r--r--  1 root root 5056 Apr 24 00:35 zookeeper.out</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes the basic working principle and installation process of zookeeper.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
      <category term="zookdata" scheme="http://yoursite.com/tags/zookdata/"/>
    
  </entry>
  
  <entry>
    <title>pseudo-distributed hbase configuration</title>
    <link href="http://yoursite.com/2018/04/24/pseudo-distributed%20hbase%20configuration/"/>
    <id>http://yoursite.com/2018/04/24/pseudo-distributed hbase configuration/</id>
    <published>2018-04-24T03:56:31.000Z</published>
    <updated>2018-05-01T13:16:37.989Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes how to build a pseudo-distributed hbase on a virtual machine.</p><a id="more"></a><h2 id="preconditions"><a href="#preconditions" class="headerlink" title="preconditions"></a>preconditions</h2><ul><li>jdk environment<pre><code>root@ubuntu:~# echo $JAVA_HOME/root/app/jdk1.8.0_171</code></pre></li><li><p>a pseudo-distributed hadoop</p><pre><code>root@ubuntu:~/app/hadoop/sbin# jps12550 NodeManager34152 Jps12408 ResourceManager12024 DataNode12235 SecondaryNameNode11852 NameNode</code></pre></li><li><p>test</p><pre><code>http://localhost:50070http://192.168.231.150:8099http://192.168.231.150:8042</code></pre></li></ul><h2 id="configure-hbase"><a href="#configure-hbase" class="headerlink" title="configure hbase"></a>configure hbase</h2><ul><li>download hbase from <a href="http://mirror.bit.edu.cn/apache/" target="_blank" rel="noopener">apache mirrors</a></li><li>extract files from hbase-2.0.0-beta-2-bin.tar.gz</li><li>configure xml files<pre><code>hbase-env.shhbase-site.xmlregionservers#================================root@ubuntu:~/app/hbase/conf# pwd/root/app/hbase/conf#================================root@ubuntu:~/app/hbase/conf# lltotal 48drwxr-xr-x 2 root root 4096 Apr 23 20:15 ./drwxr-xr-x 8 root root 4096 Apr 23 20:17 ../-rw-r--r-- 1 root root 1811 Dec 26  2015 hadoop-metrics2-hbase.properties-rw-r--r-- 1 root root 4537 Jan 28  2016 hbase-env.cmd-rw-r--r-- 1 root root 7537 Apr 23 20:12 hbase-env.sh-rw-r--r-- 1 root root 2257 Dec 26  2015 hbase-policy.xml-rw-r--r-- 1 root root 1355 Apr 23 20:09 hbase-site.xml-rw-r--r-- 1 root root 4603 May 28  2017 log4j.properties-rw-r--r-- 1 root root   16 Apr 23 20:15 regionservers#================================vim hbase-env.sh# The java implementation to use.  Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/export JAVA_HOME=/root/app/jdk1.8.0_171#--------------------------------# Tell HBase whether it should manage it&#39;s own instance of Zookeeper or not.# export HBASE_MANAGES_ZK=trueexport HBASE_MANAGES_ZK=true#================================vim regionserversroot@ubuntu:~/app/hbase/conf# cat regionservers192.168.231.150#================================vim hbase-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;hbase.rootdir&lt;/name&gt;      &lt;value&gt;hdfs://192.168.231.150:9000/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;  &lt;value&gt;192.168.231.150&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;      &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre></li></ul><h2 id="run-stop-hbase"><a href="#run-stop-hbase" class="headerlink" title="run/stop hbase"></a>run/stop hbase</h2><ul><li>run hbase<pre><code>root@ubuntu:~/app/hbase/bin# ./start-hbase.shlocalhost: starting zookeeper, logging to /root/app/hbase/bin/../logs/hbase-root-zookeeper-ubuntu.outstarting master, logging to /root/app/hbase/bin/../logs/hbase-root-master-ubuntu.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0192.168.231.150: starting regionserver, logging to /root/app/hbase/bin/../logs/hbase-root-regionserver-ubuntu.out192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0root@ubuntu:~/app/hbase/bin# jps34545 HQuorumPeer34757 HRegionServer12550 NodeManager12408 ResourceManager12024 DataNode34618 HMaster12235 SecondaryNameNode11852 NameNode35053 Jps</code></pre></li><li>test hbase<pre><code>http://192.168.231.150:16010</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes how to build a pseudo-distributed hbase on a virtual machine.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>how to download 1080p videos from youtube</title>
    <link href="http://yoursite.com/2018/04/23/how%20to%20download%201080p%20videos%20from%20youtube/"/>
    <id>http://yoursite.com/2018/04/23/how to download 1080p videos from youtube/</id>
    <published>2018-04-23T12:36:16.000Z</published>
    <updated>2018-05-01T13:16:37.986Z</updated>
    
    <content type="html"><![CDATA[<p>the article describes how to download 1080p videos form youtube with your foreigen vps/ecs.</p><a id="more"></a><h2 id="general-method"><a href="#general-method" class="headerlink" title="general method"></a>general method</h2><ul><li>chrome browser</li><li>copy the link of the video from youtube</li><li>paste the link to the input box of <a href="https://en.savefrom.net/" target="_blank" rel="noopener">https://en.savefrom.net/</a></li><li>download videos though chrome</li></ul><h2 id="advance-method"><a href="#advance-method" class="headerlink" title="advance method"></a>advance method</h2><ul><li>login in your foreign vps</li><li>such as: ubuntu from digitalocean</li><li>use the following cmd to download the video<pre><code>apt-get install youtube-dl -yyoutube-dl -f 22 your_video_link</code></pre></li></ul><h2 id="ultimate-method"><a href="#ultimate-method" class="headerlink" title="ultimate method"></a>ultimate method</h2><ul><li><p>use the following cmd to analysis all videos and audios about your_video_link</p><pre><code>youtube-dl -F your_video_link</code></pre></li><li><p>use the following cmd to download videos and audios with the specified code</p><pre><code>youtube-dl -f 10 your_video_link</code></pre></li><li><p>install the tools of video and audio</p><pre><code>apt-get install ffmpeg -y</code></pre></li><li><p>merge the video and audio</p><pre><code>ffmpeg -i /tmp/a.wav -i /tmp/a.avi /tmp/out.avi</code></pre></li></ul><h2 id="how-to-download-playlist-from-youtube"><a href="#how-to-download-playlist-from-youtube" class="headerlink" title="how to download playlist from youtube"></a>how to download playlist from youtube</h2><ul><li>cmd<pre><code>youtube-dl -citk –format mp4 –yes-playlist VIDEO_PLAYLIST_LINKyoutube-dl -citk –format mp4 –yes-playlist https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfuayoutube-dl -cit &quot;https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfua&quot;</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;the article describes how to download 1080p videos form youtube with your foreigen vps/ecs.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="vps" scheme="http://yoursite.com/tags/vps/"/>
    
      <category term="youtube" scheme="http://yoursite.com/tags/youtube/"/>
    
  </entry>
  
  <entry>
    <title>usage of scp</title>
    <link href="http://yoursite.com/2018/04/23/usage%20of%20scp/"/>
    <id>http://yoursite.com/2018/04/23/usage of scp/</id>
    <published>2018-04-23T11:48:40.000Z</published>
    <updated>2018-05-01T13:16:37.990Z</updated>
    
    <content type="html"><![CDATA[<p>this article describes how download/upload files from server using scp command line.</p><a id="more"></a><h2 id="usage-of-scp"><a href="#usage-of-scp" class="headerlink" title="usage of scp"></a>usage of scp</h2><ol><li><p>download file from server</p><pre><code>scp root@servername:/path/filename /tmp/local_destinationscp root@192.168.0.101:/home/kimi/test.txt /home/kimi/test.txt</code></pre></li><li><p>upload file to server</p><pre><code>scp /path/local_filename root@servername:/path  scp /var/www/test.php root@192.168.0.101:/var/www/</code></pre></li><li><p>download directory to client</p><pre><code>scp -r root@servername:remote_dir/ /tmp/local_dir scp -r root@192.168.0.101:/home/kimi/test /tmp/local_dir</code></pre></li><li><p>upload directory to server</p><pre><code>scp -r /tmp/local_dir root@servername:remote_dirscp -P 22 -r test root@192.168.0.101:/var/www/</code></pre></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;this article describes how download/upload files from server using scp command line.&lt;/p&gt;
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
      <category term="cmd" scheme="http://yoursite.com/tags/cmd/"/>
    
  </entry>
  
  <entry>
    <title>write wordcount program on eclipse and run on hadoop in win10</title>
    <link href="http://yoursite.com/2018/04/23/write%20wordcount%20program%20on%20eclipse%20and%20run%20on%20hadoop%20in%20win10/"/>
    <id>http://yoursite.com/2018/04/23/write wordcount program on eclipse and run on hadoop in win10/</id>
    <published>2018-04-23T06:36:16.000Z</published>
    <updated>2018-05-01T13:16:37.990Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes how to write the wordcount program on eclipse and run it on local hadoop.</p><a id="more"></a><h2 id="prerequisites"><a href="#prerequisites" class="headerlink" title="prerequisites"></a><strong>prerequisites</strong></h2><ul><li><strong>server</strong></li></ul><ol><li>win10</li><li>hadoop 2.7.6</li></ol><ul><li><strong>client</strong></li></ul><ol><li>win10</li><li>eclipse neon</li><li>hadoop-eclipse-plugin-2.7.2.jar</li></ol><h2 id="create-project-and-coding"><a href="#create-project-and-coding" class="headerlink" title="create project and coding"></a><strong>create project and coding</strong></h2><ul><li><p><strong>create project</strong></p><pre><code>new &gt; other &gt; map reduce program &gt; fix the boxes with name, ${HADOOP_HOME} &gt; finish</code></pre></li><li><p><strong>coding</strong></p><pre><code>package com.hikvision.bigdata.hadoop.hadoop_wordcount;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.log4j.BasicConfigurator;/*** wordcount*/public class WordCount {  public static void main(String[] args) throws Exception {      BasicConfigurator.configure();      System.out.println(&quot;Hello World!&quot;);      Configuration conf = new Configuration();      @SuppressWarnings(&quot;deprecation&quot;)      Job job = new Job(conf, &quot;wordcount&quot;);      job.setJarByClass(WordCount.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);      job.setMapperClass(Map.class);      job.setReducerClass(Reduce.class);      job.setInputFormatClass(TextInputFormat.class);      job.setOutputFormatClass(TextOutputFormat.class);      FileInputFormat.addInputPath(job, new Path(args[0]));      FileOutputFormat.setOutputPath(job, new Path(args[1]));      job.waitForCompletion(true);  }  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {      private final static IntWritable one = new IntWritable(1);      private Text word = new Text();      public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String line = value.toString();          StringTokenizer tokenizer = new StringTokenizer(line);          while (tokenizer.hasMoreTokens()) {              word.set(tokenizer.nextToken());              context.write(word, one);          }      }  }  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {      public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)              throws IOException, InterruptedException {          int sum = 0;          for (IntWritable val : values) {              sum += val.get();          }          context.write(key, new IntWritable(sum));      }  }}</code></pre></li></ul><h2 id="configure-program"><a href="#configure-program" class="headerlink" title="configure program"></a><strong>configure program</strong></h2><ul><li><p><strong>configure hadoop</strong></p><pre><code>step 1:core-site.xml&lt;configuration&gt; &lt;property&gt;    &lt;name&gt;fs.default.name&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 2:hdfs-site.xml&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;dfs.name.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.data.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;    &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 3:mapred-site.xml&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;    &lt;value&gt;localhost:10020&lt;/value&gt;    &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 4:yarn-site.xml&lt;configuration&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;        &lt;value&gt;localhost:8099&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p><strong>format namenode</strong></p><pre><code>cd binhdfs namenode -format# format only once</code></pre></li><li><p><strong>start hadoop and upload data to hdfs</strong></p><pre><code>cd ${HADOOP_HOME}input cmd to open cmd linecd sbinstart-all.cmd##hadoop fs -mkdir /datahadoop fs -put D:\a.txt /data/a.txt</code></pre></li><li><strong>configure input and output path for program in eclipse</strong><pre><code>1 project name &gt; right clieck &gt; run as &gt; run configuration &gt; java application &gt; new2 fix the boxes with the program name, main class, run name, and arguments3 the arguments as follows:hdfs://localhost:9000/data/a.txt hdfs://localhost:9000/data/output</code></pre></li></ul><h2 id="run-program"><a href="#run-program" class="headerlink" title="run program"></a><strong>run program</strong></h2><ul><li><strong>precondition</strong></li></ul><ol><li>delete the output floder on hdfs</li><li>data is ready</li><li>input/output path is configured in eclipse</li><li>hadoop is running</li></ol><ul><li><strong>run program</strong><pre><code>project name &gt; right clieck &gt; run as &gt; run on hadoop &gt; select the main class &gt; enjoy the ouput</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes how to write the wordcount program on eclipse and run it on local hadoop.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="coding" scheme="http://yoursite.com/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>configure hadoop on ubuntu and connect to eclipse on ubuntu or win10</title>
    <link href="http://yoursite.com/2018/04/22/configure%20hadoop%20on%20ubuntu%20and%20connect%20to%20eclipse%20on%20ubuntu%20or%20win10/"/>
    <id>http://yoursite.com/2018/04/22/configure hadoop on ubuntu and connect to eclipse on ubuntu or win10/</id>
    <published>2018-04-22T04:49:06.000Z</published>
    <updated>2018-05-01T13:16:37.977Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes how to configure hadoop2.7.6 on ubuntu and connect to hadoop2.7.6 using eclipse on ubuntu and win10 respectively.</p><a id="more"></a><h2 id="prerequisites"><a href="#prerequisites" class="headerlink" title="prerequisites"></a><strong>prerequisites</strong></h2><ul><li><strong>server</strong></li></ul><ol><li>win10</li><li>vmware workstation pro 12</li><li>Ubuntu 16.04.4 LTS</li><li>hadoop 2.7.6 <a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">download</a></li></ol><ul><li><strong>ubuntu clinet</strong></li></ul><ol><li>eclipse neon <a href="http://www.eclipse.org/downloads/packages/release/Neon/3" target="_blank" rel="noopener">download</a></li><li>hadoop-eclipse-plugin-2.7.2.jar <a href="https://download.csdn.net/download/tondayong1981/9432425" target="_blank" rel="noopener">download</a></li></ol><ul><li><strong>win10 client</strong></li></ul><ol><li>eclipse neon <a href="http://www.eclipse.org/downloads/packages/release/Neon/3" target="_blank" rel="noopener">download</a></li><li>hadoop-eclipse-plugin-2.7.2.jar <a href="https://download.csdn.net/download/tondayong1981/9432425" target="_blank" rel="noopener">download</a></li></ol><h2 id="pretreatment-for-ubuntu-virtual-machine"><a href="#pretreatment-for-ubuntu-virtual-machine" class="headerlink" title="pretreatment for ubuntu virtual machine"></a><strong>pretreatment for ubuntu virtual machine</strong></h2><ul><li><strong>enable the user-passwd input box on the login screen</strong><pre><code>sudo passwd rootsu rootcd /usr/share/lightdm/lightdm.conf.d/vim 50-unity-greeter.conf# adduser-session=ubuntugreeter-show-manual-login=trueall-guest=false# rebootreboot# login in ubuntu with root and get a error reportvim /root/.profile# locate tomesg n || true# change totty -s &amp;&amp; mesg n || true</code></pre></li><li><strong>configure jdk for ubuntu</strong></li><li>jdk 1.8 <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">download</a><pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binsource /etc/profilejava -version</code></pre></li></ul><h2 id="configure-hadoop2-7-6"><a href="#configure-hadoop2-7-6" class="headerlink" title="configure hadoop2.7.6"></a><strong>configure hadoop2.7.6</strong></h2><ul><li><p><strong>append <em>JAVA_HOME</em> to stat script</strong></p><pre><code>step 1:vim /root/app/hadoop/etc/hadoop/hadoop-env.sh# export JAVA_HOME=${JAVA_HOME}export JAVA_HOME=/root/app/jdk1.8.0_171###########################################step 2:vim /root/app/hadoop/etc/hadoop/yarn-env.sh# some Java parameters# export JAVA_HOME=/home/y/libexec/jdk1.6.0/export JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p><strong>configure core-site.xml, hdfs-site.xml,  mapred-site.xml, yarn-site.xml</strong></p><pre><code>step 1:vim /root/app/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;   &lt;property&gt;      &lt;name&gt;fs.default.name&lt;/name&gt;      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 2:vim /root/app/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;dfs.name.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.data.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt;  &lt;/property&gt; &lt;property&gt;      &lt;name&gt;dfs.permissions&lt;/name&gt;      &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;      &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 3:vim /root/app/hadoop/etc/hadoop/mapred-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;      &lt;value&gt;yarn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;      &lt;value&gt;localhost:10020&lt;/value&gt;      &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 4:vim /root/app/hadoop/etc/hadoop/yarn-site.xml&lt;configuration&gt;  &lt;property&gt;          &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;          &lt;value&gt;mapreduce_shuffle&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;          &lt;value&gt;localhost:8099&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p><strong>create floder for hadoop work directory</strong></p><pre><code>cd /root/app/hadoopmkdir hdfs -p hdfs/data hdfs/namemkdir tmp</code></pre></li><li><p><strong>format hdfs and start/stop hadoop</strong></p></li></ul><ol><li>format hdfs<pre><code># keep all hadoop process stopped./sbin/stop-all.sh# remove tmp directoryrm -rdf tmp/# format hdfs only oncebin/hdfs namenode -format# see if the format is successfultree hdfs/</code></pre></li><li>start/stop hadoop<pre><code># start hadoop./sbin/start-all.shroot@ubuntu:~/app/hadoop# jps63809 Jps63474 NodeManager62950 DataNode63335 ResourceManager62775 NameNode63163 SecondaryNameNode# stop hadoop./sbin/stop-all.sh</code></pre></li></ol><h2 id="connect-to-hadoop-with-eclipse"><a href="#connect-to-hadoop-with-eclipse" class="headerlink" title="connect to hadoop with eclipse"></a><strong>connect to hadoop with eclipse</strong></h2><ul><li><strong>install plugin in eclipse on ubuntu</strong><pre><code>1 copy to hadoop-eclipse-plugin-2.7.2.jar to ${eclipse_home}/dropins;2 open eclipse;3 open menu &gt; windows &gt; show view &gt; other &gt; mapreduce tools &gt; map/reduce locations;4 map/reduce locations &gt; right click &gt; edit hadoop location;location name: XXXmap/reduce master:  host: localhost  port: 50020dfs master:  host: localhost  port: 90005 open dfs locations, you will find the file in hdfs.</code></pre></li><li><strong>install plugin in eclipse on win10</strong><pre><code>1 on win10, your eclipse serves as a clinet, you can connect your server with ip, so you should firstly replace *localhost* with your server ip in all etc files, such as core-site.xml, hdfs-site.xml,  mapred-site.xml, yarn-site.xml;2 repeat the step 1,2,3,4 above;3 map/reduce locations &gt; right click &gt; edit hadoop location &gt; advace parameters, replace *hadoop.tmp.dir* with your own address in /hdfs-site.xml;4 enjoy the local developing and the remote debuging.</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes how to configure hadoop2.7.6 on ubuntu and connect to hadoop2.7.6 using eclipse on ubuntu and win10 respectively.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="eclipse" scheme="http://yoursite.com/tags/eclipse/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
  </entry>
  
  <entry>
    <title>hexo构建博客搜索框加载中的解决方案</title>
    <link href="http://yoursite.com/2018/04/19/hexo%E6%9E%84%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%90%9C%E7%B4%A2%E6%A1%86%E5%8A%A0%E8%BD%BD%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://yoursite.com/2018/04/19/hexo构建博客搜索框加载中的解决方案/</id>
    <published>2018-04-19T06:37:10.000Z</published>
    <updated>2018-05-01T13:16:37.985Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。</p><a id="more"></a><h2 id="问题与现象"><a href="#问题与现象" class="headerlink" title="问题与现象"></a>问题与现象</h2><ul><li>nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。</li></ul><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><ul><li>开发的markdown中出现了非utf-8的字符。</li><li>访问可以查找错误出现的位置：<a href="https://leebin.top/search.xml" target="_blank" rel="noopener">https://leebin.top/search.xml</a></li></ul><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ul><li>逐个排查每个markdown文件，直到找到非utf-8字符，删除，重新部署，点击搜索框测试。</li><li>访问：<a href="https://leebin.top/search.xml" target="_blank" rel="noopener">https://leebin.top/search.xml</a> 发现可以解析成源文件。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>hadoop中wordcount程序开发</title>
    <link href="http://yoursite.com/2018/04/19/Hadoop%E4%B8%ADwordcount%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/"/>
    <id>http://yoursite.com/2018/04/19/Hadoop中wordcount程序开发/</id>
    <published>2018-04-19T05:09:22.000Z</published>
    <updated>2018-05-01T13:16:37.975Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何利用java和hadoop组件开发wordcount程序。</p><a id="more"></a><h2 id="开发与测试环境"><a href="#开发与测试环境" class="headerlink" title="开发与测试环境"></a>开发与测试环境</h2><ul><li>windows</li><li>eclipse</li><li>maven，常见的组件如下：</li></ul><ol><li>Apache Hadoop Common 3.1</li><li>Apache Hadoop Client Aggregator 3.1</li><li>Hadoop Core 1.2</li><li>Apache Hadoop HDFS 3.1</li><li>Apache Hadoop MapReduce Core 3.1</li></ol><ul><li>ubuntu中hadoop单机模式，搭建过程参考: <a href="https://leebin.top/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">如何hadoop单机版</a></li></ul><h2 id="添加依赖后maven报错"><a href="#添加依赖后maven报错" class="headerlink" title="添加依赖后maven报错"></a>添加依赖后maven报错</h2><ul><li><p>报错</p><pre><code>Buiding Hadoop with Eclipse / Maven - Missing artifact jdk.tools:jdk.tools:jar:1.6</code></pre></li><li><p>解决</p><pre><code># cmdC:\Users\BinLee&gt;java -versionjava version &quot;1.8.0_144&quot;Java(TM) SE Runtime Environment (build 1.8.0_144-b01)Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)# 添加下面的依赖到maven的pom.xml&lt;dependency&gt;  &lt;groupId&gt;jdk.tools&lt;/groupId&gt;  &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;  &lt;version&gt;1.8.0_144&lt;/version&gt;  &lt;scope&gt;system&lt;/scope&gt;  &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt;</code></pre></li></ul><h2 id="wordcount程序开发"><a href="#wordcount程序开发" class="headerlink" title="wordcount程序开发"></a>wordcount程序开发</h2><ul><li><p>pom.xml</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.jordiburgos&lt;/groupId&gt;  &lt;artifactId&gt;wordcount&lt;/artifactId&gt;  &lt;packaging&gt;jar&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;name&gt;wordcount&lt;/name&gt;  &lt;url&gt;http://jordiburgos.com&lt;/url&gt;  &lt;properties&gt;      &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;  &lt;/properties&gt;  &lt;repositories&gt;      &lt;repository&gt;          &lt;id&gt;cloudera&lt;/id&gt;          &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;      &lt;/repository&gt;  &lt;/repositories&gt;  &lt;dependencies&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;          &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;          &lt;version&gt;2.2.0&lt;/version&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;          &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;          &lt;version&gt;1.2.1&lt;/version&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;jdk.tools&lt;/groupId&gt;          &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;          &lt;version&gt;1.7&lt;/version&gt;          &lt;scope&gt;system&lt;/scope&gt;          &lt;systemPath&gt;${java.home}/../lib/tools.jar&lt;/systemPath&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;junit&lt;/groupId&gt;          &lt;artifactId&gt;junit&lt;/artifactId&gt;          &lt;version&gt;3.8.1&lt;/version&gt;          &lt;scope&gt;test&lt;/scope&gt;      &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;      &lt;plugins&gt;          &lt;plugin&gt;              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;              &lt;version&gt;2.4&lt;/version&gt;              &lt;executions&gt;                  &lt;execution&gt;                      &lt;id&gt;distro-assembly&lt;/id&gt;                      &lt;phase&gt;package&lt;/phase&gt;                      &lt;goals&gt;                          &lt;goal&gt;single&lt;/goal&gt;                      &lt;/goals&gt;                      &lt;configuration&gt;                          &lt;descriptors&gt;                              &lt;descriptor&gt;assembly.xml&lt;/descriptor&gt;                          &lt;/descriptors&gt;                      &lt;/configuration&gt;                  &lt;/execution&gt;              &lt;/executions&gt;          &lt;/plugin&gt;      &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre></li><li><p>wordcount.java</p><pre><code>package com.jordiburgos;import java.io.IOException;import java.util.*;import org.apache.hadoop.fs.Path;import org.apache.hadoop.conf.*;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.*;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;public class WordCount {  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {      private final static IntWritable one = new IntWritable(1);      private Text word = new Text();      public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String line = value.toString();          StringTokenizer tokenizer = new StringTokenizer(line);          while (tokenizer.hasMoreTokens()) {              word.set(tokenizer.nextToken());              context.write(word, one);          }      }  }  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {      public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)              throws IOException, InterruptedException {          int sum = 0;          for (IntWritable val : values) {              sum += val.get();          }          context.write(key, new IntWritable(sum));      }  }  public static void main(String[] args) throws Exception {      Configuration conf = new Configuration();      Job job = new Job(conf, &quot;wordcount&quot;);      job.setJarByClass(WordCount.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);      job.setMapperClass(Map.class);      job.setReducerClass(Reduce.class);      job.setInputFormatClass(TextInputFormat.class);      job.setOutputFormatClass(TextOutputFormat.class);      FileInputFormat.addInputPath(job, new Path(args[0]));      FileOutputFormat.setOutputPath(job, new Path(args[1]));      job.waitForCompletion(true);  }}</code></pre></li></ul><h2 id="使用maven打包程序"><a href="#使用maven打包程序" class="headerlink" title="使用maven打包程序"></a>使用maven打包程序</h2><ul><li><p>打包命令</p><pre><code>项目右键&gt;run as&gt;maven build</code></pre></li><li><p>打包后jar的结构</p><pre><code>C:.└─wordcount  ├─com  │  └─jordiburgos  └─META-INF      └─maven          └─com.jordiburgos              └─wordcount</code></pre></li></ul><h2 id="在hadoop上运行程序"><a href="#在hadoop上运行程序" class="headerlink" title="在hadoop上运行程序"></a>在hadoop上运行程序</h2><ul><li><p>上传待分析的文本到hdfs</p><pre><code># 本地创建input文件夹和a.txt文件cd /root/app/hadoop-3.1.0mkdir inputvim a.txt## 创建文件夹hadoop fs -mkdir hdfs://localhost:9001/tmp## 上传文件到hdfshadoop fs -put /root/app/hadoop-3.1.0/input hdfs://127.0.0.1:9001/tmp</code></pre></li><li><p>运行jar程序</p><pre><code>cd /root/app/hadoop-3.1.0bin/hadoop jar wordcount.jar com.jordiburgos.WordCount hdfs://localhost:9001/tmp/input/ file:///root/app/hadoop-3.1.0/output/</code></pre></li><li><p>在linux中查看输出文件</p><pre><code>cd /root/app/hadoop-3.1.0/outputroot@ubuntu:~/app/hadoop-3.1.0/output# lspart-r-00000  _SUCCESSroot@ubuntu:~/app/hadoop-3.1.0/output# cat part-r-000000000    1aaaa    1ddfh    1ff    1ggg    1hj    1iiiii    1sss    1</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何利用java和hadoop组件开发wordcount程序。&lt;/p&gt;
    
    </summary>
    
      <category term="开发" scheme="http://yoursite.com/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="maven" scheme="http://yoursite.com/tags/maven/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu中hadoop单机模式和伪分布式搭建</title>
    <link href="http://yoursite.com/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/04/18/ubuntu中hadoop单机模式和伪分布式搭建/</id>
    <published>2018-04-18T05:09:22.000Z</published>
    <updated>2018-05-01T13:16:37.989Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何搭建hadoop单机版本/独立模式/standalone模式？<br><a id="more"></a></p><h2 id="ubuntu开启root用户登录的方法"><a href="#ubuntu开启root用户登录的方法" class="headerlink" title="ubuntu开启root用户登录的方法"></a>ubuntu开启root用户登录的方法</h2><ul><li>设置密码、添加信息<pre><code>sudo passwd -u rootsudo passwd rootsu rootcd /usr/share/lightdm/lightdm.conf.d/vim 50-unity-greeter.conf# 添加user-session=ubuntugreeter-show-manual-login=trueall-guest=false# 重启reboot# 使用user和passwd进入root报错vim /root/.profile# 找到mesg n || true# 改为tty -s &amp;&amp; mesg n || true</code></pre></li></ul><h2 id="ubuntu中的java环境变量配置"><a href="#ubuntu中的java环境变量配置" class="headerlink" title="ubuntu中的java环境变量配置"></a>ubuntu中的java环境变量配置</h2><ul><li>编辑 sudo vim /etc/profile<pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binsource /etc/profile</code></pre></li><li>验证<pre><code>java -version</code></pre></li></ul><h2 id="单机版hadoop配置"><a href="#单机版hadoop配置" class="headerlink" title="单机版hadoop配置"></a>单机版hadoop配置</h2><ul><li><p><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation" target="_blank" rel="noopener">官方文档</a></p></li><li><p>生成ssh密钥</p><pre><code>cd ~ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keysssh localhost</code></pre></li><li><p>配置java环境</p><pre><code>root@ubuntu:~# vim /etc/profileexport JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</code></pre></li><li><p>配置hadoop环境 vim /etc/profile</p><pre><code>#HADOOP VARIABLES STARTexport JAVA_HOME=/root/app/jdk1.8.0_171export HADOOP_HOME=/root/app/hadoop-3.1.0export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#HADOOP VARIABLES ENDsource /etc/profile</code></pre></li><li><p>单机版测试</p><pre><code>root@ubuntu:~# /root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jarAn example program must be given as the first argument.Valid program names are:aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.dbcount: An example job that count the pageview counts from a database.distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.grep: A map/reduce program that counts the matches of a regex in the input.join: A job that effects a join over sorted, equally partitioned datasetsmultifilewc: A job that counts words from several files.pentomino: A map/reduce tile laying program to find solutions to pentomino problems.pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.randomwriter: A map/reduce program that writes 10GB of random data per node.secondarysort: An example defining a secondary sort to the reduce.sort: A map/reduce program that sorts the data written by the random writer.sudoku: A sudoku solver.teragen: Generate data for the terasortterasort: Run the terasortteravalidate: Checking results of terasortwordcount: A map/reduce program that counts the words in the input files.wordmean: A map/reduce program that counts the average length of the words in the input files.wordmedian: A map/reduce program that counts the median length of the words in the input files.wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.</code></pre></li><li><p>实例测试</p><pre><code>/root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar grep ./input ./output &#39;dfs[a-z.]+&#39;cat ./output/*  # 查看结果rm -r ./output  # 删除结果# 结果root@ubuntu:~/app/hadoop-3.1.0# cat ./output/*  1    dfsadmin</code></pre></li></ul><h2 id="伪分布式hadoop配置"><a href="#伪分布式hadoop配置" class="headerlink" title="伪分布式hadoop配置"></a>伪分布式hadoop配置</h2><ul><li><p>格式化hdfs</p><pre><code>cd /root/app/hadoop-3.1.0./bin/hdfs namenode -format</code></pre></li><li><p>添加变量到 vim /etc/profile</p><pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin## HADOOP VARIABLES STARTexport JAVA_HOME=/root/app/jdk1.8.0_171export HADOOP_HOME=/root/app/hadoop-3.1.0#export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin#export HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOME#export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#export HDFS_DATANODE_USER=rootexport HDFS_NAMENODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=root#export YARN_RESOURCEMANAGER_USER=rootexport HADOOP_SECURE_DN_USER=yarnexport YARN_NODEMANAGER_USER=root# HADOOP VARIABLES END</code></pre></li><li><p>编辑/root/app/hadoop-3.1.0/etc/hadoop/core-site.xml</p><pre><code>&lt;configuration&gt;     &lt;property&gt;          &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;          &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;     &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>编辑/root/app/hadoop-3.1.0/etc/hadoop/hdfs-site.xml</p><pre><code>&lt;configuration&gt;      &lt;property&gt;           &lt;name&gt;dfs.replication&lt;/name&gt;           &lt;value&gt;2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;           &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/name&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;           &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/data&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.http.address&lt;/name&gt;          &lt;value&gt;0.0.0.0:50070&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>编辑 hadoop-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/hadoop-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p>编辑 yarn-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/yarn-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p>编辑 mapred-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/mapred-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li></ul><h2 id="hadoop的使用"><a href="#hadoop的使用" class="headerlink" title="hadoop的使用"></a>hadoop的使用</h2><ul><li><p>启动与停止</p><pre><code>cd /root/app/hadoop-3.1.0./sbin/start-all.sh./sbin/stop-all.sh</code></pre></li><li><p>查看服务</p><pre><code>root@ubuntu:~/app/hadoop-3.1.0# jps23058 NameNode23491 SecondaryNameNode23753 ResourceManager23225 DataNode24427 Jps24030 NodeManager</code></pre></li><li>Resource Manager <a href="http://localhost:8088" target="_blank" rel="noopener">http://localhost:8088</a></li><li>Web UI of the NameNode daemon <a href="http://localhost:50070" target="_blank" rel="noopener">http://localhost:50070</a></li><li>HDFS NameNode web interface <a href="http://localhost:8042" target="_blank" rel="noopener">http://localhost:8042</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何搭建hadoop单机版本/独立模式/standalone模式？&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
</feed>
