<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>初心</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-30T06:23:51.170Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LeeBin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PE hadoop 2.x hdfs availablity implement</title>
    <link href="http://yoursite.com/2018/04/30/PE%20hadoop%202.x%20hdfs%20availablity%20implement/"/>
    <id>http://yoursite.com/2018/04/30/PE hadoop 2.x hdfs availablity implement/</id>
    <published>2018-04-30T06:08:31.000Z</published>
    <updated>2018-04-30T06:23:51.170Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何在hdfs上实现namenode集群的高可用。</p><a id="more"></a><h2 id="集群配置需求"><a href="#集群配置需求" class="headerlink" title="集群配置需求"></a>集群配置需求</h2><ul><li>集群配置地点<pre><code>  NN  DN  JN  ZK  ZKFCn1  1           1   1n2  1   1   1   1   1n3      1   1   1   n4      1   1</code></pre></li></ul><h2 id="配置HDFS"><a href="#配置HDFS" class="headerlink" title="配置HDFS"></a>配置HDFS</h2><ul><li><p>hdfs-site.xml</p><pre><code>vim /root/app/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;      &lt;!-- 配置NN空间 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.nameservices&lt;/name&gt;              &lt;value&gt;sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.namenodes.sxt&lt;/name&gt;              &lt;value&gt;nn1,nn2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.rpc-address.sxt.nn1&lt;/name&gt;              &lt;value&gt;n1:8020&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.rpc-address.sxt.nn2&lt;/name&gt;              &lt;value&gt;n2:8020&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.http-address.sxt.nn1&lt;/name&gt;              &lt;value&gt;n1:50070&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.http-address.sxt.nn2&lt;/name&gt;              &lt;value&gt;n2:50070&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置JN能处理的Node，可以理解为DN --&gt;      &lt;property&gt;              &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;              &lt;value&gt;qjournal://n2:8485;n3:8485;n4:8485/sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.client.failover.proxy.provider.sxt&lt;/name&gt;              &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置密钥 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;              &lt;value&gt;sshfence&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;              &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置JN的临时文件夹 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;              &lt;value&gt;/opt/journal/node/data&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 单节点故障自动迁移 --&gt;      &lt;property&gt;              &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;              &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>core-site.xml</p><pre><code>vim /root/app/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;      &lt;property&gt;              &lt;name&gt;fs.defaultFS&lt;/name&gt;              &lt;value&gt;hdfs://sxt&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;              &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;              &lt;value&gt;/opt/hadoop&lt;/value&gt;      &lt;/property&gt;      &lt;!-- 配置Quorum Journal Manager的zk集群，JN管理集群 --&gt;      &lt;property&gt;              &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;              &lt;value&gt;n1:2181,n2:2181,n3:2181&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li></ul><h2 id="配置zookeeper"><a href="#配置zookeeper" class="headerlink" title="配置zookeeper"></a>配置zookeeper</h2><ul><li><p>环境变量</p><pre><code>cat /etc/profileexport ANT_HOME=/root/app/antexport HADOOP_HOME=/root/app/hadoopexport JAVA_HOME=/root/app/jdkexport JRE_HOME=/root/app/jdk/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$ANT_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/app/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin注意只有前面三台的zk路径加入环境变量</code></pre></li><li><p>配置文件</p><pre><code>cat /root/app/zookeeper/conf/zoo.cfg# The number of milliseconds of each tick# dataDir=/tmp/zookeeperdataDir=/opt/zookeeperserver.1=n1:2888:3888server.2=n2:2888:3888server.3=n3:2888:3888</code></pre></li><li><p>分别配置/opt/zookeeper目录，创建myid文件，加入1, 2, 3</p></li><li><p>同时启动zk</p><pre><code>zkServer.sh start</code></pre></li><li><p>查看状态</p><pre><code>zkServer.sh status</code></pre></li><li><p>启动输出</p><pre><code>root@n1:~# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower</code></pre></li><li><p>n1拷贝zookeeper到n2, n3, n4</p><pre><code>scp -r ./dir n1:`pwd`</code></pre></li></ul><h2 id="启动ZK"><a href="#启动ZK" class="headerlink" title="启动ZK"></a>启动ZK</h2><ul><li>启动并查看状态（1, 2, 3同时启动）<pre><code>zkServer.sh startzkServer.sh status</code></pre></li></ul><h2 id="启动JN"><a href="#启动JN" class="headerlink" title="启动JN"></a>启动JN</h2><ul><li><p>在n2. n3, n4启动JN</p><pre><code>hadoop-daemon.sh start journalnode</code></pre></li><li><p>输出</p><pre><code>root@n2:~# hadoop-daemon.sh start journalnodestarting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outroot@n2:~# jps1751 QuorumPeerMain1895 JournalNode1945 Jps</code></pre></li></ul><h2 id="启动NN，高可用HA操作"><a href="#启动NN，高可用HA操作" class="headerlink" title="启动NN，高可用HA操作"></a>启动NN，高可用HA操作</h2><ul><li><p>在一台NN格式化（NN:n1）</p><pre><code>hdfs namenode -format</code></pre></li><li><p>在没有格式化的另外一台hadoop执行standby操作（NN:n2）</p><pre><code>hdfs namenode -bootstrapStandby</code></pre></li><li><p>报错提示n1没有启动namenode，先启动namenode</p><pre><code>hadoop-daemon.sh start namenode</code></pre></li><li><p>在此执行standby成功（格式化+启动n1，standby另外n2）</p><pre><code>18/04/29 11:14:41 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/04/29 11:14:41 INFO namenode.NameNode: createNameNode [-bootstrapStandby]#=====================================================About to bootstrap Standby ID nn2 from:         Nameservice ID: sxt      Other Namenode ID: nn1Other NN&#39;s HTTP address: http://n1:50070Other NN&#39;s IPC  address: n1/192.168.44.100:8020           Namespace ID: 1765158274          Block pool ID: BP-1441163464-192.168.44.100-1525025632122             Cluster ID: CID-2e601647-294c-4e70-8e72-7a82bea94fa9         Layout version: -63     isUpgradeFinalized: true#=====================================================18/04/29 11:14:42 INFO common.Storage: Storage directory /opt/hadoop/dfs/name has been successfully formatted.18/04/29 11:14:43 INFO namenode.TransferFsImage: Opening connection to http://n1:50070/imagetransfer?getimage=1&amp;txid=0&amp;storageInfo=-63:1765158274:0:CID-2e601647-294c-4e70-8e72-7a82bea94fa918/04/29 11:14:43 INFO namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds18/04/29 11:14:43 INFO namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s18/04/29 11:14:43 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 321 bytes.18/04/29 11:14:43 INFO util.ExitUtil: Exiting with status 018/04/29 11:14:43 INFO namenode.NameNode: SHUTDOWN_MSG: SHUTDOWN_MSG: Shutting down NameNode at n2/192.168.44.101</code></pre></li></ul><h2 id="查看HA效果"><a href="#查看HA效果" class="headerlink" title="查看HA效果"></a>查看HA效果</h2><ul><li><p>在一个NN上格式化zookeeper（n1）</p><pre><code>hdfs zkfc -formatZK</code></pre></li><li><p>在单节点NN启动n1</p><pre><code>start-dfs.sh</code></pre></li><li><p>输出</p><pre><code>Starting namenodes on [n1 n2]n1: namenode running as process 2411. Stop it first.n2: namenode running as process 2239. Stop it first.n2: datanode running as process 2413. Stop it first.n4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outStarting journal nodes [n2 n3 n4]n3: journalnode running as process 2207. Stop it first.n2: journalnode running as process 1895. Stop it first.n4: journalnode running as process 1732. Stop it first.Starting ZK Failover Controllers on NN hosts [n1 n2]n2: zkfc running as process 2804. Stop it first.n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outroot@n1:~# </code></pre></li><li><p>进程查看</p><pre><code>root@n1:~# jps3091 DFSZKFailoverController3285 Jps2411 NameNode2188 QuorumPeerMain#root@n2:~# jps3184 Jps2804 DFSZKFailoverController1751 QuorumPeerMain1895 JournalNode2413 DataNode2239 NameNode#root@n3:~# jps2512 Jps2121 QuorumPeerMain2362 DataNode2207 JournalNode#root@n4:~# jps1732 JournalNode2039 Jps1887 DataNode#  NN  DN  JN  ZK  ZKFCn1  1           1   1n2  1   1   1   1   1n3      1   1   1   n4      1   1</code></pre></li><li><p>查看webUI</p><pre><code>http://n2:50070/dfshealth.html#tab-overviewOverview &#39;n2:8020&#39; (active)#http://n1:50070/dfshealth.html#tab-overviewOverview &#39;n1:8020&#39; (standby)#Datanode Information三个#http://n1:50070/explorer.html#/Operation category READ is not supported in state standby#http://n2:50070/explorer.html#/Browse Directory 可见</code></pre></li><li><p>故障测试</p><pre><code>hadoop-daemon.sh stop namenode（n2）查看网页，文件系统，DNhadoop-daemon.sh start namenode（n2）再次查看交换了状态Overview &#39;n1:8020&#39; (active)Overview &#39;n2:8020&#39; (standby)</code></pre></li></ul><h2 id="重新启动与停止"><a href="#重新启动与停止" class="headerlink" title="重新启动与停止"></a>重新启动与停止</h2><ul><li>启停操作<pre><code>stop-dfs.shroot@n1:~# stop-dfs.sh Stopping namenodes on [n1 n2]n1: stopping namenoden2: stopping namenoden3: stopping datanoden4: stopping datanoden2: stopping datanodeStopping journal nodes [n2 n3 n4]n2: stopping journalnoden3: stopping journalnoden4: stopping journalnodeStopping ZK Failover Controllers on NN hosts [n1 n2]n1: stopping zkfcn2: stopping zkfc#下次启动jps查看ZK是不是启动先启动ZK，在启动DFS#root@n1:~# zkServer.sh start（根据列表中的三台都同时启动zk）root@n1:~# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: follower#root@n1:~# start-dfs.sh Starting namenodes on [n1 n2]n1: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n1.outn2: starting namenode, logging to /root/app/hadoop/logs/hadoop-root-namenode-n2.outn4: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n4.outn3: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n3.outn2: starting datanode, logging to /root/app/hadoop/logs/hadoop-root-datanode-n2.outStarting journal nodes [n2 n3 n4]n2: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n2.outn3: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n3.outn4: starting journalnode, logging to /root/app/hadoop/logs/hadoop-root-journalnode-n4.outStarting ZK Failover Controllers on NN hosts [n1 n2]n1: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n1.outn2: starting zkfc, logging to /root/app/hadoop/logs/hadoop-root-zkfc-n2.out</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何在hdfs上实现namenode集群的高可用。&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="ha" scheme="http://yoursite.com/tags/ha/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>hbase的基本构成与实践</title>
    <link href="http://yoursite.com/2018/04/24/hbase%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
    <id>http://yoursite.com/2018/04/24/hbase的基本构成与实践/</id>
    <published>2018-04-24T13:14:15.000Z</published>
    <updated>2018-04-24T14:00:22.559Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍hbase的基本构成与实践。</p><a id="more"></a><h2 id="hbase的基本构成"><a href="#hbase的基本构成" class="headerlink" title="hbase的基本构成"></a>hbase的基本构成</h2><ul><li><p>表空间 namespace</p><pre><code>两个默认的表空间hbase： 系统默认表空间default： 不指定自动加入的表空间root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/dataFound 2 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/defaultdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase</code></pre></li><li><p>表 table</p><pre><code>表的存在形式：表以文件夹的形式存在于hdfs中表的基本组成是：RowKey, Column Family, Column, Value(Cell):Byte array表的物理属性：以RowKey进行字典排序，行的方向存在多个Region，Region是存储和负载均衡的最小单元，不同的Region分布到不同的RegionServer上#=============================root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbaseFound 2 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/metadrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/namespace#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/metaFound 3 itemsdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/meta/.tabledescdrwxr-xr-x   - root supergroup          0 2018-04-23 20:18 /hbase/data/hbase/meta/.tmpdrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/hbase/meta/1588230740Found 4 items-rw-r--r--   1 root supergroup         32 2018-04-23 20:18 /hbase/data/hbase/meta/1588230740/.regioninfodrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/.tmpdrwxr-xr-x   - root supergroup          0 2018-04-24 01:45 /hbase/data/hbase/meta/1588230740/infodrwxr-xr-x   - root supergroup          0 2018-04-24 01:37 /hbase/data/hbase/meta/1588230740/recovered.edits#-----------------------------root@ubuntu:~/app/hbase/bin# ./hbase shell2018-04-24 06:29:19,776 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017hbase(main):001:0&gt;#-----------------------------hbase(main):001:0&gt; create &#39;maizi_hbase&#39;,&#39;f&#39;0 row(s) in 2.5670 seconds=&gt; Hbase::Table - maizi_hbasehbase(main):002:0&gt;#=============================访问 http://192.168.231.150:50070/explorer.html#/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458得到hbse的路径#-----------------------------root@ubuntu:~/app/hadoop/bin# ./hadoop fs -ls /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458Found 3 items-rw-r--r--   1 root supergroup         46 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/.regioninfodrwxr-xr-x   - root supergroup          0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/fdrwxr-xr-x   - root supergroup          0 2018-04-24 06:30 /hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/recovered.edits#-----------------------------上述操作中，6a028f21704f8fc6bf298598f6b8a458为Region的编号#=============================访问 http://192.168.231.150:16010/table.jsp?name=maizi_hbase 得到hbase的管理界面，可以看出路径结构</code></pre></li><li><p>列族 column family</p><pre><code>很多列的集合hbase中的每个列都属于一个column family每个column family存在于hdfs的单独文件中列名以column family为前缀， info:name, info:age#-----------------------------/hbase/data/default/maizi_hbase/6a028f21704f8fc6bf298598f6b8a458/f数据库/数据/表空间/表/region/列族#-----------------------------创建表的时候必须定义列族，因为在hdfs上必须要创建文件夹#-----------------------------何如设计RowKey是经典问题？</code></pre></li><li><p>列</p><pre><code>存放数据的地方</code></pre></li><li><p>RowKey</p><pre><code>可以理解为主键，最大长度为64k，RowKey保存为字节数组是非关系型数据库中key-value类型的数据的key自动字典排序散列原则，分布到不同的Region中，RegionServer的负载均衡问题</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍hbase的基本构成与实践。&lt;/p&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>how to understand habse components ?</title>
    <link href="http://yoursite.com/2018/04/24/how%20to%20understand%20habse%20component%20/"/>
    <id>http://yoursite.com/2018/04/24/how to understand habse component /</id>
    <published>2018-04-24T08:45:26.000Z</published>
    <updated>2018-04-24T09:07:21.985Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes the basic components of hbase, including HMaster, HRegionServer, Region.</p><a id="more"></a><h2 id="principle-of-hbase"><a href="#principle-of-hbase" class="headerlink" title="principle of hbase"></a>principle of hbase</h2><ul><li><strong>features of HMaster (technical director)</strong></li></ul><ol><li>add, delete, and modify tables</li><li>Region load balancing</li><li>HMaster manages the distribution of data</li></ol><ul><li><strong>features of RegionServer (department manager)</strong></li></ul><ol><li>RegionServer is the service component of Hbase</li><li>RegionServer maintains Regions assigned by HMaster</li><li>RegionServer can divide big Regions</li></ol><ul><li><strong>features of Region (developers)</strong></li></ul><ol><li>Region is a partition</li><li>handling the tasks assigned by RegionServer</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes the basic components of hbase, including HMaster, HRegionServer, Region.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper working mechanism and installation</title>
    <link href="http://yoursite.com/2018/04/24/zooKeeper%20working%20mechanism%20and%20installation/"/>
    <id>http://yoursite.com/2018/04/24/zooKeeper working mechanism and installation/</id>
    <published>2018-04-24T06:55:35.000Z</published>
    <updated>2018-04-24T07:51:18.928Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes the basic working principle and installation process of zookeeper.</p><a id="more"></a><h2 id="zookeeper-working-mechanism"><a href="#zookeeper-working-mechanism" class="headerlink" title="zookeeper working mechanism"></a>zookeeper working mechanism</h2><ul><li>zookeeper is a high-performance application coordination server that is mainly used to maintain a file system-like namespace.</li><li>zookeeper itself contains 2n+1 servers, and their roles are divided into leader and follower.</li><li>zookeeper maintains multiple server services and maintains data consistency to ensure that clients connecting to any server can get consistent data services.</li><li>zookeeper’s nodes have 4 life cycles.<pre><code>PERSISTENT (persistent node)PERSISTENT_SEQUENTIAL (Sequential automatic numbering of persistent nodes, this node automatically adds 1 based on the number of existing nodes)EPHEMERAL (temporary node, client session timeout such nodes will be automatically deleted)EPHEMERAL_SEQUENTIAL (temporary automatic numbering node)</code></pre></li></ul><h2 id="install-and-configure-zookeeper"><a href="#install-and-configure-zookeeper" class="headerlink" title="install and configure zookeeper"></a>install and configure zookeeper</h2><ul><li>download &gt; extract &gt; configure &gt; start/stop &gt; test</li><li>configure<pre><code>root@ubuntu:~/app/zookeeper# lltotal 1596drwxr-xr-x 10 1001 1001    4096 Mar 23  2017 ./drwxr-xr-x 13 root root    4096 Apr 23 23:50 ../drwxr-xr-x  2 1001 1001    4096 Mar 23  2017 bin/-rw-rw-r--  1 1001 1001   84725 Mar 23  2017 build.xmldrwxr-xr-x  2 1001 1001    4096 Mar 23  2017 conf/drwxr-xr-x 10 1001 1001    4096 Mar 23  2017 contrib/drwxr-xr-x  2 1001 1001    4096 Mar 23  2017 dist-maven/drwxr-xr-x  6 1001 1001    4096 Mar 23  2017 docs/-rw-rw-r--  1 1001 1001    1709 Mar 23  2017 ivysettings.xml-rw-rw-r--  1 1001 1001    5691 Mar 23  2017 ivy.xmldrwxr-xr-x  4 1001 1001    4096 Mar 23  2017 lib/-rw-rw-r--  1 1001 1001   11938 Mar 23  2017 LICENSE.txt-rw-rw-r--  1 1001 1001    3132 Mar 23  2017 NOTICE.txt-rw-rw-r--  1 1001 1001    1770 Mar 23  2017 README_packaging.txt-rw-rw-r--  1 1001 1001    1585 Mar 23  2017 README.txtdrwxr-xr-x  5 1001 1001    4096 Mar 23  2017 recipes/drwxr-xr-x  8 1001 1001    4096 Mar 23  2017 src/-rw-rw-r--  1 1001 1001 1456729 Mar 23  2017 zookeeper-3.4.10.jar-rw-rw-r--  1 1001 1001     819 Mar 23  2017 zookeeper-3.4.10.jar.asc-rw-rw-r--  1 1001 1001      33 Mar 23  2017 zookeeper-3.4.10.jar.md5-rw-rw-r--  1 1001 1001      41 Mar 23  2017 zookeeper-3.4.10.jar.sha1#==============================-rw-rw-r--  1 1001 1001  535 Mar 23  2017 configuration.xsl-rw-rw-r--  1 1001 1001 2161 Mar 23  2017 log4j.properties-rw-rw-r--  1 1001 1001  922 Mar 23  2017 zoo_sample.cfgroot@ubuntu:~/app/zookeeper/conf# cp zoo_sample.cfg zoo.cfgroot@ubuntu:~/app/zookeeper/conf# lltotal 24drwxr-xr-x  2 1001 1001 4096 Apr 24 00:17 ./drwxr-xr-x 10 1001 1001 4096 Mar 23  2017 ../-rw-rw-r--  1 1001 1001  535 Mar 23  2017 configuration.xsl-rw-rw-r--  1 1001 1001 2161 Mar 23  2017 log4j.properties-rw-r--r--  1 root root  922 Apr 24 00:17 zoo.cfg-rw-rw-r--  1 1001 1001  922 Mar 23  2017 zoo_sample.cfg#------------------------------vim zoo.cfgmkdir -p /root/app/zookeeper/zookdata# dataDir=/tmp/zookeeperdataDir=/root/app/zookeeper/zookdata# append the following:server.1=192.168.231.150:2888:3888#------------------------------root@ubuntu:~/app/zookeeper/zookdata# pwd/root/app/zookeeper/zookdataroot@ubuntu:~/app/zookeeper/zookdata# touch myid &amp;&amp; echo 1 &gt; myidroot@ubuntu:~/app/zookeeper/zookdata# cat myid1#==============================scp the zookeeper to other serverreset myid file in zookeeper on other server#==============================</code></pre></li></ul><h2 id="start-stop-zookeeper"><a href="#start-stop-zookeeper" class="headerlink" title="start/stop zookeeper"></a>start/stop zookeeper</h2><ul><li><p>start zookeeper</p><pre><code>root@ubuntu:~/app/zookeeper/bin# lltotal 52drwxr-xr-x  2 1001 1001 4096 Apr 24 00:35 ./drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../-rwxr-xr-x  1 1001 1001  232 Mar 23  2017 README.txt*-rwxr-xr-x  1 1001 1001 1937 Mar 23  2017 zkCleanup.sh*-rwxr-xr-x  1 1001 1001 1056 Mar 23  2017 zkCli.cmd*-rwxr-xr-x  1 1001 1001 1534 Mar 23  2017 zkCli.sh*-rwxr-xr-x  1 1001 1001 1628 Mar 23  2017 zkEnv.cmd*-rwxr-xr-x  1 1001 1001 2696 Mar 23  2017 zkEnv.sh*-rwxr-xr-x  1 1001 1001 1089 Mar 23  2017 zkServer.cmd*-rwxr-xr-x  1 1001 1001 6773 Mar 23  2017 zkServer.sh*-rw-r--r--  1 root root 5056 Apr 24 00:35 zookeeper.out#------------------------------Using config: /root/app/zookeeper/bin/../conf/zoo.cfgUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}#------------------------------root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh startzookeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED#------------------------------root@ubuntu:~/app/zookeeper/bin# jps38113 Jps34545 HQuorumPeer34757 HRegionServer12550 NodeManager12408 ResourceManager12024 DataNode34618 HMaster12235 SecondaryNameNode11852 NameNode#------------------------------root@ubuntu:~/app/zookeeper/bin# ./zkServer.sh statuszookeeper JMX enabled by defaultUsing config: /root/app/zookeeper/bin/../conf/zoo.cfgMode: standalone</code></pre></li><li><p>use zookeeper</p><pre><code>root@ubuntu:~/app/zookeeper/bin# ./zkCli.shConnecting to localhost:2181...2018-04-24 00:40:32,972 [myid:] - INFO  [main:Environment@100] - Client...2018-04-24 00:40:32,984 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2018-04-24 00:40:32,984 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp...WatchedEvent state:SyncConnected type:None path:null#------------------------------[zk: localhost:2181(CONNECTED) 0] helpzookeeper -server host:port cmd args  stat path [watch]  set path data [version]  ls path [watch]  delquota [-n|-b] path  ls2 path [watch]  setAcl path acl  setquota -n|-b val path  history  redo cmdno  printwatches on|off  delete path [version]  sync path  listquota path  rmr path  get path [watch]  create [-s] [-e] path data acl  addauth scheme auth  quit  getAcl path  close  connect host:port#------------------------------[zk: localhost:2181(CONNECTED) 1] create /data_test &#39;data_test&#39;       Created /data_test[zk: localhost:2181(CONNECTED) 2] ls /[data_test, zookeeper, hbase][zk: localhost:2181(CONNECTED) 3] get /data_testdata_testcZxid = 0x75ctime = Tue Apr 24 00:42:11 PDT 2018mZxid = 0x75mtime = Tue Apr 24 00:42:11 PDT 2018pZxid = 0x75cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 0#------------------------------[zk: localhost:2181(CONNECTED) 4] create /data_test/dir_2 &#39;123_value&#39;Created /data_test/dir_2[zk: localhost:2181(CONNECTED) 5] get /data_testdata_testcZxid = 0x75ctime = Tue Apr 24 00:42:11 PDT 2018mZxid = 0x75mtime = Tue Apr 24 00:42:11 PDT 2018pZxid = 0x76cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 1#------------------------------[zk: localhost:2181(CONNECTED) 6] quitQuitting...2018-04-24 00:47:19,034 [myid:] - INFO  [main:zookeeper@684] - Session: 0x162f5c0c60f0007 closed2018-04-24 00:47:19,037 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x162f5c0c60f0007</code></pre></li></ul><h2 id="view-the-log"><a href="#view-the-log" class="headerlink" title="view the log"></a>view the log</h2><ul><li>where is the log?<pre><code>root@ubuntu:~/app/zookeeper/bin# lltotal 52drwxr-xr-x  2 1001 1001 4096 Apr 24 00:35 ./drwxr-xr-x 11 1001 1001 4096 Apr 24 00:19 ../-rwxr-xr-x  1 1001 1001  232 Mar 23  2017 README.txt*-rwxr-xr-x  1 1001 1001 1937 Mar 23  2017 zkCleanup.sh*...-rw-r--r--  1 root root 5056 Apr 24 00:35 zookeeper.out</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes the basic working principle and installation process of zookeeper.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
      <category term="zookdata" scheme="http://yoursite.com/tags/zookdata/"/>
    
  </entry>
  
  <entry>
    <title>pseudo-distributed hbase configuration</title>
    <link href="http://yoursite.com/2018/04/24/pseudo-distributed%20hbase%20configuration/"/>
    <id>http://yoursite.com/2018/04/24/pseudo-distributed hbase configuration/</id>
    <published>2018-04-24T03:56:31.000Z</published>
    <updated>2018-04-24T04:24:55.590Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes how to build a pseudo-distributed hbase on a virtual machine.</p><a id="more"></a><h2 id="preconditions"><a href="#preconditions" class="headerlink" title="preconditions"></a>preconditions</h2><ul><li>jdk environment<pre><code>root@ubuntu:~# echo $JAVA_HOME/root/app/jdk1.8.0_171</code></pre></li><li><p>a pseudo-distributed hadoop</p><pre><code>root@ubuntu:~/app/hadoop/sbin# jps12550 NodeManager34152 Jps12408 ResourceManager12024 DataNode12235 SecondaryNameNode11852 NameNode</code></pre></li><li><p>test</p><pre><code>http://localhost:50070http://192.168.231.150:8099http://192.168.231.150:8042</code></pre></li></ul><h2 id="configure-hbase"><a href="#configure-hbase" class="headerlink" title="configure hbase"></a>configure hbase</h2><ul><li>download hbase from <a href="http://mirror.bit.edu.cn/apache/" target="_blank" rel="noopener">apache mirrors</a></li><li>extract files from hbase-2.0.0-beta-2-bin.tar.gz</li><li>configure xml files<pre><code>hbase-env.shhbase-site.xmlregionservers#================================root@ubuntu:~/app/hbase/conf# pwd/root/app/hbase/conf#================================root@ubuntu:~/app/hbase/conf# lltotal 48drwxr-xr-x 2 root root 4096 Apr 23 20:15 ./drwxr-xr-x 8 root root 4096 Apr 23 20:17 ../-rw-r--r-- 1 root root 1811 Dec 26  2015 hadoop-metrics2-hbase.properties-rw-r--r-- 1 root root 4537 Jan 28  2016 hbase-env.cmd-rw-r--r-- 1 root root 7537 Apr 23 20:12 hbase-env.sh-rw-r--r-- 1 root root 2257 Dec 26  2015 hbase-policy.xml-rw-r--r-- 1 root root 1355 Apr 23 20:09 hbase-site.xml-rw-r--r-- 1 root root 4603 May 28  2017 log4j.properties-rw-r--r-- 1 root root   16 Apr 23 20:15 regionservers#================================vim hbase-env.sh# The java implementation to use.  Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/export JAVA_HOME=/root/app/jdk1.8.0_171#--------------------------------# Tell HBase whether it should manage it&#39;s own instance of Zookeeper or not.# export HBASE_MANAGES_ZK=trueexport HBASE_MANAGES_ZK=true#================================vim regionserversroot@ubuntu:~/app/hbase/conf# cat regionservers192.168.231.150#================================vim hbase-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;hbase.rootdir&lt;/name&gt;      &lt;value&gt;hdfs://192.168.231.150:9000/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;  &lt;value&gt;192.168.231.150&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;      &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre></li></ul><h2 id="run-stop-hbase"><a href="#run-stop-hbase" class="headerlink" title="run/stop hbase"></a>run/stop hbase</h2><ul><li>run hbase<pre><code>root@ubuntu:~/app/hbase/bin# ./start-hbase.shlocalhost: starting zookeeper, logging to /root/app/hbase/bin/../logs/hbase-root-zookeeper-ubuntu.outstarting master, logging to /root/app/hbase/bin/../logs/hbase-root-master-ubuntu.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0192.168.231.150: starting regionserver, logging to /root/app/hbase/bin/../logs/hbase-root-regionserver-ubuntu.out192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0192.168.231.150: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0root@ubuntu:~/app/hbase/bin# jps34545 HQuorumPeer34757 HRegionServer12550 NodeManager12408 ResourceManager12024 DataNode34618 HMaster12235 SecondaryNameNode11852 NameNode35053 Jps</code></pre></li><li>test hbase<pre><code>http://192.168.231.150:16010</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes how to build a pseudo-distributed hbase on a virtual machine.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>how to download 1080p videos from youtube</title>
    <link href="http://yoursite.com/2018/04/23/how%20to%20download%201080p%20videos%20from%20youtube/"/>
    <id>http://yoursite.com/2018/04/23/how to download 1080p videos from youtube/</id>
    <published>2018-04-23T12:36:16.000Z</published>
    <updated>2018-04-27T03:53:52.640Z</updated>
    
    <content type="html"><![CDATA[<p>the article describes how to download 1080p videos form youtube with your foreigen vps/ecs.</p><a id="more"></a><h2 id="general-method"><a href="#general-method" class="headerlink" title="general method"></a>general method</h2><ul><li>chrome browser</li><li>copy the link of the video from youtube</li><li>paste the link to the input box of <a href="https://en.savefrom.net/" target="_blank" rel="noopener">https://en.savefrom.net/</a></li><li>download videos though chrome</li></ul><h2 id="advance-method"><a href="#advance-method" class="headerlink" title="advance method"></a>advance method</h2><ul><li>login in your foreign vps</li><li>such as: ubuntu from digitalocean</li><li>use the following cmd to download the video<pre><code>apt-get install youtube-dl -yyoutube-dl -f 22 your_video_link</code></pre></li></ul><h2 id="ultimate-method"><a href="#ultimate-method" class="headerlink" title="ultimate method"></a>ultimate method</h2><ul><li><p>use the following cmd to analysis all videos and audios about your_video_link</p><pre><code>youtube-dl -F your_video_link</code></pre></li><li><p>use the following cmd to download videos and audios with the specified code</p><pre><code>youtube-dl -f 10 your_video_link</code></pre></li><li><p>install the tools of video and audio</p><pre><code>apt-get install ffmpeg -y</code></pre></li><li><p>merge the video and audio</p><pre><code>ffmpeg -i /tmp/a.wav -i /tmp/a.avi /tmp/out.avi</code></pre></li></ul><h2 id="how-to-download-playlist-from-youtube"><a href="#how-to-download-playlist-from-youtube" class="headerlink" title="how to download playlist from youtube"></a>how to download playlist from youtube</h2><ul><li>cmd<pre><code>youtube-dl -citk –format mp4 –yes-playlist VIDEO_PLAYLIST_LINKyoutube-dl -citk –format mp4 –yes-playlist https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfuayoutube-dl -cit &quot;https://www.youtube.com/playlist?list=PLi8jnEH_cKdzioH63X5NLJjHGJcYZcfua&quot;</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;the article describes how to download 1080p videos form youtube with your foreigen vps/ecs.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="vps" scheme="http://yoursite.com/tags/vps/"/>
    
      <category term="youtube" scheme="http://yoursite.com/tags/youtube/"/>
    
  </entry>
  
  <entry>
    <title>usage of scp</title>
    <link href="http://yoursite.com/2018/04/23/usage%20of%20scp/"/>
    <id>http://yoursite.com/2018/04/23/usage of scp/</id>
    <published>2018-04-23T11:48:40.000Z</published>
    <updated>2018-04-23T11:51:29.908Z</updated>
    
    <content type="html"><![CDATA[<p>this article describes how download/upload files from server using scp command line.</p><a id="more"></a><h2 id="usage-of-scp"><a href="#usage-of-scp" class="headerlink" title="usage of scp"></a>usage of scp</h2><ol><li><p>download file from server</p><pre><code>scp root@servername:/path/filename /tmp/local_destinationscp root@192.168.0.101:/home/kimi/test.txt /home/kimi/test.txt</code></pre></li><li><p>upload file to server</p><pre><code>scp /path/local_filename root@servername:/path  scp /var/www/test.php root@192.168.0.101:/var/www/</code></pre></li><li><p>download directory to client</p><pre><code>scp -r root@servername:remote_dir/ /tmp/local_dir scp -r root@192.168.0.101:/home/kimi/test /tmp/local_dir</code></pre></li><li><p>upload directory to server</p><pre><code>scp -r /tmp/local_dir root@servername:remote_dirscp -P 22 -r test root@192.168.0.101:/var/www/</code></pre></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;this article describes how download/upload files from server using scp command line.&lt;/p&gt;
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
      <category term="cmd" scheme="http://yoursite.com/tags/cmd/"/>
    
  </entry>
  
  <entry>
    <title>write wordcount program on eclipse and run on hadoop in win10</title>
    <link href="http://yoursite.com/2018/04/23/write%20wordcount%20program%20on%20eclipse%20and%20run%20on%20hadoop%20in%20win10/"/>
    <id>http://yoursite.com/2018/04/23/write wordcount program on eclipse and run on hadoop in win10/</id>
    <published>2018-04-23T06:36:16.000Z</published>
    <updated>2018-04-23T07:16:36.476Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes how to write the wordcount program on eclipse and run it on local hadoop.</p><a id="more"></a><h2 id="prerequisites"><a href="#prerequisites" class="headerlink" title="prerequisites"></a><strong>prerequisites</strong></h2><ul><li><strong>server</strong></li></ul><ol><li>win10</li><li>hadoop 2.7.6</li></ol><ul><li><strong>client</strong></li></ul><ol><li>win10</li><li>eclipse neon</li><li>hadoop-eclipse-plugin-2.7.2.jar</li></ol><h2 id="create-project-and-coding"><a href="#create-project-and-coding" class="headerlink" title="create project and coding"></a><strong>create project and coding</strong></h2><ul><li><p><strong>create project</strong></p><pre><code>new &gt; other &gt; map reduce program &gt; fix the boxes with name, ${HADOOP_HOME} &gt; finish</code></pre></li><li><p><strong>coding</strong></p><pre><code>package com.hikvision.bigdata.hadoop.hadoop_wordcount;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.log4j.BasicConfigurator;/*** wordcount*/public class WordCount {  public static void main(String[] args) throws Exception {      BasicConfigurator.configure();      System.out.println(&quot;Hello World!&quot;);      Configuration conf = new Configuration();      @SuppressWarnings(&quot;deprecation&quot;)      Job job = new Job(conf, &quot;wordcount&quot;);      job.setJarByClass(WordCount.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);      job.setMapperClass(Map.class);      job.setReducerClass(Reduce.class);      job.setInputFormatClass(TextInputFormat.class);      job.setOutputFormatClass(TextOutputFormat.class);      FileInputFormat.addInputPath(job, new Path(args[0]));      FileOutputFormat.setOutputPath(job, new Path(args[1]));      job.waitForCompletion(true);  }  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {      private final static IntWritable one = new IntWritable(1);      private Text word = new Text();      public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String line = value.toString();          StringTokenizer tokenizer = new StringTokenizer(line);          while (tokenizer.hasMoreTokens()) {              word.set(tokenizer.nextToken());              context.write(word, one);          }      }  }  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {      public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)              throws IOException, InterruptedException {          int sum = 0;          for (IntWritable val : values) {              sum += val.get();          }          context.write(key, new IntWritable(sum));      }  }}</code></pre></li></ul><h2 id="configure-program"><a href="#configure-program" class="headerlink" title="configure program"></a><strong>configure program</strong></h2><ul><li><p><strong>configure hadoop</strong></p><pre><code>step 1:core-site.xml&lt;configuration&gt; &lt;property&gt;    &lt;name&gt;fs.default.name&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 2:hdfs-site.xml&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;dfs.name.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.data.dir&lt;/name&gt;    &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;    &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 3:mapred-site.xml&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;    &lt;value&gt;localhost:10020&lt;/value&gt;    &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;###########################################step 4:yarn-site.xml&lt;configuration&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;        &lt;value&gt;localhost:8099&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p><strong>format namenode</strong></p><pre><code>cd binhdfs namenode -format# format only once</code></pre></li><li><p><strong>start hadoop and upload data to hdfs</strong></p><pre><code>cd ${HADOOP_HOME}input cmd to open cmd linecd sbinstart-all.cmd##hadoop fs -mkdir /datahadoop fs -put D:\a.txt /data/a.txt</code></pre></li><li><strong>configure input and output path for program in eclipse</strong><pre><code>1 project name &gt; right clieck &gt; run as &gt; run configuration &gt; java application &gt; new2 fix the boxes with the program name, main class, run name, and arguments3 the arguments as follows:hdfs://localhost:9000/data/a.txt hdfs://localhost:9000/data/output</code></pre></li></ul><h2 id="run-program"><a href="#run-program" class="headerlink" title="run program"></a><strong>run program</strong></h2><ul><li><strong>precondition</strong></li></ul><ol><li>delete the output floder on hdfs</li><li>data is ready</li><li>input/output path is configured in eclipse</li><li>hadoop is running</li></ol><ul><li><strong>run program</strong><pre><code>project name &gt; right clieck &gt; run as &gt; run on hadoop &gt; select the main class &gt; enjoy the ouput</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes how to write the wordcount program on eclipse and run it on local hadoop.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
      <category term="coding" scheme="http://yoursite.com/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>configure hadoop on ubuntu and connect to eclipse on ubuntu or win10</title>
    <link href="http://yoursite.com/2018/04/22/configure%20hadoop%20on%20ubuntu%20and%20connect%20to%20eclipse%20on%20ubuntu%20or%20win10/"/>
    <id>http://yoursite.com/2018/04/22/configure hadoop on ubuntu and connect to eclipse on ubuntu or win10/</id>
    <published>2018-04-22T04:49:06.000Z</published>
    <updated>2018-04-22T08:34:41.549Z</updated>
    
    <content type="html"><![CDATA[<p>This article describes how to configure hadoop2.7.6 on ubuntu and connect to hadoop2.7.6 using eclipse on ubuntu and win10 respectively.</p><a id="more"></a><h2 id="prerequisites"><a href="#prerequisites" class="headerlink" title="prerequisites"></a><strong>prerequisites</strong></h2><ul><li><strong>server</strong></li></ul><ol><li>win10</li><li>vmware workstation pro 12</li><li>Ubuntu 16.04.4 LTS</li><li>hadoop 2.7.6 <a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">download</a></li></ol><ul><li><strong>ubuntu clinet</strong></li></ul><ol><li>eclipse neon <a href="http://www.eclipse.org/downloads/packages/release/Neon/3" target="_blank" rel="noopener">download</a></li><li>hadoop-eclipse-plugin-2.7.2.jar <a href="https://download.csdn.net/download/tondayong1981/9432425" target="_blank" rel="noopener">download</a></li></ol><ul><li><strong>win10 client</strong></li></ul><ol><li>eclipse neon <a href="http://www.eclipse.org/downloads/packages/release/Neon/3" target="_blank" rel="noopener">download</a></li><li>hadoop-eclipse-plugin-2.7.2.jar <a href="https://download.csdn.net/download/tondayong1981/9432425" target="_blank" rel="noopener">download</a></li></ol><h2 id="pretreatment-for-ubuntu-virtual-machine"><a href="#pretreatment-for-ubuntu-virtual-machine" class="headerlink" title="pretreatment for ubuntu virtual machine"></a><strong>pretreatment for ubuntu virtual machine</strong></h2><ul><li><strong>enable the user-passwd input box on the login screen</strong><pre><code>sudo passwd rootsu rootcd /usr/share/lightdm/lightdm.conf.d/vim 50-unity-greeter.conf# adduser-session=ubuntugreeter-show-manual-login=trueall-guest=false# rebootreboot# login in ubuntu with root and get a error reportvim /root/.profile# locate tomesg n || true# change totty -s &amp;&amp; mesg n || true</code></pre></li><li><strong>configure jdk for ubuntu</strong></li><li>jdk 1.8 <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">download</a><pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binsource /etc/profilejava -version</code></pre></li></ul><h2 id="configure-hadoop2-7-6"><a href="#configure-hadoop2-7-6" class="headerlink" title="configure hadoop2.7.6"></a><strong>configure hadoop2.7.6</strong></h2><ul><li><p><strong>append <em>JAVA_HOME</em> to stat script</strong></p><pre><code>step 1:vim /root/app/hadoop/etc/hadoop/hadoop-env.sh# export JAVA_HOME=${JAVA_HOME}export JAVA_HOME=/root/app/jdk1.8.0_171###########################################step 2:vim /root/app/hadoop/etc/hadoop/yarn-env.sh# some Java parameters# export JAVA_HOME=/home/y/libexec/jdk1.6.0/export JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p><strong>configure core-site.xml, hdfs-site.xml,  mapred-site.xml, yarn-site.xml</strong></p><pre><code>step 1:vim /root/app/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;   &lt;property&gt;      &lt;name&gt;fs.default.name&lt;/name&gt;      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/tmp&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 2:vim /root/app/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;dfs.name.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/hdfs/name&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.data.dir&lt;/name&gt;      &lt;value&gt;/root/app/hadoop/hdfs/data&lt;/value&gt;  &lt;/property&gt; &lt;property&gt;      &lt;name&gt;dfs.permissions&lt;/name&gt;      &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;      &lt;description&gt;defult 3, less than numbers of datanode&lt;/description&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 3:vim /root/app/hadoop/etc/hadoop/mapred-site.xml&lt;configuration&gt;  &lt;property&gt;      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;      &lt;value&gt;yarn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;      &lt;value&gt;localhost:10020&lt;/value&gt;      &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;  &lt;/property&gt;&lt;/configuration&gt;###########################################step 4:vim /root/app/hadoop/etc/hadoop/yarn-site.xml&lt;configuration&gt;  &lt;property&gt;          &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;          &lt;value&gt;mapreduce_shuffle&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;          &lt;value&gt;localhost:8099&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p><strong>create floder for hadoop work directory</strong></p><pre><code>cd /root/app/hadoopmkdir hdfs -p hdfs/data hdfs/namemkdir tmp</code></pre></li><li><p><strong>format hdfs and start/stop hadoop</strong></p></li></ul><ol><li>format hdfs<pre><code># keep all hadoop process stopped./sbin/stop-all.sh# remove tmp directoryrm -rdf tmp/# format hdfs only oncebin/hdfs namenode -format# see if the format is successfultree hdfs/</code></pre></li><li>start/stop hadoop<pre><code># start hadoop./sbin/start-all.shroot@ubuntu:~/app/hadoop# jps63809 Jps63474 NodeManager62950 DataNode63335 ResourceManager62775 NameNode63163 SecondaryNameNode# stop hadoop./sbin/stop-all.sh</code></pre></li></ol><h2 id="connect-to-hadoop-with-eclipse"><a href="#connect-to-hadoop-with-eclipse" class="headerlink" title="connect to hadoop with eclipse"></a><strong>connect to hadoop with eclipse</strong></h2><ul><li><strong>install plugin in eclipse on ubuntu</strong><pre><code>1 copy to hadoop-eclipse-plugin-2.7.2.jar to ${eclipse_home}/dropins;2 open eclipse;3 open menu &gt; windows &gt; show view &gt; other &gt; mapreduce tools &gt; map/reduce locations;4 map/reduce locations &gt; right click &gt; edit hadoop location;location name: XXXmap/reduce master:  host: localhost  port: 50020dfs master:  host: localhost  port: 90005 open dfs locations, you will find the file in hdfs.</code></pre></li><li><strong>install plugin in eclipse on win10</strong><pre><code>1 on win10, your eclipse serves as a clinet, you can connect your server with ip, so you should firstly replace *localhost* with your server ip in all etc files, such as core-site.xml, hdfs-site.xml,  mapred-site.xml, yarn-site.xml;2 repeat the step 1,2,3,4 above;3 map/reduce locations &gt; right click &gt; edit hadoop location &gt; advace parameters, replace *hadoop.tmp.dir* with your own address in /hdfs-site.xml;4 enjoy the local developing and the remote debuging.</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article describes how to configure hadoop2.7.6 on ubuntu and connect to hadoop2.7.6 using eclipse on ubuntu and win10 respectively.&lt;/p&gt;
    
    </summary>
    
      <category term="configuration" scheme="http://yoursite.com/categories/configuration/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="eclipse" scheme="http://yoursite.com/tags/eclipse/"/>
    
      <category term="configuration" scheme="http://yoursite.com/tags/configuration/"/>
    
  </entry>
  
  <entry>
    <title>hexo构建博客搜索框加载中的解决方案</title>
    <link href="http://yoursite.com/2018/04/19/hexo%E6%9E%84%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%90%9C%E7%B4%A2%E6%A1%86%E5%8A%A0%E8%BD%BD%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://yoursite.com/2018/04/19/hexo构建博客搜索框加载中的解决方案/</id>
    <published>2018-04-19T06:37:10.000Z</published>
    <updated>2018-04-21T05:52:38.661Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。</p><a id="more"></a><h2 id="问题与现象"><a href="#问题与现象" class="headerlink" title="问题与现象"></a>问题与现象</h2><ul><li>nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。</li></ul><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><ul><li>开发的markdown中出现了非utf-8的字符。</li><li>访问可以查找错误出现的位置：<a href="https://leebin.top/search.xml" target="_blank" rel="noopener">https://leebin.top/search.xml</a></li></ul><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ul><li>逐个排查每个markdown文件，直到找到非utf-8字符，删除，重新部署，点击搜索框测试。</li><li>访问：<a href="https://leebin.top/search.xml" target="_blank" rel="noopener">https://leebin.top/search.xml</a> 发现可以解析成源文件。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍nodejs+hexo+github+markdown搭建博客后，点击搜素框，一直在加载中的解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>hadoop中wordcount程序开发</title>
    <link href="http://yoursite.com/2018/04/19/Hadoop%E4%B8%ADwordcount%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/"/>
    <id>http://yoursite.com/2018/04/19/Hadoop中wordcount程序开发/</id>
    <published>2018-04-19T05:09:22.000Z</published>
    <updated>2018-04-21T05:52:38.657Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何利用java和hadoop组件开发wordcount程序。</p><a id="more"></a><h2 id="开发与测试环境"><a href="#开发与测试环境" class="headerlink" title="开发与测试环境"></a>开发与测试环境</h2><ul><li>windows</li><li>eclipse</li><li>maven，常见的组件如下：</li></ul><ol><li>Apache Hadoop Common 3.1</li><li>Apache Hadoop Client Aggregator 3.1</li><li>Hadoop Core 1.2</li><li>Apache Hadoop HDFS 3.1</li><li>Apache Hadoop MapReduce Core 3.1</li></ol><ul><li>ubuntu中hadoop单机模式，搭建过程参考: <a href="https://leebin.top/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">如何hadoop单机版</a></li></ul><h2 id="添加依赖后maven报错"><a href="#添加依赖后maven报错" class="headerlink" title="添加依赖后maven报错"></a>添加依赖后maven报错</h2><ul><li><p>报错</p><pre><code>Buiding Hadoop with Eclipse / Maven - Missing artifact jdk.tools:jdk.tools:jar:1.6</code></pre></li><li><p>解决</p><pre><code># cmdC:\Users\BinLee&gt;java -versionjava version &quot;1.8.0_144&quot;Java(TM) SE Runtime Environment (build 1.8.0_144-b01)Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)# 添加下面的依赖到maven的pom.xml&lt;dependency&gt;  &lt;groupId&gt;jdk.tools&lt;/groupId&gt;  &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;  &lt;version&gt;1.8.0_144&lt;/version&gt;  &lt;scope&gt;system&lt;/scope&gt;  &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt;</code></pre></li></ul><h2 id="wordcount程序开发"><a href="#wordcount程序开发" class="headerlink" title="wordcount程序开发"></a>wordcount程序开发</h2><ul><li><p>pom.xml</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.jordiburgos&lt;/groupId&gt;  &lt;artifactId&gt;wordcount&lt;/artifactId&gt;  &lt;packaging&gt;jar&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;name&gt;wordcount&lt;/name&gt;  &lt;url&gt;http://jordiburgos.com&lt;/url&gt;  &lt;properties&gt;      &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;  &lt;/properties&gt;  &lt;repositories&gt;      &lt;repository&gt;          &lt;id&gt;cloudera&lt;/id&gt;          &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;      &lt;/repository&gt;  &lt;/repositories&gt;  &lt;dependencies&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;          &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;          &lt;version&gt;2.2.0&lt;/version&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;          &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;          &lt;version&gt;1.2.1&lt;/version&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;jdk.tools&lt;/groupId&gt;          &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;          &lt;version&gt;1.7&lt;/version&gt;          &lt;scope&gt;system&lt;/scope&gt;          &lt;systemPath&gt;${java.home}/../lib/tools.jar&lt;/systemPath&gt;      &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;junit&lt;/groupId&gt;          &lt;artifactId&gt;junit&lt;/artifactId&gt;          &lt;version&gt;3.8.1&lt;/version&gt;          &lt;scope&gt;test&lt;/scope&gt;      &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;      &lt;plugins&gt;          &lt;plugin&gt;              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;              &lt;version&gt;2.4&lt;/version&gt;              &lt;executions&gt;                  &lt;execution&gt;                      &lt;id&gt;distro-assembly&lt;/id&gt;                      &lt;phase&gt;package&lt;/phase&gt;                      &lt;goals&gt;                          &lt;goal&gt;single&lt;/goal&gt;                      &lt;/goals&gt;                      &lt;configuration&gt;                          &lt;descriptors&gt;                              &lt;descriptor&gt;assembly.xml&lt;/descriptor&gt;                          &lt;/descriptors&gt;                      &lt;/configuration&gt;                  &lt;/execution&gt;              &lt;/executions&gt;          &lt;/plugin&gt;      &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre></li><li><p>wordcount.java</p><pre><code>package com.jordiburgos;import java.io.IOException;import java.util.*;import org.apache.hadoop.fs.Path;import org.apache.hadoop.conf.*;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.*;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;public class WordCount {  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {      private final static IntWritable one = new IntWritable(1);      private Text word = new Text();      public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {          String line = value.toString();          StringTokenizer tokenizer = new StringTokenizer(line);          while (tokenizer.hasMoreTokens()) {              word.set(tokenizer.nextToken());              context.write(word, one);          }      }  }  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {      public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)              throws IOException, InterruptedException {          int sum = 0;          for (IntWritable val : values) {              sum += val.get();          }          context.write(key, new IntWritable(sum));      }  }  public static void main(String[] args) throws Exception {      Configuration conf = new Configuration();      Job job = new Job(conf, &quot;wordcount&quot;);      job.setJarByClass(WordCount.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);      job.setMapperClass(Map.class);      job.setReducerClass(Reduce.class);      job.setInputFormatClass(TextInputFormat.class);      job.setOutputFormatClass(TextOutputFormat.class);      FileInputFormat.addInputPath(job, new Path(args[0]));      FileOutputFormat.setOutputPath(job, new Path(args[1]));      job.waitForCompletion(true);  }}</code></pre></li></ul><h2 id="使用maven打包程序"><a href="#使用maven打包程序" class="headerlink" title="使用maven打包程序"></a>使用maven打包程序</h2><ul><li><p>打包命令</p><pre><code>项目右键&gt;run as&gt;maven build</code></pre></li><li><p>打包后jar的结构</p><pre><code>C:.└─wordcount  ├─com  │  └─jordiburgos  └─META-INF      └─maven          └─com.jordiburgos              └─wordcount</code></pre></li></ul><h2 id="在hadoop上运行程序"><a href="#在hadoop上运行程序" class="headerlink" title="在hadoop上运行程序"></a>在hadoop上运行程序</h2><ul><li><p>上传待分析的文本到hdfs</p><pre><code># 本地创建input文件夹和a.txt文件cd /root/app/hadoop-3.1.0mkdir inputvim a.txt## 创建文件夹hadoop fs -mkdir hdfs://localhost:9001/tmp## 上传文件到hdfshadoop fs -put /root/app/hadoop-3.1.0/input hdfs://127.0.0.1:9001/tmp</code></pre></li><li><p>运行jar程序</p><pre><code>cd /root/app/hadoop-3.1.0bin/hadoop jar wordcount.jar com.jordiburgos.WordCount hdfs://localhost:9001/tmp/input/ file:///root/app/hadoop-3.1.0/output/</code></pre></li><li><p>在linux中查看输出文件</p><pre><code>cd /root/app/hadoop-3.1.0/outputroot@ubuntu:~/app/hadoop-3.1.0/output# lspart-r-00000  _SUCCESSroot@ubuntu:~/app/hadoop-3.1.0/output# cat part-r-000000000    1aaaa    1ddfh    1ff    1ggg    1hj    1iiiii    1sss    1</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何利用java和hadoop组件开发wordcount程序。&lt;/p&gt;
    
    </summary>
    
      <category term="开发" scheme="http://yoursite.com/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="maven" scheme="http://yoursite.com/tags/maven/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu中hadoop单机模式和伪分布式搭建</title>
    <link href="http://yoursite.com/2018/04/18/ubuntu%E4%B8%ADhadoop%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/04/18/ubuntu中hadoop单机模式和伪分布式搭建/</id>
    <published>2018-04-18T05:09:22.000Z</published>
    <updated>2018-04-21T05:52:38.663Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何搭建hadoop单机版本/独立模式/standalone模式？<br><a id="more"></a></p><h2 id="ubuntu开启root用户登录的方法"><a href="#ubuntu开启root用户登录的方法" class="headerlink" title="ubuntu开启root用户登录的方法"></a>ubuntu开启root用户登录的方法</h2><ul><li>设置密码、添加信息<pre><code>sudo passwd -u rootsudo passwd rootsu rootcd /usr/share/lightdm/lightdm.conf.d/vim 50-unity-greeter.conf# 添加user-session=ubuntugreeter-show-manual-login=trueall-guest=false# 重启reboot# 使用user和passwd进入root报错vim /root/.profile# 找到mesg n || true# 改为tty -s &amp;&amp; mesg n || true</code></pre></li></ul><h2 id="ubuntu中的java环境变量配置"><a href="#ubuntu中的java环境变量配置" class="headerlink" title="ubuntu中的java环境变量配置"></a>ubuntu中的java环境变量配置</h2><ul><li>编辑 sudo vim /etc/profile<pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binsource /etc/profile</code></pre></li><li>验证<pre><code>java -version</code></pre></li></ul><h2 id="单机版hadoop配置"><a href="#单机版hadoop配置" class="headerlink" title="单机版hadoop配置"></a>单机版hadoop配置</h2><ul><li><p><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation" target="_blank" rel="noopener">官方文档</a></p></li><li><p>生成ssh密钥</p><pre><code>cd ~ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keysssh localhost</code></pre></li><li><p>配置java环境</p><pre><code>root@ubuntu:~# vim /etc/profileexport JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</code></pre></li><li><p>配置hadoop环境 vim /etc/profile</p><pre><code>#HADOOP VARIABLES STARTexport JAVA_HOME=/root/app/jdk1.8.0_171export HADOOP_HOME=/root/app/hadoop-3.1.0export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#HADOOP VARIABLES ENDsource /etc/profile</code></pre></li><li><p>单机版测试</p><pre><code>root@ubuntu:~# /root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jarAn example program must be given as the first argument.Valid program names are:aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.dbcount: An example job that count the pageview counts from a database.distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.grep: A map/reduce program that counts the matches of a regex in the input.join: A job that effects a join over sorted, equally partitioned datasetsmultifilewc: A job that counts words from several files.pentomino: A map/reduce tile laying program to find solutions to pentomino problems.pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.randomwriter: A map/reduce program that writes 10GB of random data per node.secondarysort: An example defining a secondary sort to the reduce.sort: A map/reduce program that sorts the data written by the random writer.sudoku: A sudoku solver.teragen: Generate data for the terasortterasort: Run the terasortteravalidate: Checking results of terasortwordcount: A map/reduce program that counts the words in the input files.wordmean: A map/reduce program that counts the average length of the words in the input files.wordmedian: A map/reduce program that counts the median length of the words in the input files.wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.</code></pre></li><li><p>实例测试</p><pre><code>/root/app/hadoop-3.1.0/bin/hadoop jar /root/app/hadoop-3.1.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar grep ./input ./output &#39;dfs[a-z.]+&#39;cat ./output/*  # 查看结果rm -r ./output  # 删除结果# 结果root@ubuntu:~/app/hadoop-3.1.0# cat ./output/*  1    dfsadmin</code></pre></li></ul><h2 id="伪分布式hadoop配置"><a href="#伪分布式hadoop配置" class="headerlink" title="伪分布式hadoop配置"></a>伪分布式hadoop配置</h2><ul><li><p>格式化hdfs</p><pre><code>cd /root/app/hadoop-3.1.0./bin/hdfs namenode -format</code></pre></li><li><p>添加变量到 vim /etc/profile</p><pre><code>export JAVA_HOME=/root/app/jdk1.8.0_171export JRE_HOME=/root/app/jdk1.8.0_171/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin## HADOOP VARIABLES STARTexport JAVA_HOME=/root/app/jdk1.8.0_171export HADOOP_HOME=/root/app/hadoop-3.1.0#export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin#export HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOME#export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#export HDFS_DATANODE_USER=rootexport HDFS_NAMENODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=root#export YARN_RESOURCEMANAGER_USER=rootexport HADOOP_SECURE_DN_USER=yarnexport YARN_NODEMANAGER_USER=root# HADOOP VARIABLES END</code></pre></li><li><p>编辑/root/app/hadoop-3.1.0/etc/hadoop/core-site.xml</p><pre><code>&lt;configuration&gt;     &lt;property&gt;          &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;          &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;     &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>编辑/root/app/hadoop-3.1.0/etc/hadoop/hdfs-site.xml</p><pre><code>&lt;configuration&gt;      &lt;property&gt;           &lt;name&gt;dfs.replication&lt;/name&gt;           &lt;value&gt;2&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;           &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/name&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;           &lt;value&gt;file:/root/app/hadoop-3.1.0/tmp/dfs/data&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.http.address&lt;/name&gt;          &lt;value&gt;0.0.0.0:50070&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;</code></pre></li><li><p>编辑 hadoop-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/hadoop-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p>编辑 yarn-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/yarn-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li><li><p>编辑 mapred-env.sh</p><pre><code>vim /root/app/hadoop-3.1.0/etc/hadoop/mapred-env.shexport JAVA_HOME=/root/app/jdk1.8.0_171</code></pre></li></ul><h2 id="hadoop的使用"><a href="#hadoop的使用" class="headerlink" title="hadoop的使用"></a>hadoop的使用</h2><ul><li><p>启动与停止</p><pre><code>cd /root/app/hadoop-3.1.0./sbin/start-all.sh./sbin/stop-all.sh</code></pre></li><li><p>查看服务</p><pre><code>root@ubuntu:~/app/hadoop-3.1.0# jps23058 NameNode23491 SecondaryNameNode23753 ResourceManager23225 DataNode24427 Jps24030 NodeManager</code></pre></li><li>Resource Manager <a href="http://localhost:8088" target="_blank" rel="noopener">http://localhost:8088</a></li><li>Web UI of the NameNode daemon <a href="http://localhost:50070" target="_blank" rel="noopener">http://localhost:50070</a></li><li>HDFS NameNode web interface <a href="http://localhost:8042" target="_blank" rel="noopener">http://localhost:8042</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何搭建hadoop单机版本/独立模式/standalone模式？&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>apache maven的配置与使用</title>
    <link href="http://yoursite.com/2018/04/17/maven%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/04/17/maven的配置与使用/</id>
    <published>2018-04-17T03:49:36.000Z</published>
    <updated>2018-04-22T10:50:49.146Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了apache maven的配置与使用过程，【清理项目】→【编译项目】→【测试项目】→【生成测试报告】→【打包项目】→【部署项目】，maven详细讲解：<a href="https://www.yiibai.com/maven/" target="_blank" rel="noopener">他山之石</a></p><a id="more"></a><h2 id="需要先配置java和maven环境变量"><a href="#需要先配置java和maven环境变量" class="headerlink" title="需要先配置java和maven环境变量"></a><strong>需要先配置java和maven环境变量</strong></h2><ul><li><p>CLASSPATH</p><pre><code>.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</code></pre></li><li><p>JAVA_HOME</p><pre><code>C:\app3\Java\jdk1.8.0_144</code></pre></li><li><p>JRE_HOME</p><pre><code>C:\app3\Java\jre1.8.0_144</code></pre></li><li><p>MVN_HOME</p><pre><code>C:\app3\apache-maven-3.5.3</code></pre></li><li><p>Path</p><pre><code>C:\app3\Python35\Scripts\;C:\app3\Python35\;C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%JAVA_HOME%\bin;%JRE_HOME%\bin;C:\app3\Git\cmd;C:\app3\MinGW\bin;C:\app3\nodejs\;C:\app3\MATLAB\R2017b\runtime\win64;C:\app3\MATLAB\R2017b\bin;%MVN_HOME%\bin;</code></pre></li></ul><h2 id="更换maven的仓库为自定义的仓库"><a href="#更换maven的仓库为自定义的仓库" class="headerlink" title="更换maven的仓库为自定义的仓库"></a><strong>更换maven的仓库为自定义的仓库</strong></h2><ul><li>创建目标位置如，d:\maven\repo</li><li>拷贝C:\app3\apache-maven-3.5.3\conf\settings.xml文件到d:\maven</li><li>修改两处的settings.xml文件</li><li>定位到localRepository<pre><code>&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;# 修改为：&lt;localRepository&gt;d:\maven\repo&lt;/localRepository&gt;</code></pre></li></ul><h2 id="maven手动创建项目"><a href="#maven手动创建项目" class="headerlink" title="maven手动创建项目"></a><strong>maven手动创建项目</strong></h2><ul><li><p><a href="https://www.cnblogs.com/yjmyzz/p/3495762.html" target="_blank" rel="noopener">他山之石</a></p></li><li><p>创建项目</p><pre><code># cmdcd /d d:\testmvn archetype:generate# logChoose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): 1169:（和eclipse上的maven插件有关系，直接回车）Choose org.apache.maven.archetypes:maven-archetype-quickstart version:1: 1.0-alpha-12: 1.0-alpha-23: 1.0-alpha-34: 1.0-alpha-45: 1.06: 1.17: 1.3Choose a number: 7:(直接回车)Define value for property &#39;groupId&#39;: com.hikvision.ai_data.data（从大往小填写自己公司的名字）Define value for property &#39;artifactId&#39;: test_mvn（项目的名字）Define value for property &#39;version&#39; 1.0-SNAPSHOT: :（默认就行）Define value for property &#39;package&#39; com.hikvision.ai_data.data: : test_mvn_pkg（将class打包的jar文件的名称）Confirm properties configuration:groupId: com.hikvision.ai_data.dataartifactId: test_mvnversion: 1.0-SNAPSHOTpackage: test_mvn_pkgY: :(直接回车)[INFO] ----------------------------------------------------------------------------[INFO] Using following parameters for creating project from Archetype: maven-archetype-quickstart:1.3[INFO] ----------------------------------------------------------------------------[INFO] Parameter: groupId, Value: com.hikvision.ai_data.data[INFO] Parameter: artifactId, Value: test_mvn[INFO] Parameter: version, Value: 1.0-SNAPSHOT[INFO] Parameter: package, Value: test_mvn_pkg[INFO] Parameter: packageInPathFormat, Value: test_mvn_pkg[INFO] Parameter: package, Value: test_mvn_pkg[INFO] Parameter: version, Value: 1.0-SNAPSHOT[INFO] Parameter: groupId, Value: com.hikvision.ai_data.data[INFO] Parameter: artifactId, Value: test_mvn[INFO] Project created from Archetype in dir: D:\003---WorkSpace\06---testmaven\test_mvn[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 13:57 min[INFO] Finished at: 2018-04-17T14:54:37+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>创建项目后查看文件</p><pre><code>D:\003---WorkSpace\06---testmaven&gt;tree卷 工厂 的文件夹 PATH 列表卷序列号为 0000006C BAA7:827CD:.└─test_mvn  └─src      ├─main      │  └─java      │      └─test_mvn_pkg      └─test          └─java              └─test_mvn_pkg</code></pre></li><li><p>编译项目</p><pre><code># cmdcd test_mvnmvn clean compile# logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean compile[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn ---[INFO][INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.570 s[INFO] Finished at: 2018-04-17T14:58:59+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>单元测试</p><pre><code># cmdmvn clean test# logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean test[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn ---[INFO] Deleting D:\003---WorkSpace\06---testmaven\test_mvn\target[INFO][INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes[INFO][INFO] --- maven-resources-plugin:3.0.2:testResources (default-testResources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\test\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\test-classes[INFO][INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ test_mvn ---[INFO][INFO] -------------------------------------------------------[INFO]  T E S T S[INFO] -------------------------------------------------------[INFO] Running test_mvn_pkg.AppTest[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.057 s - in test_mvn_pkg.AppTest[INFO][INFO] Results:[INFO][INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 3.227 s[INFO] Finished at: 2018-04-17T15:00:33+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>打包项目</p><pre><code># cmdmvn clean package# logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn clean package[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ test_mvn ---[INFO] Deleting D:\003---WorkSpace\06---testmaven\test_mvn\target[INFO][INFO] --- maven-resources-plugin:3.0.2:resources (default-resources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\main\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\classes[INFO][INFO] --- maven-resources-plugin:3.0.2:testResources (default-testResources) @ test_mvn ---[INFO] Using &#39;UTF-8&#39; encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\003---WorkSpace\06---testmaven\test_mvn\src\test\resources[INFO][INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ test_mvn ---[INFO] Changes detected - recompiling the module![INFO] Compiling 1 source file to D:\003---WorkSpace\06---testmaven\test_mvn\target\test-classes[INFO][INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ test_mvn ---[INFO][INFO] -------------------------------------------------------[INFO]  T E S T S[INFO] -------------------------------------------------------[INFO] Running test_mvn_pkg.AppTest[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.052 s - in test_mvn_pkg.AppTest[INFO][INFO] Results:[INFO][INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO][INFO][INFO] --- maven-jar-plugin:3.0.2:jar (default-jar) @ test_mvn ---[INFO] Building jar: D:\003---WorkSpace\06---testmaven\test_mvn\target\test_mvn-1.0-SNAPSHOT.jar[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 3.599 s[INFO] Finished at: 2018-04-17T15:02:35+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>运行项目</p><pre><code># cmd# 1.无参数，类在target下面test_mvn\target\classes\test_mvn_pkg\App.classmvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot;# 即mvn exec:java -Dexec.mainClass=&quot;test_mvn_pkg.App&quot;## 2.有参数mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.args=&quot;arg0 arg1 arg2&quot;## 3.指定对classpath的运行时依赖mvn exec:java -Dexec.mainClass=&quot;com.vineetmanohar.module.Main&quot; -Dexec.classpathScope=runtime## logD:\003---WorkSpace\06---testmaven\test_mvn&gt;mvn exec:java -Dexec.mainClass=&quot;test_mvn_pkg.App&quot;[INFO] Scanning for projects...[INFO][INFO] ----------------&lt; com.hikvision.ai_data.data:test_mvn &gt;-----------------[INFO] Building test_mvn 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- exec-maven-plugin:1.6.0:java (default-cli) @ test_mvn ---Hello World![INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.010 s[INFO] Finished at: 2018-04-17T15:09:49+08:00[INFO] ------------------------------------------------------------------------</code></pre></li><li><p>项目部署</p><pre><code># 前提是jboss web server已经成功启动# cmdmvn clean jboss-as:deploy</code></pre></li></ul><h2 id="eclipse上的maven项目"><a href="#eclipse上的maven项目" class="headerlink" title="eclipse上的maven项目"></a><strong>eclipse上的maven项目</strong></h2><ul><li><p>打开eclipse进行java配置，然后关闭</p><pre><code>windows &gt; preferences &gt; java &gt; installed jres &gt; add jdk floder and jre floderselect jdk floder &gt; apply</code></pre></li><li><p>eclipse上的maven插件M2Eclipse</p><pre><code>help menu &gt; install new software &gt; input the url as followhttp://download.eclipse.org/technology/m2e/releases/# 备注插件官网http://www.eclipse.org/m2e/# 该插件可以解决mvn install报错问题</code></pre></li><li><p>eclipse中的maven配置</p><pre><code>windows &gt; preferences &gt; maven &gt; installations &gt; maven &gt; $(maven_home) &gt; applywindows &gt; preferences &gt; maven &gt; users seting &gt; user setting &gt; C:\app3\apache-maven-3.5.3\conf\settings.xmlwindows &gt; preferences &gt; maven &gt; users seting &gt; local repository &gt; C:\Users\BinLee\.m2\repository</code></pre></li><li><p>创建maven项目</p></li></ul><ol><li>创建 New-&gt;Other…-&gt;Maven-&gt;Maven Project</li><li>use default workspace location</li><li>archetypes maven-archetype-quickstart</li><li><p>new maven project</p><pre><code>com.hikvision.big_data.datatest_eclipse_maven0.0.1-SNAPSHOTcom.hikvision.big_data.data.test_eclipse_maven</code></pre></li><li><p>其中pom.xml</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.hikvision.big_data.data&lt;/groupId&gt;&lt;artifactId&gt;test_eclipse_maven&lt;/artifactId&gt;&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;name&gt;test_eclipse_maven&lt;/name&gt;&lt;url&gt;http://maven.apache.org&lt;/url&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt;   &lt;groupId&gt;junit&lt;/groupId&gt;   &lt;artifactId&gt;junit&lt;/artifactId&gt;   &lt;version&gt;3.8.1&lt;/version&gt;   &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;/project&gt;</code></pre></li><li><p>在网站上找到自己需要的依赖 <a href="http://mvnrepository.com/" target="_blank" rel="noopener">http://mvnrepository.com/</a></p><pre><code># 比如：我需要找到time相关的操作，直接mavenrepository中搜索time， 得到的Joda Time# 再用google搜索Joda Time，查看其用法# mavenrepository中的Joda Time依赖添加到pom.xml&lt;!-- https://mvnrepository.com/artifact/org.webjars.npm/d3-array --&gt;&lt;!-- https://mvnrepository.com/artifact/joda-time/joda-time --&gt;&lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;version&gt;2.9.9&lt;/version&gt;&lt;/dependency&gt;# 保存自动下载# 使用everything搜索Joda Time发现已经在C:\Users\BinLee\.m2\repository\joda-time\joda-time\2.9.9\joda-time-2.9.9.jar下面</code></pre></li><li><p>使用依赖</p><pre><code># 在窗口上project explorer&gt;maven dependencies查看需要的依赖类# 在需要地方直接插入# codepackage com.hikvision.big_data.data.test_eclipse_maven;import org.joda.time.DateTime;import org.joda.time.Days;import org.joda.time.LocalDateTime;/*** Hello world!**/public class App { public static void main(String[] args) {     System.out.println(&quot;Hello World!&quot;);     DateTime now = DateTime.now();     System.out.println(now);     Days maxValue = Days.MAX_VALUE;     System.out.println(maxValue);     System.out.println(LocalDateTime.now()); }}# outputHello World!2018-04-17T16:15:02.211+08:00P2147483647D2018-04-17T16:15:02.289</code></pre></li></ol><h2 id="关于maven源码打包"><a href="#关于maven源码打包" class="headerlink" title="关于maven源码打包"></a><strong>关于maven源码打包</strong></h2><ul><li><p>命令行方式，<a href="https://blog.csdn.net/symgdwyh/article/details/4407945" target="_blank" rel="noopener">他山之石</a></p><pre><code>cd {项目目录下}mvn source:jarmvn source:test-jar</code></pre></li><li><p>eclipse中pom.xml结尾加入插件，然后执行maven install</p><pre><code>...  &lt;/dependencies&gt;  &lt;build&gt;      &lt;plugins&gt;          &lt;plugin&gt;              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;              &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;              &lt;executions&gt;                  &lt;execution&gt;                      &lt;id&gt;attach-sources&lt;/id&gt;                      &lt;goals&gt;                          &lt;goal&gt;jar&lt;/goal&gt;                      &lt;/goals&gt;                  &lt;/execution&gt;              &lt;/executions&gt;          &lt;/plugin&gt;      &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre></li></ul><h2 id="利用idea运行maven-install报错的问题解决"><a href="#利用idea运行maven-install报错的问题解决" class="headerlink" title="利用idea运行maven install报错的问题解决"></a><strong>利用idea运行maven install报错的问题解决</strong></h2><ul><li>参考 <a href="http://tieba.baidu.com/p/4810060893?traceid=" target="_blank" rel="noopener">他山之石</a></li><li>右边maven projects &gt; lifecycle &gt; install</li><li>不要点击plugins &gt; install会报错</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍了apache maven的配置与使用过程，【清理项目】→【编译项目】→【测试项目】→【生成测试报告】→【打包项目】→【部署项目】，maven详细讲解：&lt;a href=&quot;https://www.yiibai.com/maven/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;他山之石&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="maven" scheme="http://yoursite.com/tags/maven/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="windows" scheme="http://yoursite.com/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title>常见的环境变量配置</title>
    <link href="http://yoursite.com/2018/04/17/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2018/04/17/常见的环境变量配置/</id>
    <published>2018-04-17T03:49:36.000Z</published>
    <updated>2018-04-21T05:52:38.667Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍常见的环境变量配置方法。</p><a id="more"></a><h2 id="windows常见的环境变量配置"><a href="#windows常见的环境变量配置" class="headerlink" title="windows常见的环境变量配置"></a>windows常见的环境变量配置</h2><ul><li><p>CLASSPATH</p><pre><code>.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</code></pre></li><li><p>JAVA_HOME</p><pre><code>C:\app3\Java\jdk1.8.0_144</code></pre></li><li><p>JRE_HOME</p><pre><code>C:\app3\Java\jre1.8.0_144</code></pre></li><li><p>MVN_HOME<br>C:\app3\apache-maven-3.5.3</p></li><li><p>Path</p><pre><code>C:\app3\Python35\Scripts\;C:\app3\Python35\;C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%JAVA_HOME%\bin;%JRE_HOME%\bin;C:\app3\Git\cmd;C:\app3\MinGW\bin;C:\app3\nodejs\;C:\app3\MATLAB\R2017b\runtime\win64;C:\app3\MATLAB\R2017b\bin;%MVN_HOME%\bin;</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍常见的环境变量配置方法。&lt;/p&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="windows" scheme="http://yoursite.com/tags/windows/"/>
    
      <category term="环境变量" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>eclipse中的git基本配置</title>
    <link href="http://yoursite.com/2018/04/16/eclipse%E4%B8%AD%E7%9A%84git%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2018/04/16/eclipse中的git基本配置/</id>
    <published>2018-04-16T10:56:26.000Z</published>
    <updated>2018-04-21T05:52:38.657Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了如何使用git和eclipse进行代码的版本控制。<br><a id="more"></a></p><h2 id="命令行模式操作"><a href="#命令行模式操作" class="headerlink" title="命令行模式操作"></a>命令行模式操作</h2><ul><li>服务端注册github或者giteee账号</li><li>客户端下载git软件</li><li>使用命令生成本地的密钥</li><li>将秘钥添加到服务端的git中</li><li>服务端新建git仓库，客户端克隆到本地</li><li>客户端添加文件到仓库中，使用各种命令对该仓库进行版本控制</li><li>上述的属于git的基本操作详细步骤参考 <a href="https://leebin.top/2018/03/27/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ubuntu%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89git%E6%9C%8D%E5%8A%A1%E7%AB%AF-%E9%99%84ssh%E6%93%8D%E4%BD%9C%EF%BC%9F/" target="_blank" rel="noopener">如何利用ubuntu实现私有git服务端-附ssh操作？</a></li></ul><h2 id="eclipse中git上传代码"><a href="#eclipse中git上传代码" class="headerlink" title="eclipse中git上传代码"></a>eclipse中git上传代码</h2><ul><li>服务端已经添加了客户端的ssh密钥</li><li>服务端已经新建了仓库</li><li>客户端eclipse新建项目</li><li>在路径eclipse&gt;windows&gt;preference&gt;team&gt;git&gt;configuration下查看user和passwd的配置</li><li>在路径package explorer&gt;项目右键&gt;share project&gt;repository&gt;create，新建本地的仓库名字要和服务端的名字一致，如：d:\test.git，完成了新建仓库</li><li>在路径package explorer&gt;项目右键&gt;team&gt;add to index，完成文件的add</li><li>在路径package explorer&gt;项目右键&gt;team&gt;commit或者Ctrl+#，提交</li><li>接上一步，先填写commit message</li><li>接上一步，填写服务器地址<pre><code>remote name: originurl: git@github.com:xjdlb/testgit.git # git 地址hostname: github.com # 域名repository path: xjdlb/testgit.git</code></pre></li><li>一路next就好了</li><li><a href="https://blog.csdn.net/u014079773/article/details/51595127" target="_blank" rel="noopener">他山之石</a></li></ul><h2 id="eclipse中git下载代码"><a href="#eclipse中git下载代码" class="headerlink" title="eclipse中git下载代码"></a>eclipse中git下载代码</h2><ul><li>在路径package explorer&gt;空白右键&gt;import&gt;Git&gt;Projects from Git，next</li><li>接上步，选择URI，包含了远程和本地</li><li>主要的分支</li><li>新建本地的仓库，如：d:\test.git</li><li>继续coding</li><li>返回上面上传代码操作</li></ul><h2 id="eclipse-push-出现了-rejected-non-fast-forward错误"><a href="#eclipse-push-出现了-rejected-non-fast-forward错误" class="headerlink" title="eclipse push 出现了 rejected-non-fast-forward错误"></a>eclipse push 出现了 rejected-non-fast-forward错误</h2><ul><li><a href="https://blog.csdn.net/chenshun123/article/details/46756087" target="_blank" rel="noopener">他山之石</a></li><li>打开windows&gt;show view&gt;other&gt;git repositories</li><li>git repositories&gt;remote&gt;origin&gt;绿色分支&gt;右键&gt;configure fetch&gt;save and fetch</li><li>此时可以看见remote tracking&gt;origin/mater&gt;右键&gt;merge</li><li>问题解决，可以上传了</li><li>add&gt;commit&gt;push</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍了如何使用git和eclipse进行代码的版本控制。&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
      <category term="eclipse" scheme="http://yoursite.com/tags/eclipse/"/>
    
  </entry>
  
  <entry>
    <title>使用kcptun加速ss服务</title>
    <link href="http://yoursite.com/2018/04/15/%E4%BD%BF%E7%94%A8kcptun%E5%8A%A0%E9%80%9Fss%E6%9C%8D%E5%8A%A1/"/>
    <id>http://yoursite.com/2018/04/15/使用kcptun加速ss服务/</id>
    <published>2018-04-15T15:55:21.000Z</published>
    <updated>2018-04-21T05:52:38.664Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何使用kcptun加速ss服务。<br><a id="more"></a></p><h2 id="软件准备"><a href="#软件准备" class="headerlink" title="软件准备"></a>软件准备</h2><ul><li>安装组件<pre><code>apt-get updateapt-get upgradeapt-get install build-essential python-pip m2crypto supervisor</code></pre></li><li>安装ss<pre><code>pip install shadowsocks</code></pre></li><li>安装加密用软件 libsodium<pre><code>wget https://github.com/jedisct1/libsodium/releases/download/1.0.11/libsodium-1.0.11.tar.gztar zxvf libsodium-1.0.11.tar.gzcd libsodium-1.0.11./configuremake &amp;&amp; make checkmake installecho /usr/local/lib &gt; /etc/ld.so.conf.d/usr_local_lib.confldconfig</code></pre></li><li><p>[可选] 配置supervisor, vi /etc/supervisor/conf.d/shadowsocks.conf</p><pre><code>[program:shadowsocks]command=ssserver -c /etc/shadowsocks.jsonautorestart=trueuser=root</code></pre></li><li><p>[可选] 使用supervisor</p><pre><code>supervisorctl reloadsupervisorctl status</code></pre></li></ul><h2 id="ss配置"><a href="#ss配置" class="headerlink" title="ss配置"></a>ss配置</h2><ul><li>ss服务器配置ss_config.json<pre><code>{&quot;server&quot;: &quot;127.0.0.1&quot;,&quot;port_password&quot;: {&quot;10001&quot;: &quot;helloworld&quot;,&quot;10002&quot;: &quot;helloworld&quot;,&quot;10003&quot;: &quot;helloworld&quot;},&quot;local_port&quot;: 1080,&quot;timeout&quot;: 600,&quot;method&quot;: &quot;chacha20&quot;,&quot;auth&quot;: true}</code></pre></li><li>启动和停止脚本<pre><code>ssserver -c /root/shadowsocks/ss_config.json -d startssserver -c /root/shadowsocks/ss_config.json -d stop</code></pre></li></ul><h2 id="kcptun配置"><a href="#kcptun配置" class="headerlink" title="kcptun配置"></a>kcptun配置</h2><ul><li>kcptun官网 <a href="https://github.com/xtaci/kcptun/releases" target="_blank" rel="noopener">https://github.com/xtaci/kcptun/releases</a></li><li>其中 kcptun-linux-amd64-20180316.tar.gz 为Linux版本</li><li>其中 kcptun-windows-amd64-20180316.tar.gz 为Windows版本</li><li>安装 kcptun<pre><code>mkdir /root/kcptuncd /root/kcptunln -sf /bin/bash /bin/shwget https://github.com/xtaci/kcptun/releases/download/v20161118/kcptun-linux-amd64-20161118.tar.gztar -zxf kcptun-linux-amd64-*.tar.gz</code></pre></li><li>配置三个脚本start.sh, stop.sh, server-config.json</li></ul><ol><li><p>启动脚本vi /root/kcptun/start.sh</p><pre><code>#!/bin/bashcd /root/kcptun/./server_linux_amd64 -c /root/kcptun/server-config.json &gt; kcptun.log 2&gt;&amp;1 &amp;echo &quot;Kcptun started.&quot;</code></pre></li><li><p>停止脚本 vi /root/kcptun/stop.sh</p><pre><code>#!/bin/bashecho &quot;Stopping Kcptun...&quot;PID=`ps -ef | grep server_linux_amd64 | grep -v grep | awk &#39;{print $2}&#39;`if [ &quot;&quot; !=  &quot;$PID&quot; ]; thenecho &quot;killing $PID&quot;kill -9 $PIDfiecho &quot;Kcptun stoped.&quot;</code></pre></li><li><p>kcptun配置文件 vi /root/kcptun/server-config.json</p><pre><code>{&quot;listen&quot;: &quot;:443&quot;,&quot;target&quot;: &quot;127.0.0.1:10001&quot;,&quot;key&quot;: &quot;helloworld&quot;,&quot;crypt&quot;: &quot;salsa20&quot;,&quot;mode&quot;: &quot;fast2&quot;,&quot;mtu&quot;: 1350,&quot;sndwnd&quot;: 1024,&quot;rcvwnd&quot;: 1024,&quot;datashard&quot;: 5,&quot;parityshard&quot;: 5,&quot;dscp&quot;: 46,&quot;nocomp&quot;: true,&quot;acknodelay&quot;: false,&quot;nodelay&quot;: 0,&quot;interval&quot;: 40,&quot;resend&quot;: 0,&quot;nc&quot;: 0,&quot;sockbuf&quot;: 4194304,&quot;keepalive&quot;: 10}</code></pre></li></ol><ul><li>启动或停止kcptun<pre><code>sh /root/kcptun/start.shsh /root/kcptun/stop.sh</code></pre></li></ul><hr><h2 id="客户端windows环境中的kcptun配置"><a href="#客户端windows环境中的kcptun配置" class="headerlink" title="客户端windows环境中的kcptun配置"></a>客户端windows环境中的kcptun配置</h2><ul><li>kcptun官网 <a href="https://github.com/xtaci/kcptun/releases" target="_blank" rel="noopener">https://github.com/xtaci/kcptun/releases</a></li><li>client_windows_amd64.exe 放在全部英文目录下</li><li>创建下面的三个文件：run.vbs, client-config.json, stop.sh</li></ul><ol><li>在当前文件夹下，创建 run.vbs<pre><code>Dim RunKcptunSet fso = CreateObject(&quot;Scripting.FileSystemObject&quot;)Set WshShell = WScript.CreateObject(&quot;WScript.Shell&quot;)currentPath = fso.GetFile(Wscript.ScriptFullName).ParentFolder.Path &amp; &quot;\&quot;configFile = currentPath &amp; &quot;client-config.json&quot;logFile = currentPath &amp; &quot;kcptun.log&quot;exeConfig = currentPath &amp; &quot;client_windows_amd64.exe -c &quot; &amp; configFilecmdLine = &quot;cmd /c &quot; &amp; exeConfig &amp; &quot; &gt; &quot; &amp; logFile &amp; &quot; 2&gt;&amp;1&quot;WshShell.Run cmdLine, 0, False&#39;WScript.Sleep 1000&#39;Wscript.echo cmdLineSet WshShell = NothingSet fso = NothingWScript.quit</code></pre></li><li>在当前文件夹下，创建client-config.json<pre><code>{&quot;localaddr&quot;: &quot;:12345&quot;,&quot;remoteaddr&quot;: &quot;165.227.213.57:443&quot;,&quot;key&quot;: &quot;helloworld&quot;,&quot;crypt&quot;: &quot;salsa20&quot;,&quot;mode&quot;: &quot;fast2&quot;,&quot;conn&quot;: 1,&quot;autoexpire&quot;: 60,&quot;mtu&quot;: 1350,&quot;sndwnd&quot;: 128,&quot;rcvwnd&quot;: 1024,&quot;datashard&quot;: 5,&quot;parityshard&quot;: 5,&quot;dscp&quot;: 46,&quot;nocomp&quot;: true,&quot;acknodelay&quot;: false,&quot;nodelay&quot;: 0,&quot;interval&quot;: 40,&quot;resend&quot;: 0,&quot;nc&quot;: 0,&quot;sockbuf&quot;: 4194304,&quot;keepalive&quot;: 10}</code></pre></li><li>在当前文件夹下，创建stop.sh<pre><code>taskkill /f /im client_windows_amd64.exe</code></pre></li></ol><h2 id="客户端windows环境中的ss配置"><a href="#客户端windows环境中的ss配置" class="headerlink" title="客户端windows环境中的ss配置"></a>客户端windows环境中的ss配置</h2><ul><li>使用本地的配置<pre><code>127.0.0.112345helloworld(服务端ss的密码，不是kcptun的密码)chacha20</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何使用kcptun加速ss服务。&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="kcptun" scheme="http://yoursite.com/tags/kcptun/"/>
    
      <category term="ss" scheme="http://yoursite.com/tags/ss/"/>
    
  </entry>
  
  <entry>
    <title>如何去掉valine的Powered By信息？</title>
    <link href="http://yoursite.com/2018/04/03/%E5%A6%82%E4%BD%95%E5%8E%BB%E6%8E%89valine%E7%9A%84Powered%20By%E4%BF%A1%E6%81%AF%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/03/如何去掉valine的Powered By信息？/</id>
    <published>2018-04-03T14:42:05.000Z</published>
    <updated>2018-04-21T05:52:38.665Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何去掉valine页面上的Powered By信息。<br><a id="more"></a></p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul><li>找到配置文件<pre><code>blog/themes/next/layout/_third-party/comments/valine.swig</code></pre></li><li>配置如下<pre><code>{% if theme.valine.enable and theme.valine.appid and theme.valine.appkey %}  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>  <script type="text/javascript">    var GUEST = ['nick','mail','link'];    var guest = '{{ theme.valine.guest_info }}';    guest = guest.split(',').filter(item=>{      return GUEST.indexOf(item)>-1;    });    new Valine({        el: '#comments' ,        verify: {{ theme.valine.verify }},        notify: {{ theme.valine.notify }},        appId: '{{ theme.valine.appid }}',        appKey: '{{ theme.valine.appkey }}',        placeholder: '{{ theme.valine.placeholder }}',        avatar:'{{ theme.valine.avatar }}',        guest_info:guest,        pageSize:'{{ theme.valine.pageSize }}' || 10,    });//新增以下代码即可，可以移除.info下所有子节点。var infoEle = document.querySelector('#comments .info');if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){  infoEle.childNodes.forEach(function(item) {item.parentNode.removeChild(item);  });}  </script>{% endif %}</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何去掉valine页面上的Powered By信息。&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="valine" scheme="http://yoursite.com/tags/valine/"/>
    
  </entry>
  
  <entry>
    <title>hexo如何开启语法高亮？</title>
    <link href="http://yoursite.com/2018/04/03/hexo%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/03/hexo如何开启语法高亮？/</id>
    <published>2018-04-03T14:42:05.000Z</published>
    <updated>2018-04-21T05:52:38.660Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍hexo有关语法高亮的配置方案。<br><a id="more"></a></p><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><ul><li>配置主站点下的配置文件<pre><code>highlight:enable: trueline_number: trueauto_detect: truetab_replace:</code></pre></li><li>代码后面添加名称，如```java code ```</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍hexo有关语法高亮的配置方案。&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>LC_001_TwoSum_HashMap</title>
    <link href="http://yoursite.com/2018/04/03/LC_001_TwoSum_HashMap/"/>
    <id>http://yoursite.com/2018/04/03/LC_001_TwoSum_HashMap/</id>
    <published>2018-04-03T14:42:05.000Z</published>
    <updated>2018-04-21T05:52:38.657Z</updated>
    
    <content type="html"><![CDATA[<p>leetcode第001题，主要用到了hashmap数据结构。<br><a id="more"></a></p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><pre><code class="java">package LC;import java.util.Arrays;import java.util.HashMap;/** * https://leetcode.com/problems/two-sum/description/ * Given an array of integers, * return indices of the two numbers such that they add up to a specific target. * You may assume that each input would have exactly one solution, * and you may not use the same element twice. * Example: * Given nums = [2, 7, 11, 15], target = 9, * Because nums[0] + nums[1] = 2 + 7 = 9, * return [0, 1]. */public class LC_001_TwoSum_HashMap {    public static void main(String[] args) {        int[] a = {1, 2, 3, 4, 5, 7};        int t = 10;        System.out.println(Arrays.toString(twoSum(a, t)));    }    private static int[] twoSum(int[] nums, int target) {        HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();        for (int i = 0; i &lt; nums.length; i++) {            int diff = target - nums[i];            if (map.containsKey(diff)) return new int[]{map.get(diff), i};            map.put(nums[i], i);        }        throw new IllegalArgumentException(&quot;-1&quot;);    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;leetcode第001题，主要用到了hashmap数据结构。&lt;br&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://yoursite.com/categories/leetcode/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="leetcode" scheme="http://yoursite.com/tags/leetcode/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="basic algorithm" scheme="http://yoursite.com/tags/basic-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>如何在next配置站内的搜索引擎？</title>
    <link href="http://yoursite.com/2018/04/03/%E5%A6%82%E4%BD%95%E5%9C%A8next%E9%85%8D%E7%BD%AE%E7%AB%99%E5%86%85%E7%9A%84%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/03/如何在next配置站内的搜索引擎？/</id>
    <published>2018-04-03T07:13:52.000Z</published>
    <updated>2018-04-21T05:52:38.666Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何在next配置站内的搜索引擎。<br><a id="more"></a></p><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><ul><li>安装hexo-generator-searchdb插件，以管理员身份打开cmd进入项目目录下，运行<pre><code>npm install hexo-generator-searchdb --save</code></pre></li><li>在站点的-config.yml文件中增加<pre><code>search:path: search.xmlfield: postformat: htmllimit: 10000</code></pre></li><li>配置theme/next/-config.yml文件<pre><code># Algolia Searchalgolia_search:enable: falsehits:  per_page: 10labels:  input_placeholder: Search for Posts  hits_empty: &quot;We didn&#39;t find any results for the search: ${query}&quot;  hits_stats: &quot;${hits} results found in ${time} ms&quot;## Local search# Dependencies: https://github.com/flashlab/hexo-generator-searchlocal_search:enable: true# if auto, trigger search by changing input# if manual, trigger search by pressing enter key or search buttontrigger: auto# show top n results per article, show all results by setting to -1top_n_per_article: 1</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何在next配置站内的搜索引擎。&lt;br&gt;
    
    </summary>
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="列表" scheme="http://yoursite.com/tags/%E5%88%97%E8%A1%A8/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="next" scheme="http://yoursite.com/tags/next/"/>
    
  </entry>
  
</feed>
